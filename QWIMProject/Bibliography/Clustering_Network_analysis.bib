% Encoding: UTF-8

@Article{Aked-Ko-2017,
  author               = {Aked, Michael and Ko, Amie},
  date                 = {2017},
  journaltitle         = {SSRN e-Print},
  title                = {Time Diversification Redux},
  url                  = {https://ssrn.com/abstract=3040967},
  abstract             = {Conventional risk measures may not accurately describe the volatility investors actually experience, especially for portfolios servicing their retirement spending needs. Return volatility rises as its calculated holding period nears 1 year and falls as it lengthens to 10 years. Lower volatility at longer holding periods implies that longer-term mean reversion exists. A portfolio achieves the greatest extra-return benefit by rebalancing over the holding period of highest volatility. Time diversification is helpful, up until long-term uncertainty about the value of reinvested cash flows from dividends leads to rising volatility.},
  citeulike-article-id = {14438504},
  citeulike-linkout-0  = {https://papers.ssrn.com/sol3/papers.cfm?abstractid=3040967},
  groups               = {Diversification_Measure, Diversified_Invest, Invest_Diversif},
  posted-at            = {2017-09-26 22:40:12},
  timestamp            = {2020-02-25 20:57},
}

@Article{Allen-et-al-2016,
  author               = {Allen, David and McAleer, Michael and Powell, Robert and Singh, Abhay},
  date                 = {2016-06},
  journaltitle         = {Journal of Risk and Financial Management},
  title                = {Down-Side Risk Metrics as Portfolio Diversification Strategies across the Global Financial Crisis},
  doi                  = {10.3390/jrfm9020006},
  number               = {2},
  pages                = {6+},
  volume               = {9},
  abstract             = {This paper features an analysis of the effectiveness of a range of portfolio diversification strategies, with a focus on down-side risk metrics, as a portfolio diversification strategy in a European market context. We apply these measures to a set of daily arithmetically-compounded returns, in U.S. dollar terms, on a set of ten market indices representing the major European markets for a nine-year period from the beginning of 2005 to the end of 2013. The sample period, which incorporates the periods of both the Global Financial Crisis (GFC) and the subsequent European Debt Crisis (EDC), is a challenging one for the application of portfolio investment strategies. The analysis is undertaken via the examination of multiple investment strategies and a variety of hold-out periods and backtests. We commence by using four two-year estimation periods and a subsequent one-year investment hold out period, to analyse a naive 1/N diversification strategy and to contrast its effectiveness with Markowitz mean variance analysis with positive weights. Markowitz optimisation is then compared to various down-side investment optimisation strategies. We begin by comparing Markowitz with CVaR, and then proceed to evaluate the relative effectiveness of Markowitz with various draw-down strategies, utilising a series of backtests. Our results suggest that none of the more sophisticated optimisation strategies appear to dominate naive diversification.},
  citeulike-article-id = {14412283},
  citeulike-linkout-0  = {http://dx.doi.org/10.3390/jrfm9020006},
  citeulike-linkout-1  = {http://www.mdpi.com/1911-8074/9/2/6},
  citeulike-linkout-2  = {http://www.mdpi.com/1911-8074/9/2/6/pdf},
  day                  = {21},
  groups               = {Diversification_Measure, Diversified_Invest, Effective_Dim_Diversif, Invest_Diversif},
  posted-at            = {2017-08-10 23:14:01},
  timestamp            = {2020-02-25 20:57},
}

@Article{Baitinger-et-al-2015,
  author               = {Baitinger, Eduard and Kutsarov, Iliya and Maier, Thomas and Storr, Marcus and Wan, Tao},
  date                 = {2015-03},
  journaltitle         = {Credit and Capital Markets - Kredit und Kapital},
  title                = {A Wholistic Approach to Diversification Management: The Diversification Delta Strategy Applied to Non-Normal Return Distributions},
  doi                  = {10.3790/ccm.48.1.89},
  issn                 = {2199-1227},
  number               = {1},
  pages                = {89--119},
  volume               = {48},
  abstract             = {In this paper we study a higher moment diversification measure, the so-called diversification delta (Vermorken et al. (2012)), in a dynamic portfolio optimization context. Particularly, we set up an investment strategy that dynamically maximizes the diversification delta for a given set of assets. Thus, we label the resulting optimized portfolio structure as Maximum Diversification Delta Portfolio (MDDP). Our out-of-sample empirical study reveals that considering crisis-periods, the MDDP is superior to popular investment strategies, such as Minimum-Variance-Portfolio, Risk-Parity-Portfolio and Equally-Weighted-Portfolio, in terms of risk adjusted returns, risk moments and certainty equivalents. However, in line with other diversification measures the MDDP is no longer superior in upward trending markets.},
  citeulike-article-id = {14148591},
  citeulike-linkout-0  = {http://dx.doi.org/10.3790/ccm.48.1.89},
  groups               = {Diversification_Measure, Diversified_Invest, Invest_Diversif},
  owner                = {cristi},
  posted-at            = {2016-09-28 20:02:36},
  timestamp            = {2020-02-25 20:57},
}

@Article{Bianchi-et-al-2016a,
  author               = {Bianchi, Robert and Drew, Michael and Walk, Adam},
  date                 = {2016},
  journaltitle         = {Financial Planning Research Journal},
  title                = {The Time Diversification Puzzle: A Survey},
  url                  = {https://www.griffith.edu.au/__data/assets/pdf_file/0025/205729/time-diversification-puzzle-bianchi-drew-walk.pdf},
  abstract             = {Since Samuelson's (1969) theoretical proof that risk and time are unrelated, a half century of debate and controversy has ensued, leaving time diversification as one of the most enduring puzzles of modern finance. The most conspicuous aspect of the debate is the questionable assumptions that underlie much of the analysis. Thus we are left with an unsatisfying debate conducted in a paradigm where terminal wealth is usually a function only of returns, and where time-weighted measures are assumed to adequately evaluate performance. This paper reviews the major streams in the time diversification literature and argues that more realistic analysis using defensible assumptions is likely to lead to better prescriptions for improved retirement investing},
  citeulike-article-id = {14514122},
  groups               = {Diversification_Measure, Invest_Diversif},
  posted-at            = {2018-01-09 16:42:45},
  timestamp            = {2020-02-25 20:57},
}

@TechReport{Carli-et-al-2014,
  author      = {Tiffanie Carli and Romain Deguest and Lionel Martellini},
  date        = {2014},
  institution = {EDHEC-Risk Institute},
  title       = {Improved Risk Reporting with Factor-Based Diversification Measures},
  url         = {https://risk.edhec.edu/publications/improved-risk-reporting-factor-based-diversification-measures},
  abstract    = {This paper analyses various measures of portfolio diversification, and explores the implication in terms of advanced risk reporting techniques. We use the minimal linear torsion approach (Meucci et al. (2013)) to turn correlated constituents into uncorrelated factors, and focus on the effective number of (uncorrelated) bets (ENB), the entropy of the distribution of risk factor contribution to portfolio risk, as a meaningful measure of the degree of diversification in a portfolio.

In an attempt to assess whether a relationship exists between the degree of diversification of a portfolio and its performance in various market conditions, we empirically analyse the diversification of various equity indices and pension fund policy portfolios. We find strong evidence of a significantly positive time-series and cross-sectional relationship between the ENB risk diversification measure and performance in bear markets.

This relationship, however, is highly linear, and the top performing portfolios in severe bear markets are typically portfolios concentrated in safe assets, as opposed to well-diversified portfolios. We also find statistical and economic evidence that this diversification measure has predictive power for equity market returns, a predictive power which becomes substantial over long holding period.

Overall our results suggest that the ENB measure could be a useful addition to the list of risk indicators reported for equity and policy portfolios.},
  groups      = {Diversification_Measure, Effective_Dim_Diversif, Invest_Diversif},
  owner       = {Anne},
  timestamp   = {2020-02-25 20:57},
}

@Article{Carmichael-et-al-2015,
  author               = {Carmichael, Benoit and Koumou, Gilles and Moran, Kevin},
  date                 = {2015-05},
  journaltitle         = {SSRN e-Print},
  title                = {Unifying Portfolio Diversification Measures Using Rao's Quadratic Entropy},
  url                  = {https://ssrn.com/abstract=2610814},
  abstract             = {This paper extends the use of Rao (1982b)'s Quadratic Entropy (RQE) to modern portfolio theory. It argues that the RQE of a portfolio is a valid, flexible and unifying approach to measuring portfolio diversification. The paper demonstrates that portfolio's RQE can encompass most existing measures, such as the portfolio variance, the diversification ratio, the normalized portfolio variance, the diversification return or excess growth rates, the Gini-Simpson indices, the return gaps, Markowitz's utility function and Bouchaud's general free utility. The paper also shows that assets selected under RQE can protect portfolios from mass destruction (systemic risk) and an empirical illustration suggests that this protection is substantial.},
  citeulike-article-id = {13997244},
  citeulike-linkout-0  = {http://ssrn.com/abstract=2610814},
  citeulike-linkout-1  = {http://papers.ssrn.com/sol3/Delivery.cfm/SSRNID2610814code361712.pdf?abstractid=2610814 and mirid=1},
  day                  = {26},
  groups               = {Diversification_Measure, Invest_Diversif},
  owner                = {cristi},
  posted-at            = {2016-04-04 22:58:50},
  timestamp            = {2020-02-25 20:57},
}

@Article{Cesarone-Colucci-2018,
  author         = {Cesarone, Francesco and Colucci, Stefano},
  date           = {2018-02},
  journaltitle   = {Journal of the Operational Research Society},
  title          = {Minimum risk versus capital and risk diversification strategies for portfolio construction},
  doi            = {10.1057/s41274-017-0216-5},
  issn           = {0160-5682},
  number         = {2},
  pages          = {183--200},
  volume         = {69},
  abstract       = {In this paper, we propose an extensive empirical analysis on three categories of portfolio selection models with very different objectives: minimization of risk, maximization of capital diversification, and uniform distribution of risk allocation. The latter approach, also called Risk Parity or Equal Risk Contribution (ERC), is a recent strategy for asset allocation that aims at equally sharing the risk among all the assets of the selected portfolio. The risk measure commonly used to select ERC portfolios is volatility. We propose here new developments of the ERC approach using Conditional Value-at-Risk (CVaR) as a risk measure. Furthermore, under appropriate conditions, we also provide an approach to find a CVaR ERC portfolio as a solution of a convex optimization problem. We investigate how these classes of portfolio models (Minimum-Risk, Capital-Diversification, and Risk-Diversification) work on seven investment universes, each with different sources of risk, including equities, bonds, and mixed assets. Then, we highlight some strengths and weaknesses of all portfolio strategies in terms of various performance measures.},
  f1000-projects = {QuantInvest},
  groups         = {Diversification_Measure, Diversified_Invest, Invest_Diversif},
  timestamp      = {2020-02-25 20:57},
}

@Article{Cesarone-et-al-2018,
  author         = {Cesarone, Francesco and Moretti, Jacopo and Tardella, Fabio},
  date           = {2018-02-07},
  journaltitle   = {SSRN e-Print},
  title          = {Why Small Portfolios Are Preferable and How to Choose Them},
  url            = {https://ssrn.com/abstract=3154353},
  abstract       = {One of the fundamental principles in portfolio selection models is minimization of risk through diversification of the investment. However, this principle does not necessarily translate into a request for investing in all the assets of the investment universe. Indeed, following a line of research started by Evans and Archer almost 50 years ago, we provide here further evidence that small portfolios are sufficient to achieve almost optimal in-sample risk reduction with respect to variance and to some other popular risk measures, and very good out-of-sample performances. While leading to similar results, our approach is significantly different from the classical one pioneered by Evans and Archer. Indeed, we describe models for choosing the portfolio of a prescribed size with the smallest possible risk, as opposed to the random portfolio choice investigated in most of the previous works. We find that the smallest risk portfolios generally require no more than 15 assets. Furthermore, it is almost always possible to find portfolios that are just 1\% more risky than the smallest risk portfolios and contain no more than 10 assets. The preference for small optimal portfolios is also justified by recent theoretical results on the estimation errors for the parameters required by portfolio selection models. Our empirical analysis is based on some new and on some publicly available benchmark data sets often used in the literature.},
  day            = {7},
  f1000-projects = {QuantInvest},
  groups         = {Diversification_Measure},
  timestamp      = {2020-02-25 20:57},
}

@Article{Cesarone-Moretti-2016,
  author         = {Cesarone, Francesco and Moretti, Jacopo},
  date           = {2016},
  journaltitle   = {Economics Bulletin},
  title          = {Optimally chosen small portfolios are better than large ones},
  number         = {4},
  translator     = {Tardella, Fabio},
  volume         = {36},
  abstract       = {One of the fundamental principles in portfolio selection models is minimization of risk through diversification of the investment. However, this principle does not necessarily translate into a request for investing in all the assets of the investment universe. Indeed, following a line of research started by Evans and Archer almost fifty years ago, we provide here further evidence that small portfolios are sufficient to achieve almost optimal in-sample risk reduction with respect to variance and to some other popular risk measures, and very good out-of-sample performances. While leading to similar results, our approach is significantly different from the classical one pioneered by Evans and Archer. Indeed, we describe models for choosing the portfolio of a prescribed size with the smallest possible risk, as opposed to the random portfolio choice investigated in most of the previous works. We find that the smallest risk portfolios generally require no more than 15 assets. Furthermore, it is almost always possible to find portfolios that are just 1\% more risky than the smallest risk portfolios and contain no more than 10 assets. Furthermore, the optimal small portfolios generally show a better performance than the optimal large ones. Our empirical analysis is based on some new and on some publicly available benchmark data sets often used in the literature.},
  f1000-projects = {QuantInvest},
  groups         = {Diversification_Measure},
  timestamp      = {2020-02-25 20:57},
}

@Article{Chollete-et-al-2012,
  author       = {Chollete, L. and de la Pena, V. and Lu, C.},
  date         = {2012},
  journaltitle = {Journal of Banking and Finance},
  title        = {International diversification: An extreme value approach},
  number       = {3},
  pages        = {871--885},
  url          = {https://www.sciencedirect.com/science/article/pii/S0378426611002767},
  volume       = {36},
  abstract     = {International diversification has costs and benefits, depending on the degree of asset dependence. We study international diversification with two dependence measures: correlations and extreme dependence. We discover that dependence has typically increased over time, and document mixed evidence on heavy tails in individual countries.

Moreover, we uncover three additional findings related to dependence. First, the timing of downside risk differs depending on the region. Surprisingly, recent Latin American returns exhibit little downside risk. Second, Latin America exhibits a great deal of correlation complexity. Third, according to the empirical results, correlation does not vary with returns, but extreme dependence does vary monotonically with regional returns.

Our results are consistent with a tradeoff between international diversification and systemic risk. They also suggest international limits to diversification, and that international investors demand some compensation for joint downside risk during extreme events.},
  groups       = {Diversification_Measure, Invest_Diversif},
  owner        = {zkgst0c},
  timestamp    = {2020-02-25 20:57},
}

@Article{Ayres-Nalebuff-2013,
  author               = {Ayres, Ian and Nalebuff, Barry},
  date                 = {2013-01},
  journaltitle         = {The Journal of Portfolio Management},
  title                = {Diversification Across Time},
  doi                  = {10.3905/jpm.2013.39.2.073},
  issn                 = {0095-4918},
  number               = {2},
  pages                = {73--86},
  volume               = {39},
  abstract             = {Young people who buy stock on margin and reduce their equity exposure as they age can reduce lifetime portfolio risk. For example, an initially leveraged portfolio produces the same mean accumulation as a constant 74 percent stock allocation with a 21 percent smaller standard deviation. Since the means are equal, the reduced volatility doesn't depend on the equity premium.

A leveraged life-cycle strategy also lets investors come closer to their utility-maximizing equity allocation. Monte Carlo simulations show that the gains continue even with equity premiums well below historical levels.},
  citeulike-article-id = {13972068},
  citeulike-linkout-0  = {http://dx.doi.org/10.3905/jpm.2013.39.2.073},
  groups               = {Diversified_Invest, Invest_Diversif},
  owner                = {cristi},
  posted-at            = {2016-03-08 06:43:35},
  timestamp            = {2020-02-25 21:00},
}

@PhdThesis{Bektic-2018,
  author         = {Bektic, Demir},
  date           = {2018},
  institution    = {Technical University of Darmstadt},
  title          = {Factor-based Portfolio Management with Corporate Bonds},
  type           = {phdthesis},
  url            = {https://tuprints.ulb.tu-darmstadt.de/7272/},
  abstract       = {Over the past 50 years financial asset pricing theories have evolved from simple single-factor models to more complex multi-factor models. Initially, Sharpe (1964) Capital Asset Pricing Model (CAPM) postulated that security markets can be described by a single factor (market beta). The basic premise of the model is that market participants require a risk premium for investing in high-beta assets that are typically considered more risky than low-beta assets. However, in the aftermath of the 2008 global financial crisis, two major trends emerged in the investment industry that laid the groundwork for the rise of factor-based investment strategies: 1) Investors started to evaluate and implement portfolio diversification in terms of underlying systematic risk factors given the failure of active management to provide adequate downside protection. 2) Investors demanded cost-effective, transparent and systematic alternative investment vehicles that could capture most or at least parts of active managers excess return. As a consequence, factor-based investing has grown in popularity and rapidly attracted academics, asset managers and institutional investors.},
  f1000-projects = {QuantInvest},
  groups         = {Diversified_Invest, Multifactor_Invest},
  timestamp      = {2020-02-25 21:00},
}

@Article{Benichou-et-al-2017,
  author               = {Benichou, Raphael and Lemperiere, Yves and Serie, Emmanuel and Kockelkoren, Julien and Seager, Philip and Bouchaud, Jean P. and Potters, Marc},
  date                 = {2017-06},
  journaltitle         = {Journal of Investment Strategies},
  title                = {Agnostic risk parity: taming known and unknown unknowns},
  doi                  = {10.21314/jois.2017.083},
  issn                 = {2047-1238},
  number               = {3},
  pages                = {1--12},
  volume               = {6},
  abstract             = {Markowitz' celebrated optimal portfolio theory generally fails to deliver out-of-sample diversification. In this note, we propose a new portfolio construction strategy based on symmetry arguments only, leading to "Eigenrisk Parity"portfolios that achieve equal realized risk on all the principal components of the covariance matrix. This holds true for any other definition of uncorrelated factors. We then specialize our general formula to the most agnostic case where the indicators of future returns are assumed to be uncorrelated and of equal variance. This "Agnostic Risk Parity"(AGP) portfolio minimizes unknown-unknown risks generated by over-optimistic hedging of the different bets. AGP is shown to fare quite well when applied to standard technical strategies such as trend following.},
  citeulike-article-id = {14386377},
  citeulike-linkout-0  = {http://dx.doi.org/10.21314/jois.2017.083},
  groups               = {Risk_Budgeting, Diversified_Invest, Invest_Risk},
  posted-at            = {2017-07-02 23:52:46},
  timestamp            = {2020-02-25 21:00},
}

@Article{Berger-et-al-2013a,
  author               = {Berger, Dave and Pukthuanthong, Kuntara and Yang, Jimmy J.},
  date                 = {2013-07-31},
  journaltitle         = {The Journal of Portfolio Management},
  title                = {Is the Diversification Benefit of Frontier Markets Realizable by Mean-Variance Investors? The Evidence of Investable Funds},
  doi                  = {10.3905/jpm.2013.39.4.036},
  issn                 = {0095-4918},
  number               = {4},
  pages                = {36--48},
  volume               = {39},
  abstract             = {The authors investigate whether the diversification benefits of frontier markets are realizable. They focus on investable frontier exchange-traded funds (ETFs) and their corresponding indices. Their analysis ncludes directly measuring the economic benefits of frontier-market diversification, as well as considering frontier-market trading dynamics. Evidence indicates that frontier markets offer diversification benefits through risk-reducing potential. The authors find that frontier market volatility tends to be largely idiosyncratic, which supports the risk-reducing role of frontier markets. Their comparison of funds and indices indicates that, to the extent that frontier-market indices offer hypothetical benefits, traders can obtain these benefits by using investable funds.},
  citeulike-article-id = {14504915},
  citeulike-linkout-0  = {http://dx.doi.org/10.3905/jpm.2013.39.4.036},
  day                  = {31},
  groups               = {Diversified_Invest, Invest_Diversif},
  posted-at            = {2017-12-18 21:50:53},
  timestamp            = {2020-02-25 21:00},
}

@Article{Bernardi-et-al-2018,
  author               = {Bernardi, Simone and Leippold, Markus and Lohre, Harald},
  date                 = {2018-01},
  journaltitle         = {European Financial Management},
  title                = {Maximum diversification strategies along commodity risk factors},
  doi                  = {10.1111/eufm.12122},
  issn                 = {1354-7798},
  number               = {1},
  pages                = {53--78},
  volume               = {24},
  abstract             = {Pursuing risk-based allocation across a universe of commodity assets, we find diversified risk parity (DRP) strategies to provide convincing results. DRP strives for maximum diversification along uncorrelated risk sources. A straightforward way to derive uncorrelated risk sources relies on principal components analysis (PCA). While the ensuing statistical factors can be associated with commodity sector bets, the corresponding DRP strategy entails excessive turnover because of the instability of the PCA factors. We suggest an alternative design of the DRP strategy relative to common commodity risk factors that implicitly allows for a uniform exposure to commodity risk premia.},
  citeulike-article-id = {14521503},
  citeulike-linkout-0  = {http://dx.doi.org/10.1111/eufm.12122},
  groups               = {Diversified_Invest, Invest_Risk, Invest_Cmdty, Invest_Diversif},
  posted-at            = {2018-01-22 17:16:36},
  timestamp            = {2020-02-25 21:00},
}

@Article{Binstock-et-al-2017,
  author               = {Binstock, Jay and Kose, Engin and Mazzoleni, Michele},
  date                 = {2017},
  journaltitle         = {SSRN e-Print},
  title                = {Diversification Strikes Again: Evidence from Global Equity Factors},
  url                  = {https://ssrn.com/abstract=3036423},
  abstract             = {The benefits of country diversification are well established. This article shows that the same benefits extend to equity factors, such as value, size, momentum, investment, and profitability. Specifically, country factor portfolios reflect both common variation, which we define as the global factor, and local variation. On average, a US investor could enjoy a 30 percent reduction in portfolio volatility by investing globally. We also document three other properties of equity factors. Like major asset classes, greater market integration is associated with greater factor co-movement, and factor portfolios of different countries tend to be more correlated during bear stock markets. However, unlike asset classes, the correlations of factor portfolios across countries have not been increasing over the last two decades, making global equity factors a particularly desirable addition to a portfolio.},
  citeulike-article-id = {14433817},
  citeulike-linkout-0  = {https://papers.ssrn.com/sol3/papers.cfm?abstractid=3036423},
  groups               = {Diversified_Invest, Invest_Diversif},
  posted-at            = {2017-09-18 20:32:50},
  timestamp            = {2020-02-25 21:00},
}

@Article{Bock-2018,
  author         = {Bock, Johannes},
  date           = {2018-11-20},
  journaltitle   = {arXiv e-Print},
  title          = {An updated review of (sub-)optimal diversification models},
  url            = {https://arxiv.org/abs/1811.08255},
  abstract       = {In the past decade many researchers have proposed new optimal portfolio selection strategies to show that sophisticated diversification can outperform the naive 1/N strategy in out-of-sample benchmarks. Providing an updated review of these models since DeMiguel et al. (2009b), I test sixteen strategies across six empirical datasets to see if indeed progress has been made. However, I find that none of the recently suggested strategies consistently outperforms the 1/N or minimum-variance approach in terms of Sharpe ratio, certainty-equivalent return or turnover. This suggests that simple diversification rules are not in fact inefficient, and gains promised by optimal portfolio choice remain unattainable out-of-sample due to large estimation errors in expected returns. Therefore, further research effort should be devoted to both improving estimation of expected returns, and possibly exploring diversification rules that do not require the estimation of expected returns directly, but also use other available information about the stock characteristics.},
  day            = {20},
  f1000-projects = {QuantInvest},
  groups         = {Diversified_Invest, Invest_Diversif},
  timestamp      = {2020-02-25 21:00},
}

@Article{Boigner-Gadzinski-2015,
  author               = {Boigner, Philip and Gadzinski, Gregory},
  date                 = {2015-03},
  journaltitle         = {Journal of Asset Management},
  title                = {Diversification with risk factors and investable hedge fund indices},
  doi                  = {10.1057/jam.2015.10},
  issn                 = {1470-8272},
  number               = {2},
  pages                = {101--116},
  volume               = {16},
  abstract             = {This article complements existing studies on dynamic portfolio construction by implementing a wide spectrum of optimization methodologies on four types of investments: traditional asset classes, risk factors and premia, and hedge fund investable indices. Portfolios are constructed using four different objectives with weights re-allocated every month during the period 2002-2013.

We show evidence that including hedge fund investable indices in a traditional portfolio is effective in mitigating volatility and drawdown. Risk factors portfolios do not benefit from the inclusion of hedge funds though. Moreover, we provide guidance on which allocation techniques should be used given the nature of the assets in one's portfolio.},
  citeulike-article-id = {13967863},
  citeulike-linkout-0  = {http://dx.doi.org/10.1057/jam.2015.10},
  groups               = {Diversified_Invest, Hedge_Funds, Invest_Diversif},
  owner                = {cristi},
  posted-at            = {2016-03-05 12:59:13},
  timestamp            = {2020-02-25 21:00},
}

@Article{Carl-2017,
  author               = {Carl, Ulrich},
  date                 = {2017},
  journaltitle         = {SSRN e-Print},
  title                = {The Power of Equity Factor Diversification},
  url                  = {https://ssrn.com/abstract=2915443},
  abstract             = {This paper analyses the diversification properties of country equity factors across six equity factors and twenty developed markets from 1991 to 2015. The factors considered are the market excess return, size, value, momentum, low beta and quality. I find substantial diversification benefits along the country dimension as well as the factor dimension. In a portfolio setting, country diversification significantly reduces the volatility compared to single country investing for each of the six equity factors. Factor diversification works in each of the twenty markets by means of reducing the portfolio volatility.},
  citeulike-article-id = {14511760},
  citeulike-linkout-0  = {https://papers.ssrn.com/sol3/papers.cfm?abstractid=2915443},
  groups               = {Diversified_Invest, Invest_Diversif},
  posted-at            = {2018-01-03 21:20:24},
  timestamp            = {2020-02-25 21:00},
}

@Article{Cesarone-et-al-2014,
  author               = {Cesarone, Francesco and Moretti, Jacopo and Tardella, Fabio},
  date                 = {2014},
  journaltitle         = {SSRN e-Print},
  title                = {Does Greater Diversification Really Improve Performance in Portfolio Selection?},
  doi                  = {10.2139/ssrn.2473630},
  issn                 = {1556-5068},
  abstract             = {One of the fundamental principles in portfolio selection models is minimization of risk through diversification of the investment. This seems to require that in a given working universe, or market, the investment should be spread among all (or almost all) the available assets. Indeed, this is what some classical investment strategies, like Equally-Weighted portfolios, or more recent and refined ones, like Risk Parity, actually recommend.

The purpose of this work consists in giving some empirical evidence of the fact that diversifying through the use of larger portfolios is not the best way to achieve an improvement in out-of-sample performance. More precisely, we investigate the role of the restriction on the number of assets in a portfolio (a cardinality constraint) on the in-sample and out-of-sample outcomes of the Equally-Weighted approach and of some well-known portfolio selection models that minimize risk through the use of Variance, Semi-Mean Absolute Deviation, and Conditional Value-at-Risk.

Our empirical analysis is based on some new and on some publicly available benchmark data sets often used in the literature.},
  citeulike-article-id = {14320281},
  citeulike-linkout-0  = {http://dx.doi.org/10.2139/ssrn.2473630},
  groups               = {Diversified_Invest, Invest_Diversif},
  posted-at            = {2017-03-26 19:16:17},
  timestamp            = {2020-02-25 21:00},
}

@Article{Chambers-Zdanowicz-2014,
  author               = {Chambers, Donald R. and Zdanowicz, John S.},
  date                 = {2014-07},
  journaltitle         = {The Journal of Portfolio Management},
  title                = {The Limitations of Diversification Return},
  doi                  = {10.3905/jpm.2014.40.4.065},
  issn                 = {0095-4918},
  number               = {4},
  pages                = {65--76},
  volume               = {40},
  abstract             = {Diversification return is the amount by which the geometric mean return (i.e., average compounded return) of a portfolio exceeds the weighted average of the geometric means of the portfolio's constituent assets. Diversification return has been touted as a source of added return, even if markets are informationally efficient. Portfolio rebalancing has been advocated as a valuable source of diversification return.

The authors demonstrate that diversification return is not a source of increased expected value. However, portfolio rebalancing can be an effective mean-reverting strategy. Any enhanced expected value from rebalancing emanates from mean-reversion, rather than from diversification or variance reduction.},
  citeulike-article-id = {13972189},
  citeulike-linkout-0  = {http://dx.doi.org/10.3905/jpm.2014.40.4.065},
  groups               = {Diversified_Invest, Invest_Diversif},
  owner                = {cristi},
  posted-at            = {2016-03-08 09:05:15},
  timestamp            = {2020-02-25 21:00},
}

@Article{Choueifaty-Coignard-2008,
  author       = {Choueifaty, Y. and Coignard, Y.},
  date         = {2008},
  journaltitle = {The Journal of Portfolio Management},
  title        = {Toward maximum diversification},
  number       = {1},
  pages        = {40--51},
  url          = {https://jpm.pm-research.com/content/35/1/40},
  volume       = {35},
  abstract     = {Along with the ongoing effort to build market cap-independent portfolios, the authors explore the properties of diversification as a driver of portfolio construction. They introduce a measure of the diversification of a portfolio that they term the diversification ratio. The measure is then employed to build a risk-efficient portfolio, or the Most- Diversified Portfolio. The theoretical properties of the resulting portfolios are discussed and compared to other popular methodologies, such as market-cap weights, equal weights, and minimum variance. The empirical results confirm that these popular methodologies are dominated by risk-efficient portfolios in many aspects. The implication is that in the long run, actively managed portfolios that maximize diversification are strong candidates for achieving consistently better results than commonly used passive index tracking methodologies. The message is clear- investors and their trustees cannot afford to ignore the benefits of maximal diversification.},
  groups       = {Diversified_Invest, Invest_Diversif},
  owner        = {zkgst0c},
  timestamp    = {2020-02-25 21:00},
}

@Article{Christophe-2017,
  author               = {Christophe, Stephen E.},
  date                 = {2017-04},
  journaltitle         = {The Journal of Wealth Management},
  title                = {Time to Stay Home?Global Diversification during the Past 25 Years},
  doi                  = {10.3905/jwm.2017.2017.1.053},
  issn                 = {1534-7524},
  abstract             = {Over the past 25 years, as financial markets have become increasingly integrated, the role of foreign equities in a well-diversified portfolio has become increasingly uncertain. We present evidence that a globally diversified portfolio underperforms, on average, a U.S.-only allocation. Investors should not be overly optimistic about the potential benefits of international portfolio diversification.},
  citeulike-article-id = {14338557},
  citeulike-linkout-0  = {http://dx.doi.org/10.3905/jwm.2017.2017.1.053},
  day                  = {07},
  groups               = {Diversified_Invest, Invest_Diversif},
  posted-at            = {2017-04-17 11:21:53},
  timestamp            = {2020-02-25 21:00},
}

@Article{Chua-et-al-2009,
  author       = {Chua, D. B. and Kritzman, M. and Page, S.},
  date         = {2009},
  journaltitle = {The Journal of Portfolio Management},
  title        = {The Myth of Diversification},
  number       = {1},
  pages        = {26--35},
  url          = {https://jpm.pm-research.com/content/36/1/26},
  volume       = {36},
  abstract     = {Perhaps the most universally accepted precept of prudent investing is to diversify, yet this precept grossly oversimplifies the challenge of portfolio construction. Correlations, as typically measured over the full sample of returns, often belie an asset's diversification properties in market environments when diversification is most needed. Moreover, upside diversification is undesirable. The authors first describe the mathematics of conditional correlations assuming returns are normally distributed. Then they present empirical results across a wide variety of assets, which reveal that, unlike the theoretical conditional correlations, empirical correlations are significantly asymmetric. Finally, the authors show that a portfolio construction technique called full-scale optimization produces portfolios in which the component assets exhibit relatively lower correlations on the downside and higher correlations on the upside than mean-variance optimization portfolios.},
  groups       = {PortfOptim_FullScale, Diversified_Invest, Invest_Diversif},
  owner        = {zkgst0c},
  timestamp    = {2020-02-25 21:00},
}

@Article{Clarke-et-al-2013,
  author       = {Roger Clarke and Harindra de Silva and Steven Thorley},
  date         = {2013},
  journaltitle = {The Journal of Portfolio Management},
  title        = {Risk Parity, Maximum Diversification, and Minimum Variance: An Analytic Perspective},
  url          = {https://jpm.pm-research.com/content/39/3/39},
  abstract     = {Analytic solutions to risk parity, maximum diversification, and minimum variance portfolios provide useful perspectives about their construction and composition. Individual asset weights depend on both systematic and idiosyncratic risk in all three risk-based portfolios, but systematic risk eliminates many investable assets in long-only, constrained, maximum-diversification, and minimum-variance portfolios. On the other hand, risk-parity portfolios include all investable assets, and idiosyncratic risk has little effect on weight magnitude.

The algebraic forms for optimal asset weights derived in this article yield generalizable properties of risk-based portfolios, in contrast to empirical simulations that employ a specific set of historical returns, proprietary risk models, and multiple constraints. These analytic solutions reveal precisely how various kinds of predicted risk affect the relative magnitude of security weights in each type of risk-based portfolio construction.},
  groups       = {Risk_Budgeting, Diversified_Invest, Invest_Risk, Invest_Diversif},
  owner        = {zkgst0c},
  timestamp    = {2020-02-25 21:00},
}

@Article{Bergin-Pyun-2016,
  author               = {Bergin, Paul R. and Pyun, Ju H.},
  date                 = {2016-04},
  journaltitle         = {Journal of International Money and Finance},
  title                = {International portfolio diversification and multilateral effects of correlations},
  doi                  = {10.1016/j.jimonfin.2015.12.012},
  issn                 = {0261-5606},
  pages                = {52--71},
  volume               = {62},
  abstract             = {Bilateral asset holdings depend on the correlation with all other countries. Higher stock return correlations lower bilateral equity asset holdings. Multilateral effects of correlations bias estimates. Not only are investors biased toward home assets, but when they do invest abroad, they appear to favor countries with returns more correlated with home assets. Often attributed to a preference for familiarity, this 'correlation puzzle' further reduces effective diversification. We use a multi-country general equilibrium model of portfolio choice to study how bilateral equity holdings are affected by return correlations among alternative destination and source countries. From the theoretical model, we develop an empirical approach to estimate a gravity equation for equity holdings that incorporates the overall covariance structure in a theoretically rigorous yet tractable manner. Estimation using this approach resolves the correlation puzzle, and finds that international investors do seek the diversification benefits of low cross-country correlations, as theory would predict.},
  citeulike-article-id = {14339368},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.jimonfin.2015.12.012},
  groups               = {Effective_Dim_Diversif, Invest_Diversif},
  posted-at            = {2017-04-19 06:32:43},
  timestamp            = {2020-02-25 21:00},
}

@InCollection{DeSilva-et-al-2017,
  author               = {{De Silva}, Harindra and McMurran, Gregory M. and Miller, Megan N.},
  booktitle            = {Factor Investing},
  date                 = {2017},
  title                = {Diversification and the Volatility Risk Premium},
  doi                  = {10.1016/b978-1-78548-201-4.50014-3},
  isbn                 = {9781785482014},
  pages                = {365--387},
  publisher            = {Elsevier},
  abstract             = {The volatility risk premium (VRP) found in options has paid off persistently across different assets, different asset classes and over time. A consistent short volatility position using options or volatility swaps has produced attractive risk-adjusted returns because of exposure to VRP. In this chapter, we have extended the study of the VRP to include not only equity indices but also commodities, government bonds and currencies. Using volatility swap returns as a measure of the payoff to the VRP, we see that the returns to a short volatility position are correlated to the volatility of the underlying instrument and to other VRPs in the same asset class. We also find that the returns are relatively uncorrelated to the VRPs of other asset classes and to the traditional equity factors represented by pure factor portfolios (PFPs). Finally, we show that the multiasset class VRP portfolio studied in this chapter has very competitive risk-adjusted returns},
  citeulike-article-id = {14499078},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/b978-1-78548-201-4.50014-3},
  groups               = {Diversified_Invest, RiskPremia_FixedIncome, RiskPremia_Other, Invest_Diversif, [nbkcbu3:]},
  posted-at            = {2017-12-08 00:25:33},
  timestamp            = {2020-02-25 21:06},
}

@Article{Dickson-2016a,
  author               = {Dickson, Mike},
  date                 = {2016-05},
  journaltitle         = {SSRN e-Print},
  title                = {Naive Diversification Isn't So Naive After All},
  url                  = {https://ssrn.com/abstract=2713501},
  abstract             = {I conduct a horse-race of 15 portfolio construction techniques over 8 empirical datasets comprised of individual stocks. I also conduct a robust Monte Carlo analysis that confirms that recent extensions of mean-variance optimization due to Kirby and Ostdiek (2012) are successful in curbing estimation risk and turnover. Despite these facts, my results indicate that no strategy consistently outperforms naive diversification in terms of mean excess return, Sharpe ratio, and turnover. I introduce a statistic, the time series average of the cross-sectional mean absolute deviation of risk and return, to explain why I observe these results. Data limitations and dataset characteristics contribute the most to the performance of a candidate strategy. I also propose several extensions to active timing strategies and include new characteristics in a parametric portfolio choice framework. Naive diversification continues to prevail, suggesting practical optimization techniques are inferior to naive diversification when forming portfolios of individual stocks.},
  citeulike-article-id = {14134601},
  citeulike-linkout-0  = {http://ssrn.com/abstract=2713501},
  citeulike-linkout-1  = {http://papers.ssrn.com/sol3/Delivery.cfm/SSRNID2713501code2352812.pdf?abstractid=2713501 and mirid=1},
  day                  = {19},
  groups               = {Diversified_Invest, Invest_Diversif},
  owner                = {cristi},
  posted-at            = {2016-09-11 12:16:21},
  timestamp            = {2020-02-25 21:06},
}

@Article{Diyarbakrloglu-Satman-2013,
  author               = {Diyarbakrloglu, Erkin and Satman, Mehmet H.},
  date                 = {2014-01},
  journaltitle         = {Journal of Asset Management},
  title                = {The Maximum Diversification Index},
  doi                  = {10.1057/jam.2013.28},
  issn                 = {1470-8272},
  number               = {6},
  pages                = {400--409},
  volume               = {14},
  abstract             = {We propose a new method to assess the risk diversification potential of a given investment set, using only the information content of the covariance matrix of returns. Namely, we extend Rudin and Morgan's (2006) work to numerically solve for the 'Maximum Diversification Index' by means of a genetic algorithm.

Using stock returns data from the S and P-500 index, we show that the MDI can be efficiently implemented to delimit a large set of investable assets by eliminating those subjects that do not improve the diversification characteristics of the underlying portfolio pool. Indeed, a subset of the S and P-500 stocks obtained using the MDI procedure preserves the mean-variance properties of the initial dataset as shown by the ex-post efficient frontiers.},
  citeulike-article-id = {13968912},
  citeulike-linkout-0  = {http://dx.doi.org/10.1057/jam.2013.28},
  day                  = {16},
  groups               = {Diversified_Invest, Invest_Diversif},
  owner                = {cristi},
  posted-at            = {2016-03-06 06:31:14},
  timestamp            = {2020-02-25 21:06},
}

@TechReport{Durante-et-al-2015a,
  author               = {Durante, Fabrizio and Foscolo, Enrico and Pappada, Roberta and Wang, Hao},
  date                 = {2015},
  institution          = {DEAMS Research Paper Series},
  title                = {A portfolio diversification strategy via tail dependence measures},
  url                  = {https://www.openstarts.units.it/handle/10077/11865},
  abstract             = {We provide a two-stage portfolio selection procedure in order to increase the diversification benefits in a bear market. By exploiting tail dependence-based risky measures a first-step cluster analysis is carried out for discerning between assets with the same performance during risky scenarios. Then a mean-variance efficient frontier is computed by fixing a number of assets per portfolio and by selecting only one item from each cluster. Empirical calculations on the EURO STOXX 50 prove that investing on selected index components in trouble periods may improve the risk-averse investor portfolio performance.},
  citeulike-article-id = {14150078},
  citeulike-linkout-0  = {https://www.openstarts.units.it/dspace/bitstream/10077/11865/1/DEAMSRP20153DuranteFoscoloPappadaWang.pdf},
  groups               = {Networks and investment management, Diversification_Measure, Diversified_Invest, Invest_Diversif},
  owner                = {cristi},
  posted-at            = {2016-10-01 20:48:29},
  timestamp            = {2020-02-25 21:06},
}

@Article{Facchinato-Pola-2014,
  author       = {Simone Facchinato and Gianni Pola},
  date         = {2014},
  journaltitle = {SSRN e-Print},
  title        = {Managing uncertainty with diversification across macroeconomic scenarios (DAMS): from asset segmentation to portfolio},
  url          = {http://research-center.amundi.com/page/Publications/Discussion-Paper/2014/Managing-uncertainty-with-Diversification-Across-Macroeconomic-Scenarios-DAMS-from-asset-segmentation-to-portfolio},
  abstract     = {Recent history has provided an excellent laboratory to test the robustness of investment processes. Despite claims of diversification, most balanced portfolios and pension funds were concentrated on equity risk and, consequently, key investment decisions ultimately consisted in a single binary bet: buy or sell equity. This led to pro-cyclical returns and generated a broad debate on the effectiveness of active management in generating performance in difficult market conditions.

In 2011 AMUNDI Italy decided to revise the asset allocation process starting with a reinterpretation of portfolio diversification in terms of Diversification Across Macroeconomic Scenarios (DAMS). The main ambitions of DAMS are: (i) to explain complex patterns of large investment universes in terms of a limited number of factors and (ii) to catch up the market risk premium without being exposed to specific macroeconomic dynamics and asset idiosyncratic risk. In a previous study we illustrated the DAMS principle and implications in terms of asset segmentation.

The aim of this paper is to move towards a new framework for multi-asset portfolio management, what we call DAMS second generation. DAMS first generation is enriched with new concepts and tools that enable us (i) to infer market expectations on relevant macroeconomic factors (growth and inflation) and global risk premium, and (ii) to properly manage portfolios via strategic and tactical asset allocation.},
  groups       = {Diversified_Invest, Scenario_Market, Scenario_Portfolio, Invest_Diversif},
  howpublished = {Available at http://research-center.amundi.com/page/Publications/Discussion-Paper/Managing-uncertainty-with-Diversification-Across-Macroeconomic-Scenarios-DAMS-from-asset-segmentation-to-portfolio},
  organization = {Amundi},
  owner        = {zkgst0c},
  timestamp    = {2020-02-25 21:06},
}

@Article{Flint-et-al-2016c,
  author               = {Flint, Emlyn J. and Chikurunhe, Florence and Seymour, Anthony J.},
  date                 = {2016},
  journaltitle         = {SSRN e-Print},
  title                = {The Cost of a Free Lunch: Dabbling in Diversification},
  url                  = {https://ssrn.com/abstract=2767436},
  abstract             = {It's often said that diversification is the only 'free lunch' available to investors; meaning that a properly diversified portfolio reduces total risk without necessarily sacrificing expected return. However, achieving true diversification is easier said than done, especially when we don't fully know what we mean when we're talking about diversification. While the qualitative purpose of diversification is well-known, a satisfactory quantitative definition of portfolio diversification is not. In this report, we summarise a wide range of diversification measures, focussing our efforts on those most commonly used in practice. We categorise each measure based on which portfolio aspect it focusses on: cardinality, weights, returns, risk or higher moments. We then apply these measures to a range of South African equity indices, thus giving a diagnostic review of historical local equity diversification and, perhaps more importantly, providing a description of the investable opportunity set available to fund managers in this space. Finally, we introduce the idea of diversification profiles. These regime-dependent profiles give a much richer description of portfolio diversification than their single-value counterparts and also allow one to manage diversification proactively based on one's view of future market conditions.},
  citeulike-article-id = {14186293},
  citeulike-linkout-0  = {https://papers.ssrn.com/sol3/papers.cfm?abstractid=2767436},
  groups               = {Diversified_Invest, Invest_Diversif},
  owner                = {cristi},
  posted-at            = {2016-11-17 22:18:46},
  timestamp            = {2020-02-25 21:06},
}

@Article{Flores-et-al-2017,
  author               = {Flores, Yuri Salazar and Bianchi, Robert J. and Drew, Michael E. and Truck, Stefan},
  date                 = {2017-07},
  journaltitle         = {The Journal of Portfolio Management},
  title                = {The Diversification Delta: A Different Perspective},
  doi                  = {10.3905/jpm.2017.43.4.112},
  issn                 = {0095-4918},
  number               = {4},
  pages                = {112--124},
  volume               = {43},
  abstract             = {In a 2012 article published in The Journal of Portfolio Management, Vermorken, Medda, and Schroder introduce a new measure of diversification, the Diversification Delta (DD), based on the entropy of the portfolio return distribution. Entropy as a measure of uncertainty has been used successfully in several frameworks and takes into account the entire statistical distribution, rather than just the first two moments. In this article, the authors highlight some drawbacks of the DD measure and go on to propose an alternative measure based on exponential entropy that overcomes the identified shortcomings. The authors present the properties of this new measure and propose it as an alternative for portfolio optimization that incorporates higher moments of asset returns, such as skewness and excess kurtosis.},
  citeulike-article-id = {14400399},
  citeulike-linkout-0  = {http://dx.doi.org/10.3905/jpm.2017.43.4.112},
  groups               = {Diversified_Invest, Invest_Diversif},
  posted-at            = {2017-07-27 09:47:48},
  timestamp            = {2020-02-25 21:06},
}

@Article{Fragkiskos-2014,
  author       = {Fragkiskos, A.},
  date         = {2014},
  journaltitle = {SSRN e-Print},
  title        = {What is portfolio diversification?},
  url          = {https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2331475},
  abstract     = {There is considerable controversy concerning what exactly portfolio diversification is and under what circumstances is it beneficial to investors, particularly in the wake of the most recent financial crisis in 2008. This paper gathers the various approaches on portfolio diversification throughout history, placing a higher emphasis on recent developments. The goal of this paper is not to provide an exhaustive list of diversification strategies, but rather to highlight the most commonly used ones, provide the motivation behind each approach, and show how they compare with real data.},
  groups       = {Diversified_Invest, Invest_Diversif},
  howpublished = {Available at SSRN: http://ssrn.com/abstract=2331475},
  owner        = {zkgst0c},
  timestamp    = {2020-02-25 21:06},
}

@Article{Geczy-2016,
  author               = {Geczy, Christopher},
  date                 = {2016-11},
  journaltitle         = {The Journal of Private Equity},
  title                = {The New Diversification: Open Your Eyes to Alternatives},
  doi                  = {10.3905/jpe.2016.20.1.072},
  issn                 = {1096-5572},
  number               = {1},
  pages                = {72--81},
  volume               = {20},
  abstract             = {During the 2008 financial crisis, many portfolios considered widely diversified failed to fulfill their expected function of protecting against large drawdowns. Historically, correlations among various types of stocks and bonds have usually increased during financial shocks, but the diversification shortcomings of standard portfolio allocations still surprised investors. Six years later, managers have a more sophisticated understanding of portfolio drawdown risk and how to mitigate it through diversification. In this article, the author advocates a focus on the risk exposures within a portfolio and inclusion of risk diversifiers-often sourced through so-called alternatives-to design portfolios more resistant to volatility spikes and major shocks.},
  citeulike-article-id = {14217774},
  citeulike-linkout-0  = {http://dx.doi.org/10.3905/jpe.2016.20.1.072},
  groups               = {Diversified_Invest, Invest_Diversif},
  owner                = {cristi},
  posted-at            = {2016-11-30 19:39:57},
  timestamp            = {2020-02-25 21:06},
}

@Article{Hardle-et-al-2018,
  author               = {Hardle, Wolfgang K. and Lee, David K. and Nasekin, Sergey and Petukhina, Alla},
  date                 = {2018},
  journaltitle         = {Journal of Asset Management},
  title                = {Tail Event Driven ASset allocation: evidence from equity and mutual funds' markets},
  doi                  = {10.1057/s41260-017-0060-9},
  pages                = {1--15},
  abstract             = {The correlation structure across assets and opposite tail movements are essential to the asset allocation problem, since they determine the level of risk in a position. Correlation alone is not informative on the distributional details of the assets. Recently introduced TEDAS-Tail Event Driven ASset allocation approach determines the dependence between assets at different tail measures. TEDAS uses adaptive Lasso-based quantile regression in order to determine an active set of negative coefficients. Based on these active risk factors, an adjustment for intertemporal correlation is made. In this research, authors aim to develop TEDAS, by introducing three TEDAS modifications differing in allocation weights' determination: a Cornish-Fisher Value-at-Risk minimization, Markowitz diversification rule or naive equal weighting. TEDAS strategies significantly outperform other widely used allocation approaches on two asset markets: German equity and Global mutual funds.},
  citeulike-article-id = {14479426},
  citeulike-linkout-0  = {http://dx.doi.org/10.1057/s41260-017-0060-9},
  citeulike-linkout-1  = {http://link.springer.com/article/10.1057/s41260-017-0060-9},
  groups               = {Diversified_Invest},
  posted-at            = {2017-11-20 20:16:48},
  publisher            = {Palgrave Macmillan UK},
  timestamp            = {2020-02-25 21:06},
}

@Article{Heinze-2016,
  author               = {Heinze, Thomas},
  date                 = {2016},
  journaltitle         = {SSRN e-Print},
  title                = {Markowitz {3.0}: Including Diversification Targets in Portfolio Optimization via Diversification Functions},
  doi                  = {10.2139/ssrn.2805368},
  issn                 = {1556-5068},
  abstract             = {Given Markowitz's mean-risk model, maximization of diversification is established as an additional investment target next to return maximization and risk minimization. This widens the opportunity to transfer market views into the model by additional diversification parameters and should therefore lead to an improved mapping of economic reality. The main focus is on the introduction of diversification functions which make diversification quantifiable and which are used as third objective in the optimization. Thus, the resulting efficient frontier extends to a three dimensional surface which includes the original efficient frontier according to Markowitz. Starting with the original Markowitz model through improvements in stochastic modelling in terms of risk measures, copulas, fat tails, etc., leaving the pure return/risk context can be interpreted as a third model generation.},
  citeulike-article-id = {14332615},
  citeulike-linkout-0  = {http://dx.doi.org/10.2139/ssrn.2805368},
  groups               = {Diversification_Measure, Diversified_Invest, Invest_Diversif},
  posted-at            = {2017-04-06 01:00:31},
  timestamp            = {2020-02-25 21:06},
}

@Article{Hjalmarsson-2011,
  author               = {Hjalmarsson, Erik},
  date                 = {2011-11},
  journaltitle         = {The Journal of Investing},
  title                = {Portfolio Diversification Across Characteristics},
  doi                  = {10.3905/joi.2011.20.4.084},
  issn                 = {1068-0896},
  number               = {4},
  pages                = {84--88},
  volume               = {20},
  abstract             = {This article studies long short portfolio strategies formed on seven different stock characteristics representing various measures of past returns, value, and size. Each individual characteristic results in a profitable portfolio strategy, but these single-characteristic strategies are dominated by a diversified strategy that places equal weight on each of the single-characteristic strategies. The benefits of diversifying across characteristic-based long short strategies are substantial and can be attributed to the mostly low, and sometimes substantially negative, correlation between the returns on the single-characteristic strategies.},
  citeulike-article-id = {13970992},
  citeulike-linkout-0  = {http://dx.doi.org/10.3905/joi.2011.20.4.084},
  groups               = {Diversification_Measure, Diversified_Invest, Invest_Diversif},
  owner                = {cristi},
  posted-at            = {2016-03-07 00:25:40},
  timestamp            = {2020-02-25 21:06},
}

@Article{Homescu-2014b,
  author       = {C. Homescu},
  date         = {2014},
  journaltitle = {SSRN e-Print},
  title        = {Many risks, one (optimal) portfolio},
  url          = {https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2473776},
  abstract     = {This study investigates how to obtain a portfolio which would provide above average returns while remaining robust to most risk exposures. Emphasis is placed on risk management, given our perspective (shared by many other practitioners), that retaining above average portfolio performance in current market environment depends strongly on having an effective risk management process.

We rely on a comprehensive survey of the literature to describe stylized facts of market returns and main categories of asset allocation methodologies, including Modern Portfolio Theory, Black-Litterman model, factor-based and risk-based strategies. Furthermore, we present both criticisms and defenses of strategies, together with potential issues identified by practitioners and corresponding solutions (if they do exist).

We outline recent enhancements to various types of portfolio strategies, and analyze how to incorporate (in the asset allocation framework) constraints, regularization, personal views, stylized features of empirical market data, and forward information given by financial options market data.

More prominence is given to strategies (risk parity, risk factors, factor investing, smart beta, dynamic, etc.) that were shown to deliver better portfolio performance in terms of returns, diversification, risk, etc. We also discuss a wide ranging collection of performance measures proposed in the literature for quantifying portfolio return, risk and diversification, identify which such measures are most popular with practitioners, and which corresponding strategies have best results (as shown in the literature).

Since a major topic of this study is managing risks, we provide details on the types of risk that portfolios may be exposed to, on approaches and strategies to handle such exposures, with highlighting of tail risk management. Portfolio insurance is also discussed.

We also describe practical aspects needed for a successful portfolio management, including robust estimation of covariances, correlations and model parameters, numerical optimization methods, key questions and issues identified by practitioners, Monte Carlo simulation, comprehensive testing framework, stress testing, available software implementations (usually in R), etc.

To summarize, the study analyzes all ingredients that are required, in our opinion, to deliver portfolios with above average performances and resilient to most risks, and concentrates on the strategies which have emerged as frontrunners in the last few years, both in the literature and in the market.},
  groups       = {Mean_Variance, Black_Litterman, Risk_Budgeting, PortfOptim_Robust, Invest_Risk, Factor_Types, Factor_Selection, Factor_Test, Invest_SmartBeta, DAA, Diversified_Invest, OBPI},
  howpublished = {Available at SSRN: http://ssrn.com/abstract=2473776},
  owner        = {cristi},
  timestamp    = {2020-02-25 21:06},
}

@Article{Hwang-et-al-2018,
  author               = {Hwang, Inchang and Xu, Simon and In, Francis},
  date                 = {2018-02},
  journaltitle         = {European Journal of Operational Research},
  title                = {Naive versus optimal diversification: Tail risk and performance},
  doi                  = {10.1016/j.ejor.2017.07.066},
  issn                 = {0377-2217},
  number               = {1},
  pages                = {372--388},
  volume               = {265},
  abstract             = {It is well documented in portfolio optimization that naive diversification outperforms optimal mean-variance diversification because the latter is subject to severe estimation error. Our study provides an alternative explanation for the outperformance of naive diversification by examining the tail risk of naive diversification relative to optimal mean-variance diversification. We utilize a rolling-sample approach and compare the out-of-sample performance and tail risk of various optimal strategies to that of the naive diversification strategy. Using portfolios consisting of individual stocks, we show that for portfolios containing relatively small number of stocks, naive diversification outperforms optimal mean-variance diversification and is less exposed to tail risk. However, for relatively large number of stocks in the portfolio, naive diversification maintains its superior performance but increases tail risk and results in more concave portfolio returns. These results imply that the outperformance of naive diversification acts as compensation for the increase in tail risk and concavity.},
  citeulike-article-id = {14500767},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.ejor.2017.07.066},
  groups               = {Diversified_Invest, Invest_Risk, Invest_TailRisk, Invest_Diversif},
  posted-at            = {2017-12-11 09:23:28},
  timestamp            = {2020-02-25 21:06},
}

@Article{Ilmanen-Kizer-2012,
  author       = {Ilmanen, Antti and Jared Kizer},
  date         = {2012},
  journaltitle = {The Journal of Portfolio Management},
  title        = {The Death of Diversification Has Been Greatly Exaggerated},
  pages        = {15--27},
  url          = {https://www.aqr.com/Insights/Research/Journal-Article/The-Death-of-Diversification-Has-Been-Greatly-Exaggerated},
  volume       = {38},
  abstract     = {Asset-class correlations generally tend to rise during crises. That certainly was true in the 2007-2009 financial crisis, and since then correlations have generally remained elevated as markets switch between binary risk-on/risk-off environments. However, we believe it would be wrong to interpret these developments as conclusive evidence of the death of diversification.

First, academics (Asness, Israelov and Liew [2011]) have stressed that while diversification often fails in short-term panics - especially one as systemic as the 2007-2009 crisis - it does effectively reduce downside risks over longer horizons. Second, high-quality bonds have fairly consistently provided positive returns during stressful market environments. Third, in this article, we argue and show that factor diversification has been more effective than asset-class diversification in general and, in particular, during crises. The last two arguments challenge the concentration in equity risk found in most institutional portfolios, which is also a central argument in favor of more risk-balanced, so-called risk parity, portfolios.

Traditional asset-class diversification involves allocating nominal dollars to various asset classes and their subsets. Several large institutions have begun to explore an alternative perspective of factor allocation, asking: What are the most important factors driving our portfolio returns? This perspective involves at least two changes. First, focus is shifted from dollar allocations to risk allocations. This change often reveals the dominant role of the most volatile asset classes and the portfolio's dependence on equity market direction. Second, portfolio analysis is extended beyond asset classes to dynamic strategy styles or to underlying risk factors. Fundamental factors such as growth, inflation and liquidity are naturally interesting, but they are inherently hard to measure. Most investors prefer investable factors and therefore use market-based proxies - equities for growth, Treasuries for deflation and commodities for inflation.},
  groups       = {Diversified_Invest, Invest_Diversif},
  owner        = {zkgst0c},
  timestamp    = {2020-02-25 21:06},
}

@Article{Iraya-Wafula-2018,
  author         = {Iraya, Cyrus and Wafula, Fernandes Juma},
  date           = {2018-02-28},
  journaltitle   = {European Scientific Journal},
  title          = {Does portfolio diversification affect performance of balanced mutual funds in kenya?},
  doi            = {10.19044/esj.2018.v14n4p158},
  issn           = {1857-7881},
  number         = {4},
  volume         = {14},
  abstract       = {Literature provides conflicting results on the effect of diversification on performance of mutual funds with some studies showing a positive relationship (Markowitz, 1952; Muriithi, 2005; Kagunga, 2010), others negative (Chang AND Elyasiani, 2008; Fiegenbaum and Thomas, 1998) and still others showing that there is no relationship between the two variables (Loeb, 1950). It is with this background that this study sought to establish the effect of diversification on performance of mutual funds in Kenya. The study took a descriptive research design approach on weekly performance of a sample of 7 balanced mutual funds for the year 2013.The study used secondary data sources available at the Capital Market Authority offices and from each mutual funds. The portfolio return was determined by computing the changes in prices of the balanced fund as traded at the Nairobi Securities Exchange (NSE) while diversification was determined from the level of Unsystematic Risk in the Performance. The study used the Ordinary Least Squares (OLS) multiple linear regression equation. Control variables of the size and age of the fund were introduced in the regression model. The results indicated the existence of a positive relationship between the Unsystematic Risk and Performance of balanced mutual funds with a beta coefficient of 0.069 (t=4.971, p 0.5. This implies that the lower the diversification the higher the performance of mutual funds.},
  day            = {28},
  f1000-projects = {QuantInvest},
  groups         = {Diversified_Invest, Invest_Diversif},
  timestamp      = {2020-02-25 21:06},
}

@Article{Kanuri-et-al-2018,
  author               = {Kanuri, Srinidhi and Malhotra, Davinder and Malm, James},
  date                 = {2018-01-23},
  journaltitle         = {The Journal of Wealth Management},
  title                = {Evaluating the Performance and Diversification Benefits of Emerging-Market Exchange-Traded Funds},
  doi                  = {10.3905/jwm.2018.20.4.085},
  issn                 = {1534-7524},
  number               = {4},
  pages                = {85--90},
  url                  = {https://jwm.pm-research.com/content/20/4/85},
  volume               = {20},
  abstract             = {This study evaluates the performance and diversification benefits for U.S. investors of emerging-market exchange-traded funds (ETFs) since their inception in January 2003 through June 2015 by comparing their absolute and risk-adjusted performance with the iShares Core SandP 500 ETF (IVV). The authors find that the emerging-market ETF portfolio has very low correlations with IVV during the period of the study. Emerging-market ETF portfolios delivered better absolute performance (returns and wealth) but also had much higher risk (standard deviation of returns). However, the risk-adjusted performance (Sharpe and Omega ratios) of IVV was better than that of the emerging-market ETF portfolio. They also look at the effect of adding some emerging-market ETFs to IVV during the period of study. The authors find that adding some emerging-market ETFs to IVV leads to higher absolute returns, better risk-adjusted performance (Sharpe and Omega ratios), higher cumulative returns, and increased wealth for U.S. investors. Results are statistically significant at 1\% in all cases. Therefore, U.S. investors should add some emerging-market ETFs to their domestic allocation based on their risk tolerance for better performance (absolute and risk-adjusted).},
  citeulike-article-id = {14525342},
  citeulike-linkout-0  = {http://dx.doi.org/10.3905/jwm.2018.20.4.085},
  day                  = {23},
  groups               = {Diversified_Invest, Invest_Diversif},
  posted-at            = {2018-01-28 16:35:16},
  timestamp            = {2020-02-25 21:10},
}

@Article{KaradedeBouras-Laopodis-2015,
  author               = {Karadede-Bouras, Markella and Laopodis, Nikiforos T.},
  date                 = {2015-07},
  journaltitle         = {The Journal of Wealth Management},
  title                = {Dynamics among Traditional and Alternative Assets: Implications for Diversification and Risk},
  doi                  = {10.3905/jwm.2015.18.2.013},
  issn                 = {1534-7524},
  number               = {2},
  pages                = {13--34},
  volume               = {18},
  abstract             = {The authors examine the dynamic correlations and implications of various fi nancial asset classes, such as equities, bonds, ETFs, commodities, and real estate, in the United States from 1990 to 2013 and find that the correlations have varied across time.

They detect no evidence of contagion but rather of herding behavior among these assets. The variation was more pronounced during market declines, but differed in extent across economic expansions and contractions. Finally, shocks from one asset class to another were not persistent, meaning that the assets were able to absorb the shocks and quickly return to normalcy.

The implications for portfolio decisions are clear: Even well-diversified portfolios must be updated to reflect changing economic and financial environments, and past asset behavior does not imply similar future behavior.},
  citeulike-article-id = {13968108},
  citeulike-linkout-0  = {http://dx.doi.org/10.3905/jwm.2015.18.2.013},
  groups               = {Diversified_Invest, Invest_Diversif},
  owner                = {cristi},
  posted-at            = {2016-03-05 18:07:15},
  timestamp            = {2020-02-25 21:10},
}

@Article{Kind-Poonia-2014,
  author               = {Kind, Christoph and Poonia, Muddit},
  date                 = {2014-03},
  journaltitle         = {SSRN e-Print},
  title                = {Diversification Management of a Multi-Asset Portfolio},
  url                  = {https://ssrn.com/abstract=2410153},
  abstract             = {It is a well-known fact in finance that classical mean-variance optimization often leads to highly concentrated portfolios. Giving equal weights to all portfolio assets will instead allow for maximum nominal diversification. More sophisticated ways of nominal diversification are the maximum diversification approach proposed by Choueifaty and Coignard (2008) or the equal weighting of total risk contributions known as risk parity . Instead of looking for nominal diversification, investors may prefer a diversification of the risk factors that drive portfolio returns. In recent papers, risk factors have been modelled by principal components following Partovi and Caputo (2004). Meucci et al. (2013) show that principal components may not be the best way to model risk factors and propose minimum torsion bets instead. The present paper discusses different ways of managing diversification and backtests these strategies in a multi-asset portfolio.},
  citeulike-article-id = {13997420},
  citeulike-linkout-0  = {http://ssrn.com/abstract=2410153},
  citeulike-linkout-1  = {http://papers.ssrn.com/sol3/Delivery.cfm/SSRNID2410153code1500322.pdf?abstractid=2410153 and mirid=1},
  day                  = {18},
  groups               = {Diversified_Invest, Invest_Diversif},
  owner                = {cristi},
  posted-at            = {2016-04-05 04:54:23},
  timestamp            = {2020-02-25 21:10},
}

@Article{Kupiec-2016,
  author               = {Kupiec, Paul},
  date                 = {2016},
  journaltitle         = {Journal of Investment Management},
  title                = {Portfolio Diversification In Concentrated Bond And Loan Portfolios},
  number               = {2},
  url                  = {https://www.joim.com/portfolio-diversification-in-concentrated-bond-and-loan-portfolios/},
  volume               = {14},
  abstract             = {I develop an algorithm to approximate the loss rate distribution for fixed income portfolios with obligor concentrations. The approximation requires no advanced mathematics or statistics, only the summation of large exposures and the evaluation of binomial probabilities. The approximation is model-independent and can be used after removing default dependence using any risk modeling approach. It is especially useful for capital calculations given its inherent accuracy in the upper tail of the cumulative portfolio loss rate distribution. The approximation provides a simple way to calculate the capital benefits of risk mitigation or the capital needed when a marginal credit is added to a concentrated portfolio},
  citeulike-article-id = {14486905},
  groups               = {Diversified_Invest, Invest_Diversif},
  posted-at            = {2017-12-01 22:20:00},
  timestamp            = {2020-02-25 21:10},
}

@Article{Lassance-et-al-2018,
  author         = {Lassance, Nathan and DeMiguel, Victor and Vrins, Frederic Daniel},
  date           = {2018},
  journaltitle   = {SSRN e-Print},
  title          = {Optimal portfolio diversification via independent component analysis},
  doi            = {10.2139/ssrn.3285156},
  issn           = {1556-5068},
  url            = {https://ssrn.com/abstract=3285156},
  abstract       = {A popular approach for enhancing diversification in portfolio selection is to rely on the factor-risk-parity portfolio, which is often defined as the portfolio whose return variance is spread equally among the principal components (PCs) of asset returns. Although PCs are useful for dimensionality reduction, they are arbitrary because any rotation of the PC basis yields an equally uncorrelated basis. This is problematic because we theoretically demonstrate that any portfolio is the factor-risk-parity portfolio corresponding to a specific uncorrelated basis. To overcome this problem, we rely on the factor-risk-parity portfolio based on the independent components (ICs), which are the rotation of the PCs that are maximally independent, and thus, account for higher-order moments. We propose a shrinkage portfolio that is obtained by combining the minimum-variance portfolio and the IC-risk-parity portfolio. We also show how to exploit the near independence of the ICs to parsimoniously estimate the factor-risk-parity portfolio with respect to Value-at-Risk. Finally, we empirically demonstrate that shrinkage portfolios based on the IC basis outperform those based on the PC basis, as well as the minimum-variance portfolio.},
  f1000-projects = {QuantInvest},
  groups         = {Diversified_Invest, Invest_Diversif},
  timestamp      = {2020-02-25 21:10},
}

@PhdThesis{Lee-2016a,
  author               = {Lee, Yongjae},
  date                 = {2016},
  institution          = {KAIST},
  title                = {Demystifying Diversification Strategies by using Portfolio Optimization Techniques},
  url                  = {https://www.researchgate.net/publication/309563859_Demystifying_Diversification_Strategies_by_using_Portfolio_Optimization_Techniques},
  abstract             = {In this dissertation, we study a series of diversification strategies in order to improve the understanding of the diversification of investments that was mathematically established by Harry Markowitz. Even though Modern Portfolio Theory considers a trade-off between generating high returns and lowering risks, investment processes inspired by the concept of diversification are generally only addressed with their diversification benefits. Therefore, we analyze the diversification strategies by using portfolio optimization techniques that primarily originated from the mean-variance framework, which considers returns as well as risks with equal importance, in order to more fully understand the quantifiable consequences of various diversification strategies. First, we investigate passive investing and performance benchmarking through analyzing the two most popular equity benchmark portfolios: the cap-weighted portfolio, and the equally-weighted portfolio. As conventional portfolio performance evaluations occur relative to benchmarks, the performance evaluation of the benchmark itself has never been a trivial issue. Thus, an alternative methodology for portfolio performance evaluation that can be conducted without peer information is proposed, and we find little or no evidence of either benchmark portfolio performing better than the average portfolio. In terms of performance benchmarking, however, equally-weighted portfolios exhibit more desirable properties than cap-weighted portfolios. Second, we examine the quantitative properties of asset allocation and asset classification. We derive and compare the closed form expressions for the portfolio performances of asset allocation and direct security selection, and we find that the majority of investors can benefit from employing asset allocation. Furthermore, our analysis indicates that the design of asset classes is a critical factor in determining the portfolio performance of employing asset allocation. Hence, we further test the two most widely used within-stock asset classification schemes, i.e. style and industry classifications, and find that the asset designs should not focus on diversification benefits only.

Third, we discuss the viability of robo-advising, which was recently developed during the ongoing expansion of financial technology (FinTech). Robo-advising attempts to lower the entry barrier to financial advising through utilizing automated but personalized algorithms, in order to attract investors with smaller accounts, who are ineligible to receive traditional financial advising services. We investigate the relationship between portfolio size and risk in order to examine the viability of roboadvisers in providing diversification benefits with limited portfolio sizes. The results indicate that a substantial investment is not necessary to gain diversification benefits.},
  citeulike-article-id = {14525362},
  groups               = {Diversified_Invest, AssetAlloc_vs_SecSelect, FinTech_WealthTech, Invest_Diversif},
  posted-at            = {2018-01-28 17:17:54},
  timestamp            = {2020-02-25 21:10},
}

@Article{Linder-2018,
  author         = {Linder, John},
  date           = {2018-05-31},
  journaltitle   = {The Journal of Investing},
  title          = {Rebalancing-Diversification Return: The Opportunity Cost of Illiquid Investments},
  doi            = {10.3905/joi.2018.27.2.057},
  issn           = {1068-0896},
  number         = {2},
  pages          = {57--65},
  volume         = {27},
  abstract       = {Institutional investors expect a return premium for illiquidity when an investment is private and cannot be sold easily in an established liquid market. For investors that have a very long investment horizon some might argue a permanent (or perpetual) investment portfolio in private markets with any such level of expected return premium might seem to be dominant to a markets only construct. However, how much should this premium be? The author hypothesizes that the illiquidity premium observed is directly related to the risk-equivalent liquid markets diversification-rebalancing returns forgone in pursuing illiquid investments. The author posits the excess return to illiquidity available to the long-term, non-liquidity constrained investor, is an investor-specific opportunity cost of illiquidity, and by logical extension, he proposes, an (efficient) market cost to illiquidity hypothesis. Finally, he examines the private equity industry benchmarking convention for performance evaluation-cap stocks + 300 bps this paradigm.},
  day            = {31},
  f1000-projects = {QuantInvest},
  groups         = {Diversified_Invest, Invest_Rebalancing, Invest_Liquidity, Invest_Diversif},
  timestamp      = {2020-02-25 21:10},
}

@Article{Lohre-et-al-2012a,
  author               = {Lohre, Harald and Neugebauer, Ulrich and Zimmer, Carsten},
  date                 = {2012},
  journaltitle         = {The Journal of Investing},
  title                = {Diversified Risk Parity Strategies for Equity Portfolio Selection},
  doi                  = {10.3905/joi.2012.21.3.111},
  number               = {3},
  volume               = {21},
  abstract             = {This article investigates a new way of equity portfolio selection that provides maximum diversification along the uncorrelated risk sources inherent in the SandP 500.This diversified risk parity strategy is distinct from prevailing risk-based portfolio construction paradigms. Especially, the strategy is characterized by a concentrated allocation that actively adjusts to changes in the underlying risk structure. In addition, x-raying the risk and diversification characteristics of traditional risk-based strategies like 1/N, minimum-variance, risk parity, or the most-diversified portfolio, the authors find the diversified risk parity strategy to be superior. Although most of these alternatives crucially pick up risk-based pricing anomalies like the low-volatility anomaly, the diversified risk parity strategy more effectively exploits systematic factor tilts.},
  citeulike-article-id = {14322318},
  citeulike-linkout-0  = {http://www.iijournals.com/doi/abs/10.3905/joi.2012.21.3.111},
  groups               = {Risk_Budgeting, Diversified_Invest, Invest_Diversif},
  posted-at            = {2017-03-29 09:07:28},
  timestamp            = {2020-02-25 21:10},
}

@Article{Lozano-2013,
  author       = {Lozano, Martin},
  date         = {2013},
  journaltitle = {SSRN e-Print},
  title        = {Diversification: A Bittersweet Story},
  url          = {https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2373738},
  abstract     = {We all have heard or even follow the proverb: Don't put all your eggs in one basket . Clever advice indeed, though it is mute about how we should distribute the eggs. Ideally, the best allocation advice is the one which minimize the number of broken eggs.

In Finance we do something similar, although we deal with wealth and assets instead of eggs and baskets. In particular, portfolio theory suggests diversification strategies aimed to reduce the overall portfolio risk by combining several assets like real state, stocks, bonds, commodities, and foreign currency, instead of concentrating in only one. In sum, we try to generate informed financial decisions within an uncertain environment.},
  groups       = {Diversified_Invest, Invest_Diversif},
  howpublished = {Available at SSRN: http://ssrn.com/abstract=2373738},
  owner        = {zkgst0c},
  timestamp    = {2020-02-25 21:10},
}

@Article{Meucci-et-al-2014,
  author       = {Meucci, A. and Santangelo, A. and Deguest, R.},
  date         = {2014},
  journaltitle = {SSRN e-Print},
  title        = {Measuring portfolio diversification based on optimized uncorrelated factors},
  url          = {https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2276632},
  abstract     = {We measure diversification in terms of the Effective Number of Minimum-Torsion Bets , namely a set of uncorrelated factors, optimized to closely track the factors used to allocate the portfolio. This way we introduce a novel notion of absolute risk contributions , which generalizes the marginal contributions to risk in traditional risk parity. We discuss the advantages of the Minimum-Torsion Bets over the traditional approach to diversification based on marginal contributions to risk.

We present a case study in the SandP 500.},
  groups       = {Diversified_Invest, Effective_Dim_Diversif, PortfOptim_Factor},
  howpublished = {Available at SSRN: http://ssrn.com/abstract=2276632},
  owner        = {zkgst0c},
  timestamp    = {2020-02-25 21:10},
}

@Article{Meucci-et-al-2015a,
  author         = {Meucci, Attilio and Santangelo, Alberto and Deguest, Romain},
  date           = {2015},
  journaltitle   = {SSRN e-Print},
  title          = {Risk Budgeting and Diversification Based on Optimized Uncorrelated Factors},
  doi            = {10.2139/ssrn.2276632},
  issn           = {1556-5068},
  url            = {https://ssrn.com/abstract=2276632},
  abstract       = {We measure the contributions to risk of a set of factors, strategies, or investments, based on "Minimum-Torsion Bets", namely a set of uncorrelated factors, optimized to closely track the factors used to allocate the portfolio. We then introduce a novel definition of contributions to risk, which generalizes the "marginal contributions to risk", traditionally used in banks for risk budgeting and in asset management to build risk parity strategies. The Minimum-Torsion Bets allow us to also introduce a natural diversification score, the Effective Number of Minimum-Torsion Bets, which we use to measure and manage diversification. We discuss the advantages of the Minimum-Torsion Bets over the traditional approach to diversification based on marginal contributions to risk. We present two case studies, a security-based investment in the stocks of the SandP 500, and a factor-based investment in the five Fama-French factors.},
  f1000-projects = {QuantInvest},
  groups         = {Diversified_Invest, Effective_Dim_Diversif, PortfOptim_Factor},
  timestamp      = {2020-02-25 21:10},
}

@Article{Miebs-2012,
  author       = {Miebs, Felix},
  date         = {2012},
  journaltitle = {SSRN e-Print},
  title        = {Diversifying Diversification Strategies: Model Averaging in Portfolio Optimization},
  url          = {https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2011969},
  abstract     = {The literature on portfolio optimization in the presence of parameter uncertainty has suggested several approaches to mitigate the impact of estimation error on portfolio performance. However, empirical evidence finds no single approach that can achieve a consistently higher risk-adjusted performance than 1/N. In this paper, I propose three averaging rules that synthesize the established approaches in order to mitigate the impact of estimation error on portfolio performance.

The evaluation of the proposed averaging rules on empirical and simulated datasets shows that each rule achieves a consistently higher risk-adjusted performance than 1/N, while all individual portfolio strategies considered in the averaging exercise do not. I find that the observed performance gains are economically and statistically significant. The performance gains are attributable to persistent diversification effects between the portfolio strategies under consideration, as well as to empirical characteristics in portfolio returns that are exploited by one of the averaging rules.},
  groups       = {Diversified_Invest, Invest_Diversif},
  howpublished = {Available at SSRN: http://ssrn.com/abstract=2011969},
  owner        = {zkgst0c},
  timestamp    = {2020-02-25 21:10},
}

@Article{Murtazashvili-Vozlyublennaia-2013,
  author               = {Murtazashvili, Irina and Vozlyublennaia, Nadia},
  date                 = {2013-06},
  journaltitle         = {Journal of Financial Research},
  title                = {Diversification strategies: do limited data constrain investors?},
  doi                  = {10.1111/j.1475-6803.2013.12008.x},
  issn                 = {0270-2592},
  number               = {2},
  pages                = {215--232},
  volume               = {36},
  abstract             = {We demonstrate that the mean-variance optimal portfolio does not outperform (out of sample) the naive 1/N diversification strategy even if securities are grouped into indexes or broad asset classes. This finding is due to insufficient data on past returns, which limit investors' ability to accurately estimate the means and covariance structure of securities. The resulting high estimation errors eliminate the benefits of using the means and covariance matrix as compared to the naive strategy in portfolio optimization. Using value-weighted indexes, characteristic-sorted portfolios, or portfolios defined by principal components as underlying assets in mean-variance optimization does not help. At the same time, increasing data frequency or adding data on past earnings may in some cases make mean-variance optimization useful.},
  citeulike-article-id = {14514968},
  citeulike-linkout-0  = {http://dx.doi.org/10.1111/j.1475-6803.2013.12008.x},
  groups               = {Diversified_Invest, Invest_Diversif},
  posted-at            = {2018-01-10 23:39:21},
  timestamp            = {2020-02-25 21:10},
}

@Article{Nystrup-et-al-2018a,
  author               = {Nystrup, Peter and Hansen, Bo W. and Larsen, Henrik O. and Madsen, Henrik and Lindstrom, Erik},
  date                 = {2018-12-22},
  journaltitle         = {The Journal of Portfolio Management},
  title                = {Dynamic Allocation or Diversification: A Regime-Based Approach to Multiple Assets},
  doi                  = {10.3905/jpm.2018.44.2.062},
  issn                 = {0095-4918},
  number               = {2},
  pages                = {62--73},
  volume               = {44},
  abstract             = {This article investigates whether regime-based asset allocation can effectively respond to changes in financial regimes at the portfolio level in an effort to provide better long-term results when compared to a static 60/40 benchmark. The potential benefit from taking large positions in a few assets at a time comes at the cost of reduced diversification. The authors analyze this trade-off in a multi-asset universe with great potential for static diversification. The regime-based approach is centered around a regime-switching model with time-varying parameters that can match financial markets' behavior and a new, more intuitive way of inferring the hidden market regimes. The empirical results show that regime-based asset allocation is profitable, even when compared to a diversified benchmark portfolio. The results are robust because they are based on available market data with no assumptions about forecasting skills.},
  citeulike-article-id = {14510367},
  citeulike-linkout-0  = {http://dx.doi.org/10.3905/jpm.2018.44.2.062},
  day                  = {22},
  groups               = {BenchmarkInvest, Diversified_Invest, Regime_Invest, Invest_Dynamic, Invest_Regime, FrcstQWIM_MedLngTerm, Invest_Diversif},
  posted-at            = {2017-12-30 13:06:32},
  timestamp            = {2020-02-25 21:10},
}

@Article{Kashyap-2017a,
  author               = {Kashyap, Ravi},
  date                 = {2017-12-01},
  journaltitle         = {arXiv e-Print},
  title                = {Combining Dimension Reduction, Distance Measures and Covariance},
  eprint               = {1603.09060},
  eprinttype           = {arXiv},
  url                  = {https://arxiv.org/abs/1603.09060},
  abstract             = {We develop a novel methodology based on the marriage between the Bhattacharyya distance, a measure of similarity across distributions of random variables, and the Johnson Lindenstrauss Lemma, a technique for dimension reduction. The resulting technique is a simple yet powerful tool that allows comparisons between data-sets representing any two distributions. The degree to which different entities, (markets, groups of securities, etc.), have different measures of their corresponding distributions tells us the extent to which they are different, aiding participants looking for diversification or looking for more of the same thing. We demonstrate a relationship between covariance and distance measures based on a generic extension of Stein's Lemma. We consider an asset pricing application and then briefly discuss how this methodology lends itself to numerous marketstructure studies and even applications outside the realm of finance / social sciences by illustrating a biological application.},
  citeulike-article-id = {14510843},
  citeulike-linkout-0  = {http://arxiv.org/abs/1603.09060},
  citeulike-linkout-1  = {http://arxiv.org/pdf/1603.09060},
  day                  = {1},
  groups               = {Diversification_Measure, Dimens_Reduc},
  posted-at            = {2018-01-02 01:13:37},
  timestamp            = {2020-02-25 21:10},
}

@Article{Lee-2011,
  author       = {Lee, W.},
  date         = {2011},
  journaltitle = {The Journal of Portfolio Management},
  title        = {Risk-Based Asset Allocation: A New Answer to an Old Question?},
  doi          = {10.3905/jpm.2011.37.4.011},
  number       = {4},
  pages        = {11--28},
  url          = {https://jpm.pm-research.com/content/37/4/11},
  volume       = {37},
  abstract     = {In recent years, we have witnessed an alarmingly large and growing amount of literature on portfolio construction approaches focused on risks and diversification rather than on estimating expected returns. Numerous simulations applied to different universes have been documented in support of these approaches based on their apparent outperformance versus passive market capitalization-weighted or static fixed-weight portfolios. Many studies attribute the better performance of these risk-based asset allocation approaches to superior diversification.

Given the absence of clearly defined investment objective functions behind these approaches as well as the metrics used by these studies to evaluate ex post performance, Lee puts these approaches into the same context of mean-variance efficiency in an attempt to understand their theoretical underpinnings. In doing so, he hopes to shed some light on what these approaches attempt to achieve and on the characteristics of the investment universe, if indeed these approaches are meant to approximate mean-variance efficiency. Rather than adding to the already large collection of simulation results, Lee uses some simple examples to compare and contrast the portfolio and risk characteristics of these approaches. He also reiterates that any portfolio which deviates from the market capitalization-weighted portfolio is an active portfolio.

He concludes that there is no theory to predict, ex ante, that any of these risk-based approaches should outperform.},
  groups       = {Diversification_Measure, ExAnte_ExPost},
  owner        = {zkgst0c},
  timestamp    = {2020-02-25 21:10},
}

@Article{Mignacca-2018,
  author         = {Mignacca, Domenico},
  date           = {2018-05-03},
  journaltitle   = {SSRN e-Print},
  title          = {A New Measure of Diversification: The M-DiX},
  url            = {https://ssrn.com/abstract=3172722},
  abstract       = {Diversification is a core concept in Asset Management. Yet diversification can mean different things to different people and there no consensus on how it is measured nor is there a broadly accepted metric for reporting of diversification. Sometimes, there is confusion in understanding diversification and how it differs from hedging. We may say that diversification and hedging both have the same objective i.e. reducing the risk of a portfolio, but diversification is obtained using correlated (in absolute value) securities, while hedging is achieved with correlated securities. In this paper we propose a new index to measure the diversification of a portfolio. Specifically, we outline a two-dimensional risk decomposition that we use to calculate our diversification index: the M-DiX.},
  day            = {3},
  f1000-projects = {QuantInvest},
  groups         = {Diversification_Measure, Invest_Diversif},
  timestamp      = {2020-02-25 21:10},
}

@Article{Oyenubi-2016,
  author               = {Oyenubi, Adeola},
  date                 = {2016},
  journaltitle         = {Computational Economics},
  title                = {Diversification Measures and the Optimal Number of Stocks in a Portfolio: An Information Theoretic Explanation},
  doi                  = {10.1007/s10614-016-9600-5},
  pages                = {1--29},
  abstract             = {This paper provides a plausible explanation for why the optimum number of stocks in a portfolio is elusive, and suggests a way to determine this optimal number. Diversification has a lot to do with the number of stocks in a portfolio. Adding stocks to a portfolio increases the level of diversification, and consequently leads to risk reduction up to a certain number of stocks, beyond which additional stocks are of no benefit, in terms of risk reduction. To explain this phenomenon, this paper investigates the relationship between portfolio diversification and concentration using a genetic algorithm. To quantify diversification, we use the portfolio Diversification Index (PDI). In the case of concentration, we introduce a new quantification method. Concentration is quantified as complexity of the correlation matrix. The proposed method quantifies the level of dependency (or redundancy) between stocks in a portfolio. By contrasting the two methods it is shown that the optimal number of stocks that optimizes diversification depends on both number of stocks and average correlation. Our result shows that, for a given universe, there is a set of Pareto optimal portfolios containing a different number of stocks that simultaneously maximizes diversification and minimizes concentration. The choice portfolio among the Pareto set will depend on the preference of the investor. Our result also suggests that an ideal condition for the optimal number of stocks is when variance reduction benefit of diversification is off-set by the variance contribution of complexity.},
  citeulike-article-id = {14398652},
  citeulike-linkout-0  = {http://dx.doi.org/10.1007/s10614-016-9600-5},
  citeulike-linkout-1  = {http://link.springer.com/article/10.1007/s10614-016-9600-5},
  groups               = {Diversification_Measure, Invest_Diversif},
  posted-at            = {2017-07-23 16:05:16},
  publisher            = {Springer US},
  timestamp            = {2020-02-25 21:10},
}

@Article{Raffinot-2016,
  author               = {Raffinot, Thomas},
  date                 = {2016-09},
  journaltitle         = {SSRN e-Print},
  title                = {Hierarchical Clustering Based Asset Allocation},
  url                  = {https://ssrn.com/abstract=2840729},
  abstract             = {Building upon the fundamental notion of hierarchy, Lopez de Prado (2016a) introduces a new portfolio diversification technique called "Hierarchical Risk Parity", which uses graph theory and machine learning techniques. Exploiting the same basic idea, a hierarchical clustering based asset allocation method is proposed. Classical and more modern hierarchical clustering methods are tested, such as Simple Linkage or Directed Bubble Hierarchical Tree for example. A simple and efficient capital allocation within and across clusters of assets at multiple hierarchical levels is computed. The out-of-sample performances of hierarchical clustering based portfolios and more traditional risk-based portfolios are evaluated across three disparate datasets. To avoid data snooping, the comparison of profit measures is assessed using the bootstrap based model confidence set procedure (Hansen et al. (2011)). The empirical results indicate that hierarchical clustering based portfolios are robust, truly diversified and achieve statistically better risk-adjusted performances than commonly used portfolio optimization techniques.},
  citeulike-article-id = {14146762},
  citeulike-linkout-0  = {http://ssrn.com/abstract=2840729},
  citeulike-linkout-1  = {http://papers.ssrn.com/sol3/Delivery.cfm/SSRNID2840729code2270025.pdf?abstractid=2840729 and mirid=1},
  day                  = {20},
  groups               = {Networks and investment management, Clustering and network analysis, Network_Invest},
  owner                = {cristi},
  posted-at            = {2016-09-26 22:30:25},
  timestamp            = {2020-02-25 21:14},
}

@Article{Ren-et-al-2016,
  author               = {Ren, Fei and Lu, Ya-Nan and Li, Sai-Ping and Jiang, Xiong-Fei and Zhong, Li-Xin and Qiu, Tian},
  date                 = {2016-08},
  journaltitle         = {arXiv e-Print},
  title                = {Dynamic portfolio strategy using clustering approach},
  eprint               = {1608.03058},
  eprinttype           = {arXiv},
  url                  = {https://arxiv.org/abs/1608.03058},
  abstract             = {The problem of portfolio optimization is one of the most important issues in asset management. This paper proposes a new dynamic portfolio strategy based on the time-varying structures of MST networks in Chinese stock markets, where the market condition is further considered when using the optimal portfolios for investment. A portfolio strategy comprises two stages: selecting the portfolios by choosing central and peripheral stocks in the selection horizon using five topological parameters, i.e., degree, betweenness centrality, distance on degree criterion, distance on correlation criterion and distance on distance criterion, then using the portfolios for investment in the investment horizon. The optimal portfolio is chosen by comparing central and peripheral portfolios under different combinations of market conditions in the selection and investment horizons. Market conditions in our paper are identified by the ratios of the number of trading days with rising index or the sum of the amplitudes of the trading days with rising index to the total number of trading days. We find that central portfolios outperform peripheral portfolios when the market is under a drawup condition, or when the market is stable or drawup in the selection horizon and is under a stable condition in the investment horizon. We also find that the peripheral portfolios gain more than central portfolios when the market is stable in the selection horizon and is drawdown in the investment horizon. Empirical tests are carried out based on the optimal portfolio strategy. Among all the possible optimal portfolio strategy based on different parameters to select portfolios and different criteria to identify market conditions, 65dollar; of our optimal portfolio strategies outperform the random strategy for the Shanghai A-Share market and the proportion is 70dollar; for the Shenzhen A-Share market.},
  citeulike-article-id = {14148628},
  citeulike-linkout-0  = {http://arxiv.org/abs/1608.03058},
  citeulike-linkout-1  = {http://arxiv.org/pdf/1608.03058},
  day                  = {10},
  groups               = {Networks and investment management, Clustering and network analysis, Network_Invest, Invest_Dynamic, Invest_Network},
  owner                = {cristi},
  posted-at            = {2016-09-28 20:56:42},
  timestamp            = {2020-02-25 21:14},
}

@InCollection{Wang-et-al-2017,
  author               = {Wang, Hao and Pappada, Roberta and Durante, Fabrizio and Foscolo, Enrico},
  booktitle            = {Soft Methods for Data Science},
  date                 = {2017},
  title                = {A Portfolio Diversification Strategy via Tail Dependence Clustering},
  doi                  = {10.1007/978-3-319-42972-4\_63},
  editor               = {Ferraro, Maria B. and Giordani, Paolo and Vantaggi, Barbara and Gagolewski, Marek and Angeles Gil, Mara and Grzegorzewski, Przemyslaw and Hryniewicz, Olgierd},
  pages                = {511--518},
  publisher            = {Springer International Publishing},
  series               = {Advances in Intelligent Systems and Computing},
  volume               = {456},
  abstract             = {We provide a two-stage portfolio selection procedure in order to increase the diversification benefits in a bear market. By exploiting tail dependence-based risky measures, a cluster analysis is carried out for discerning between assets with the same performance in risky scenarios. Then, the portfolio composition is determined by fixing a number of assets and by selecting only one item from each cluster. Empirical calculations on the EURO STOXX 50 prove that investing on selected assets in trouble periods may improve the performance of risk-averse investors.},
  citeulike-article-id = {14150080},
  citeulike-linkout-0  = {http://dx.doi.org/10.1007/978-3-319-42972-463},
  citeulike-linkout-1  = {http://link.springer.com/chapter/10.1007/978-3-319-42972-463},
  groups               = {Networks and investment management, Clustering and network analysis, Diversification_Measure, Diversified_Invest, Network_Invest, Invest_Network, Invest_Diversif},
  owner                = {cristi},
  posted-at            = {2016-10-01 20:49:54},
  timestamp            = {2020-02-25 21:14},
}

@Article{Dose-Cincotti-2005,
  author               = {Dose, Christian and Cincotti, Silvano},
  date                 = {2005-09},
  journaltitle         = {Physica A: Statistical Mechanics and its Applications},
  title                = {Clustering of financial time series with application to index and enhanced index tracking portfolio},
  doi                  = {10.1016/j.physa.2005.02.078},
  issn                 = {0378-4371},
  number               = {1},
  pages                = {145--151},
  volume               = {355},
  abstract             = {A stochastic-optimization technique based on time series cluster analysis is described for index tracking and enhanced index tracking problems. Our methodology solves the problem in two steps, i.e., by first selecting a subset of stocks and then setting the weight of each stock as a result of an optimization process (asset allocation). Present formulation takes into account constraints on the number of stocks and on the fraction of capital invested in each of them, whilst not including transaction costs. Computational results based on clustering selection are compared to those of random techniques and show the importance of clustering in noise reduction and robust forecasting applications, in particular for enhanced index tracking.},
  citeulike-article-id = {2251197},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.physa.2005.02.078},
  citeulike-linkout-1  = {http://www.sciencedirect.com/science/article/B6TVG-4G33NP0-4/2/33fd4c2a5caa0ce508e18151530c9250},
  day                  = {1},
  groups               = {Networks and investment management, Network_Invest, PortfOptim_Network, Invest_Network},
  posted-at            = {2017-03-26 18:22:57},
  timestamp            = {2020-02-25 21:14},
}

@MastersThesis{Fucik-2017,
  author               = {Fucik, Vojtech},
  date                 = {2017},
  institution          = {Charles University},
  title                = {Portfolio Construction Using Hierarchical Clustering},
  url                  = {https://dspace.cuni.cz/handle/20.500.11956/91113?locale-attribute=en},
  abstract             = {The main objective of this thesis is to summarize and mainly interconnect the existing methodology on correlation matrix filtering, graph algorithms utilized in the minimum spanning trees, hierarchical clustering and principal components analysis in order to create quantitative investment strategies. Instead of traditional usage of stocks returns series, factor models residuals are utilized. Residuals are then an ultimate input for all the algorithms to arrive at probability of centrality (PoC) - an impure probability where values near 1 signalize high probability of a stock being central in the network. Several investment strategies are created based on PoC and tested on data from major US stock market indices. It cannot be imperatively argued that peripheralbased strategies are always better than central-based strategies. Both central and peripheral-based strategies share high upside profit potential at the cost of high volatility whereas traditional Markowitz's optimization process yields stable profits with moderate upside potential.},
  citeulike-article-id = {14461303},
  groups               = {Network_Invest, PortfOptim_Network, Invest_Network, Vol_Cluster},
  posted-at            = {2017-10-19 20:42:17},
  timestamp            = {2020-02-25 21:14},
}

@Article{Page-Panariello-2018,
  author         = {Page, Sebastien and Panariello, Robert A.},
  date           = {2018-08},
  journaltitle   = {Financial Analysts Journal},
  title          = {When Diversification Fails},
  doi            = {10.2469/faj.v74.n3.3},
  issn           = {0015-{198X}},
  number         = {3},
  pages          = {19--32},
  volume         = {74},
  abstract       = {One of the most vexing problems in investment management is that diversification seems to disappear when investors need it the most. We surmise that many investors still do not fully appreciate the impact of extreme correlations on portfolio efficiency particular, on exposure to loss. We take an in-depth look at what drives the stock-to-credit, stock-to-hedge fund, stock-to-private asset, stock-to-risk factors, and stock-to-bond correlations during tail events. We introduce a data-augmentation technique to improve the robustness of tail correlation estimates. Finally, we discuss implications for multi-asset investing.},
  f1000-projects = {QuantInvest},
  groups         = {Diversified_Invest, Invest_Diversif},
  timestamp      = {2020-02-25 21:15},
}

@Article{Pappas-et-al-2014,
  author       = {Pappas, Scott N. and Bianchi, Robert J. and Drew, Michael E. and Gupta, Rakesh},
  date         = {2012},
  journaltitle = {SSRN e-Print},
  title        = {Risk-Factor Diversification and Portfolio Selection},
  url          = {https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2136827},
  abstract     = {Traditionally, investment portfolios have been constructed with a focus on what asset classes to invest in and how much to invest in each. Recent research, however, has shown that focusing on risk-factor allocations, rather than asset class allocations, can result in better risk-adjusted portfolio performance. The existing literature has focused on simple allocation strategies such as equal-weighted and equal-risk-weighted portfolios.

In addition to these simple allocation techniques, this paper compares the performance using mean-variance analysis, and presents evidence that the outperformance of risk-factor diversification may not be as conclusive as has been previously presented in the literature.

While confirming some of the prior findings on risk-factor diversification, the research shows that previous findings may be subject to strong caveats. Specifically, the evidence suggests that the selection of risk-factors, portfolio selection techniques and time-period have a large impact on performance outcomes.},
  groups       = {Diversified_Invest, Invest_Diversif},
  howpublished = {Available at SSRN: http://ssrn.com/abstract=2136827},
  owner        = {zkgst0c},
  timestamp    = {2020-02-25 21:15},
}

@MastersThesis{Parmentier-2018,
  author      = {Loic Parmentier},
  date        = {2018},
  institution = {Louvain School of Management},
  title       = {Measures of Portfolio Diversification},
  url         = {https://dial.uclouvain.be/memoire/ucl/en/object/thesis%3A14352/datastream/PDF_01/view},
  abstract    = {Diversification is one the main and most important concept in the financial world. It is often said that diversification is the only free lunch in finance. From a qualitative point of view, the concept of diversification is quite clear: a portfolio is well-diversified if shocks in the individual components do not heavily impact on the overall portfolio. Relatively simple to understand then but profoundly difficult to define. Indeed, there is no broadly accepted precise and quantitative definition of diversification.

Over the years, many different measures of diversification have been developed in the literature, each with its pros and cons. In the framework of this thesis, we have chosen to analyze six of them. Because we wanted to confront the weights concentration criterion with the risk minimization criterion, we decided to select measures that are based on the entropy of the weights and others that are based on the sources of risk. Those six different measures are the Shannon's Entropy, the Diversification Delta, the Diversification Ratio, the MarginalRisk Contributions, the Portfolio Diversification Index and the Effective Number of Bets.},
  groups      = {Diversification_Measure, Diversified_Invest, Invest_Diversif},
  timestamp   = {2020-02-25 21:15},
}

@Article{Platanakis-et-al-2017b,
  author               = {Platanakis, Emmanouil and Sakkas, Athanasios and Sutcliffe, Charles},
  date                 = {2017},
  journaltitle         = {SSRN e-Print},
  title                = {Harmful Diversification: Evidence from Alternative Investments},
  url                  = {https://ssrn.com/abstract=2911212},
  abstract             = {Alternative assets have become as important as equities and fixed income in the portfolios of major investors, and so their diversification properties are also important. However, adding five alternative assets (real estate, commodities, hedge funds, emerging markets and private equity) to equity and bond portfolios is shown to be harmful for US investors. We use 19 portfolio models, in conjunction with dummy variable regression, to demonstrate this harm over the 1997-2015 period. This finding is robust to different estimation periods, risk aversion levels, and the use of two regimes. Harmful diversification into alternatives is not primarily due to transactions costs or non-normality, but to estimation risk. This is larger for alternative assets, particularly during the credit crisis which accounts for the harmful diversification of real estate, private equity and emerging markets. Diversification into commodities, and to a lesser extent hedge funds, remains harmful even when the credit crisis is excluded.},
  citeulike-article-id = {14510389},
  citeulike-linkout-0  = {https://papers.ssrn.com/sol3/papers.cfm?abstractid=2911212},
  groups               = {Private_Equity, Diversified_Invest, Invest_Diversif},
  posted-at            = {2017-12-30 14:25:45},
  timestamp            = {2020-02-25 21:15},
}

@Article{Polbennikov-et-al-2010,
  author               = {Polbennikov, Simon and Desclee, Albert and Hyman, Jay},
  date                 = {2010-01},
  journaltitle         = {The Journal of Portfolio Management},
  title                = {Horizon Diversification: Reducing Risk in a Portfolio of Active Strategies},
  doi                  = {10.3905/jpm.2010.36.2.026},
  issn                 = {0095-4918},
  number               = {2},
  pages                = {26--38},
  volume               = {36},
  abstract             = {A primary mechanism for controlling portfolio risk is diversification. Diversification is typically addressed by distributing assets among investment sectors and issuers, preferably with low correlations among their returns, a process that can be called asset diversification. The risk reduction from this type of diversification can be less than expected in the midst of a crisis as correlations increase across market segments.

The authors of this article consider a new approach to managing the active risk profile of a portfolio, an approach that uses active strategies rather than asset allocations as its basic building blocks. The authors show that in this framework, risk reduction is achieved by a combination of two distinct mechanisms asset diversification and signal diversification. Combining alpha strategies based on independent signals can help reduce portfolio risk, even when the returns of the underlying assets are correlated.

One way to achieve signal diversification is by combining strategies with various investment horizons or trading frequencies a technique the authors call horizon diversification. Horizon diversification is an intuitive and robust way to decrease risk in a portfolio of active strategies.},
  citeulike-article-id = {13971833},
  citeulike-linkout-0  = {http://dx.doi.org/10.3905/jpm.2010.36.2.026},
  groups               = {Diversified_Invest, Invest_Diversif},
  owner                = {cristi},
  posted-at            = {2016-03-08 04:23:53},
  timestamp            = {2020-02-25 21:15},
}

@Article{Qian-2011,
  author               = {Qian, Edward},
  date                 = {2011-02},
  journaltitle         = {The Journal of Investing},
  title                = {Risk Parity and Diversification},
  doi                  = {10.3905/joi.2011.20.1.119},
  issn                 = {1068-0896},
  number               = {1},
  pages                = {119--127},
  volume               = {20},
  abstract             = {Traditional 60/40 asset allocation portfolios are not truly diversified because they have an unbalanced risk allocation to high-risk assets. As a result, their expected risk-adjusted returns are low. Risk parity is a new way to construct asset allocation portfolios based on the principle of risk diversification, achieving both higher risk-adjusted returns and higher total returns than traditional asset allocation approaches. The diversification benefits of risk parity portfolios also include balanced correlations to underlying asset classes and stronger downside protection against severe losses. Risk parity portfolios can also incorporate active views on risk-adjusted returns of different asset classes. All of these features make risk parity an attractive alternative to traditional asset allocation approaches.},
  citeulike-article-id = {13970982},
  citeulike-linkout-0  = {http://dx.doi.org/10.3905/joi.2011.20.1.119},
  groups               = {Risk_Budgeting, Diversified_Invest, Invest_Diversif},
  owner                = {cristi},
  posted-at            = {2016-03-07 00:18:12},
  timestamp            = {2020-02-25 21:15},
}

@InCollection{Richard-Roncalli-2015,
  author               = {Richard, Jean-Charles and Roncalli, Thierry},
  booktitle            = {Risk-Based and Factor Investing},
  date                 = {2015},
  title                = {Smart Beta: Managing Diversification of Minimum Variance Portfolios},
  doi                  = {10.1016/b978-1-78548-008-9.50002-2},
  isbn                 = {9781785480089},
  pages                = {31--63},
  publisher            = {Elsevier},
  abstract             = {In this chapter, we consider a new framework for understanding risk-based portfolios (global minimum variance (GMV), equally weighted (EW), equal risk contribution (ERC) and most diversified portfolio (MDP)). This framework is similar to the constrained minimum variance model of Jurczenko et al., but with another definition of the diversification constraint. The corresponding optimization problem can then be solved using the cyclical coordinate descent (CCD) algorithm. This allows us to extend the results of Cazalet et al. and to better understand the trade-off relationships between volatility reduction, tracking error and risk diversification. In particular, we show that the smart beta portfolios differ because they implicitly target different levels of volatility reduction.

We also develop new smart beta strategies by managing the level of volatility reduction and show that they present appealing properties compared to the traditional risk-based portfolios.},
  citeulike-article-id = {13978525},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/b978-1-78548-008-9.50002-2},
  groups               = {Invest_SmartBeta, Diversified_Invest},
  owner                = {cristi},
  posted-at            = {2016-03-12 19:04:23},
  timestamp            = {2020-02-25 21:15},
}

@TechReport{Romahi-Santiago-2012,
  author      = {Romahi, Y. and Santiago, K.},
  date        = {2012},
  institution = {JP Morgan Asset Management},
  title       = {Diversification - Is it still the only Free Lunch? Alternative building blocks for risk parity portfolios},
  url         = {https://am.jpmorgan.com/blobcontent/800/973/1383169203651_11_566.pdf},
  abstract    = {Risk parity has recently garnered significant attention, particularly owing to its strong performance to more traditional approaches of asset allocation in the last decade. This paper seeks to shed some light on this framework and outline the main advantages, while highlighting some of the concerns currently at the forefront of the minds of risk parity investors - namely leveraged positions in fixed income assets at this point in the interest rate cycle as well as the increasing correlation among asset classes.

The premise of risk parity as an approach to strategic asset allocation is based on maximal diversification of beta (or risk premia) as it emphasises the balanced contribution of various risk exposures to overall portfolio risk. One should essentially remain agnostic to return forecasts on the basis that volatility is a much more stable estimate than return.

Much has been made recently of the increasing correlation among asset classes and the increasing difficulty of achieving diversification - particularly at times of crisis arising from systemic risk. A number of recent studies have examined the benefits of factor diversification over asset class diversification.

The difference is subtle because when one refers to asset classes one is also referring to compensated risk premia. These themselves are therefore factors. One can think of equities as a growth factor, Treasuries as a deflation factor and commodities as an inflation factor. However, risk premia go much further than these traditional factors, as argued in a previous J.P. Morgan Asset Management white paper on alternative beta [15].

Indeed, when one focuses on the risk premia, there are a much broader and more orthogonal set of factors of which one can take advantage. In addition to those mentioned, for example, we can also include the equity value premium, the size premium, the forward rate bias and the merger arbitrage premium among others as further risk premia.

The literature is clear that factor diversification is generally more appealing to asset class diversification. Ilmanen and Kizer [8] go further and point out that factor diversification has been more effective, particularly during periods of crisis.

In this paper, extending risk parity in this direction can be seen to address the core concerns around traditional risk parity and can offer a very attractive approach to strategic asset allocation.

In order to demonstrate this, data is included from several periods going back to 1927 and shows that 'factor premium' risk parity consistently outperforms and is stronger to 'asset class' risk parity.},
  groups      = {Diversified_Invest, Invest_Diversif},
  owner       = {Anne},
  timestamp   = {2020-02-25 21:15},
}

@Article{Sharma-Vipul-2018,
  author         = {Sharma, Prateek and Vipul, A},
  date           = {2018},
  journaltitle   = {Managerial and Decision Economics},
  title          = {Improving portfolio diversification: Identifying the right baskets for putting your eggs},
  doi            = {10.1002/mde.2939},
  abstract       = {We measure the economic value of diversification for international multiasset investment strategies. This study implements five existing diversification measures and proposes a novel measure of diversification, the unsystematic risk ratio (URR). Only the URR and the effective number of bets measures predict the future risk-adjusted performance. These relations are robust to the choice of investment horizon and degree of relative risk aversion. The diversification benefits are larger for the frontier and emerging markets than for the developed markets, for multiasset strategies than for single asset class strategies, and for the pre-crisis and post-crisis periods than for the financial crisis period.},
  f1000-projects = {QuantInvest},
  groups         = {Diversification_Measure, Diversified_Invest, Invest_Diversif},
  timestamp      = {2020-02-25 21:15},
}

@Article{Shi-2015,
  author               = {Shi, Xiang},
  date                 = {2015-08},
  journaltitle         = {SSRN e-Print},
  title                = {Marginal Contribution to Risk and Generalized Effective Number of Bets},
  url                  = {https://ssrn.com/abstract=2642408},
  abstract             = {This paper extends Meucci's Effective Number of Bets to general risk measures with heavy-tailed distributions. By diagonalizing the Hessian matrix of a risk measure we are able to extract locally independent marginal contributions to the risk. The Minimal Torsion approach can still be applied to get the local coordinators of the marginal contributions.

We also calculated second derivatives of CVaR. Furthermore, the Hessian of CVaR can be computed efficiently when the underlying distribution belongs to a class of normal mixture distributions.},
  citeulike-article-id = {13926624},
  citeulike-linkout-0  = {http://ssrn.com/abstract=2642408},
  citeulike-linkout-1  = {http://papers.ssrn.com/sol3/Delivery.cfm/SSRNID2642408code2345648.pdf?abstractid=2642408 and mirid=1},
  day                  = {18},
  groups               = {Diversified_Invest},
  howpublished         = {Available at SSRN: ssrn.com/abstract=2642408},
  owner                = {zkgst0c},
  posted-at            = {2016-02-06 04:51:27},
  timestamp            = {2020-02-25 21:15},
}

@Article{Staines-et-al-2016,
  author               = {Staines, Joe and Li, Wei V. and Romahi, Yazann},
  date                 = {2016-08},
  journaltitle         = {The Journal of Index Investing},
  title                = {Dimensions of Diversification},
  doi                  = {10.3905/jii.2016.7.2.119},
  issn                 = {2154-7238},
  number               = {2},
  pages                = {119--127},
  volume               = {7},
  abstract             = {Within the investment industry, diversification now refers to not only the division of capital among a large number of securities but also the avoidance of risk concentration in any of a number of dimensions. Market-capitalization-weighted indexes often fail this requirement. The authors thus argue that although capitalization weighting makes a suitable benchmark, smart beta can provide a way to build indexes more suitable for investment. The authors present a methodology to measure and hence maximize diversification simultaneously across multiple dimensions. They show the practical value of this measure by using it to backtest equity portfolios. This provides an example of how the properties of assets, rather than historical returns, can be used to systematically construct well-diversified portfolios.},
  citeulike-article-id = {14150143},
  citeulike-linkout-0  = {http://dx.doi.org/10.3905/jii.2016.7.2.119},
  groups               = {Diversified_Invest, Effective_Dim_Diversif, Invest_Diversif},
  owner                = {cristi},
  posted-at            = {2016-10-02 02:36:41},
  timestamp            = {2020-02-25 21:15},
}

@Article{Stein-2015a,
  author               = {Stein, Michael},
  date                 = {2015-05},
  journaltitle         = {SSRN e-Print},
  title                = {Limits to Diversification: Tail Risks in Real Estate Portfolios},
  url                  = {https://ssrn.com/abstract=2611905},
  abstract             = {This study addresses real estate's riskiness from a distributional viewpoint. Several studies have found real estate returns to be best modeled with stable paretian distributions. Using NCREIF individual property returns this is confirmed, but the first application of stable distributions to real estate portfolio returns provides evidence that diversification effects ultimately reduce the tailedness and surprisingly drive the tail parameter towards normality. The study further contributes to the literature by showing the importance of a complete view, beyond pure tail parameter considerations. Even in the presence of tail parameters being close to normal, the return risk may still be tremendous, and can only be reduced by diversification effects in property portfolios, and only to a certain time-dependent extent. The results have strong implications for risk managers, fund managers and holders of large commercial real estate portfolios alike.},
  citeulike-article-id = {13997240},
  citeulike-linkout-0  = {http://ssrn.com/abstract=2611905},
  citeulike-linkout-1  = {http://papers.ssrn.com/sol3/Delivery.cfm/SSRNID2691024code1091028.pdf?abstractid=2611905 and mirid=1},
  day                  = {30},
  groups               = {Diversified_Invest, Invest_TailRisk, Invest_RealEstate, Invest_Diversif},
  owner                = {cristi},
  posted-at            = {2016-04-04 22:56:41},
  timestamp            = {2020-02-25 21:15},
}

@InCollection{Takada-Stern-2017,
  author               = {Takada, Hellinton H. and Stern, Julio M.},
  booktitle            = {Publisher Logo Conference Proceedings},
  date                 = {2017},
  title                = {On portfolio risk diversification},
  doi                  = {10.1063/1.4985363},
  location             = {Ghent, Belgium},
  pages                = {070002+},
  url                  = {https://aip.scitation.org/doi/10.1063/1.4985363},
  abstract             = {The first portfolio risk diversification strategy was put into practice by the All Weather fund in 1996. The idea of risk diversification is related to the risk contribution of each available asset class or investment factor to the total portfolio risk. The maximum diversification or the risk parity allocation is achieved when the set of risk contributions is given by a uniform distribution. Meucci (2009) introduced the maximization of the Renyi entropy as part of a leverage constrained optimization problem to achieve such diversified risk contributions when dealing with uncorrelated investment factors. A generalization of the risk parity is the risk budgeting when there is a prior for the distribution of the risk contributions. Our contribution is the generalization of the existent optimization frameworks to be able to solve the risk budgeting problem. In addition, our framework does not possess any leverage constraint.},
  citeulike-article-id = {14520884},
  citeulike-linkout-0  = {http://dx.doi.org/10.1063/1.4985363},
  groups               = {Diversified_Invest, Invest_Diversif},
  posted-at            = {2018-01-22 01:13:17},
  timestamp            = {2020-02-25 21:15},
}

@Article{Theron-vanVuuren-2018,
  author         = {Theron, Ludan and van Vuuren, Gary},
  date           = {2018-01-18},
  journaltitle   = {Cogent Economics \& Finance},
  title          = {The maximum diversification investment strategy: A portfolio performance comparison},
  doi            = {10.1080/23322039.2018.1427533},
  issn           = {2332-2039},
  number         = {1},
  volume         = {6},
  abstract       = {The efficacy of four different portfolio allocation strategies is evaluated according to their absolute returns during different economic conditions over a period of 10 years. A comparison is drawn between the Most Diversified portfolio (MD) and three alternatives; a Minimum Variance portfolio, an Equally-Weighted portfolio and a Tangent (or Maximum Sharpe ratio) portfolio. The aim is to assess portfolio performance using cumulative returns, the Sharpe ratio and the daily volatilities of each portfolio. The four asset allocation methods are governed by multiple constraints. Although previous work has shown that MD portfolios exhibit greater diversification and a higher Sharpe ratio than other investment strategies, this was not found using developed market index data.},
  day            = {18},
  f1000-projects = {QuantInvest},
  groups         = {Diversified_Invest, Invest_Diversif},
  timestamp      = {2020-02-25 21:15},
}

@Article{Vermorken-et-al-2012,
  author               = {Vermorken, Maximilian A. and Medda, Francesca R. and Schroder, Thomas},
  date                 = {2012-10},
  journaltitle         = {The Journal of Portfolio Management},
  title                = {The Diversification Delta: A Higher-MomentMeasure for Portfolio Diversification},
  doi                  = {10.3905/jpm.2012.39.1.067},
  issn                 = {0095-4918},
  number               = {1},
  pages                = {67--74},
  volume               = {39},
  abstract             = {The concept of diversification is central in finance and has become even more so since the 2008 financial crisis.

In this article, the authors introduce a new measure for diversification. The measure, referred to as diversification delta, is nonparametric, based on higher moments, easily interpretable due to its mathematical formulation, and incorporates the advantages of the present measures of diversification while extending them.

The measure is applied to infrastructure returns data in order to understand the benefits of diversifying across various infrastructure classes, gaining useful insights for infrastructure fund managers and investors.},
  citeulike-article-id = {13972052},
  citeulike-linkout-0  = {http://dx.doi.org/10.3905/jpm.2012.39.1.067},
  groups               = {Diversification_Measure, Diversified_Invest, Invest_Diversif},
  owner                = {cristi},
  posted-at            = {2016-03-08 06:20:54},
  timestamp            = {2020-02-25 21:15},
}

@Article{Viceira-et-al-2017,
  author               = {Viceira, Luis M. and Wang, Zixuan and Zhou, John},
  date                 = {2017},
  journaltitle         = {SSRN e-Print},
  title                = {Global Portfolio Diversification for Long-Horizon Investors},
  url                  = {https://ssrn.com/abstract=2941652},
  abstract             = {This paper conducts a theoretical and empirical investigation of the risks of globally diversified portfolios of stocks and bonds and of optimal intertemporal global portfolio choice for long horizon investors in the presence of permanent cash flow shocks and transitory discount rate shocks to asset values. We show that an upward shift in cross-country one-period return correlations resulting from correlated cash flow shocks increases the risk of global portfolios and reduces investors' willingness to hold risky assets at all horizons. However, a similar upward shift in cross-country one-period return correlations resulting from correlated discount rate shocks has a much more muted effect on long-run portfolio risk and on the willingness to long horizon investors to hold risky assets. Correlated cash flow shocks imply that markets tend to move together at all horizons, thus reducing the scope for global diversification for all investors regardless of their investment horizon. By contrast, correlated discount rate shocks imply that markets tend to move together only transitorily and long-horizon investors can still benefit from global portfolios to diversify long-term cash flow risk. We document a secular increase in the cross-country correlations of stock and government bond returns since the late 1990's. We show that for global equities this increase has been driven primarily by increased cross-country correlations of discount rate shocks, or global capital markets integration, while for bonds it has been driven by both global capital markets integration and increased cross-country correlations of inflation shocks that determine the real cash flows of nominal government bonds. Therefore, despite the significant increase in the short-run correlation of global equity markets, the benefits from global equity portfolio diversification have not declined nearly as much for long-horizon investors as they have for short-horizon investors. By contrast, increased correlation of inflation across markets implies that the benefits of global bond portfolio diversification have declined for long-only bond investors at all horizons. However, it also means that the scope for hedging liabilities using global bonds has increased, benefiting investors with long-dated liabilities. Finally, we show that the well documented negative stock-bond correlation in the U.S. since the late 1990's is a global phenomenon, suggesting that the benefits of stock-bond diversification have increased in all developed markets.},
  citeulike-article-id = {14327821},
  citeulike-linkout-0  = {https://papers.ssrn.com/sol3/papers.cfm?abstractid=2941652},
  groups               = {Diversified_Invest, Invest_Diversif},
  posted-at            = {2017-04-03 19:26:42},
  timestamp            = {2020-02-25 21:15},
}

@Article{Pastor-et-al-2017,
  author               = {Pastor, Lubos and Stambaugh, Robert F. and Taylor, Lucian A.},
  date                 = {2017},
  journaltitle         = {SSRN e-Print},
  title                = {Portfolio Liquidity and Diversification: Theory and Evidence},
  url                  = {https://ssrn.com/abstract=3016781},
  abstract             = {A portfolio's liquidity depends not only on the liquidity of its holdings but also on its diversification. We propose simple, theoretically motivated measures of portfolio liquidity and diversification. We also develop an equilibrium model relating portfolio liquidity to fund size, expense ratio, and turnover. As the model predicts, mutual funds with less liquid portfolios have smaller size, higher expense ratios, and lower turnover. The model also yields additional predictions that we verify empirically: larger funds are cheaper, funds that trade less are larger and cheaper, and funds that are too big perform worse. We also find that mutual fund portfolios have become more liquid because both components of diversification, coverage and balance, have trended upward.},
  citeulike-article-id = {14428169},
  citeulike-linkout-0  = {https://papers.ssrn.com/sol3/papers.cfm?abstractid=3016781},
  groups               = {Characteristics and return prediction, Diversification_Measure, Invest_Liquidity, Invest_Diversif},
  posted-at            = {2017-09-09 17:49:42},
  timestamp            = {2020-02-25 21:16},
}

@Article{Rudin-Morgan-2006,
  author               = {Rudin, Alexander M. and Morgan, Jonathan S.},
  date                 = {2006-01},
  journaltitle         = {The Journal of Portfolio Management},
  title                = {A Portfolio Diversification Index},
  doi                  = {10.3905/jpm.2006.611807},
  issn                 = {0095-4918},
  number               = {2},
  pages                = {81--89},
  volume               = {32},
  abstract             = {Despite the importance of diversification in portfolio construction, our current methods of measuring it are inefficient. Construction of a Portfolio Diversification Index (PDI) presents a new way to understand the concept. PDI, which measures the number of unique investments in a portfolio, is useful to assess marginal and cumulative diversification benefits across asset classes and across time. Implementation in hedge fund strategies reveals that various hedge funds offer less diversification than may have been thought, and that there has been reduced diversification in the past several years},
  citeulike-article-id = {14337565},
  citeulike-linkout-0  = {http://dx.doi.org/10.3905/jpm.2006.611807},
  groups               = {Diversification_Measure, Diversified_Invest, Invest_Diversif},
  posted-at            = {2017-04-15 04:50:36},
  timestamp            = {2020-02-25 21:16},
}

@Article{Yu-2014,
  author               = {Yu, Jing-Rung and Lee, Wen-Yi and Chiou, Wan-Jiun P.},
  date                 = {2014-08},
  journaltitle         = {Applied Mathematics and Computation},
  title                = {Diversified portfolios with different entropy measures},
  doi                  = {10.1016/j.amc.2014.04.006},
  issn                 = {0096-3003},
  pages                = {47--63},
  volume               = {241},
  abstract             = {One of the major issues for Markowitz mean-variance model is the errors in estimations cause "corner solutions"and low diversity in the portfolio. In this paper, we compare the mean-variance efficiency, realized portfolio values, and diversity of the models incorporating different entropy measures by applying multiple criteria method. Differing from previous studies, we evaluate twenty-three portfolio over-time rebalancing strategies with considering short-sales and various transaction costs in asset diversification. Using the data of the most liquid stocks in Taiwan's market, our finding shows that the models with Yager's entropy yield higher performance because they respond to the change in market by reallocating assets more effectively than those with Shannon's entropy and with the minimax disparity model. Furthermore, including entropy in models enhances diversity of the portfolios and makes asset allocation more feasible than the models without incorporating entropy.},
  citeulike-article-id = {14310470},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.amc.2014.04.006},
  groups               = {Diversification_Measure, Diversified_Invest, Invest_Diversif},
  posted-at            = {2017-03-14 02:52:48},
  timestamp            = {2020-02-25 21:16},
}

@Article{Amenc-et-al-2017,
  author               = {Amenc, Noel and Ducoulombier, Frederic and Esakia, Mikheil and Goltz, Felix and Sivasubramanian, Sivagaminathan},
  date                 = {2017-03},
  journaltitle         = {The Journal of Portfolio Management},
  title                = {Accounting for Cross-Factor Interactions in Multifactor Portfolios without Sacrificing Diversification and Risk Control},
  doi                  = {10.3905/jpm.2017.43.5.099},
  issn                 = {0095-4918},
  number               = {5},
  pages                = {99--114},
  volume               = {43},
  abstract             = {In this article, the authors compare different approaches for constructing multifactor equity portfolios: bottom-up score-weighting approaches that target high-factor intensity and top-down approaches that also consider diversification objectives. They find that focusing solely on increasing factor intensity leads to inefficiency in capturing factor premia, because exposure to unrewarded risks more than offsets the benefits of increased factor scores. High factor scores in bottom-up approaches also come with high instability and turnover. The authors introduce an approach that considers cross-factor interactions in top-down portfolios through an adjustment at the stock-selection level. While producing lower factor intensity than bottom-up methods, this approach leads to higher levels of diversification and produces higher returns per unit of factor intensity. The authors report that it dominates bottom-up approaches in terms of relative performance, while considerably reducing extreme relative losses and turnover.},
  citeulike-article-id = {14324691},
  citeulike-linkout-0  = {http://dx.doi.org/10.3905/jpm.2017.43.5.099},
  groups               = {Factor_Types, Multifactor_Invest, Invest_Diversif},
  posted-at            = {2017-03-30 14:36:32},
  timestamp            = {2020-02-25 21:26},
}

@Article{Choueifaty-et-al-2013,
  author       = {Yves Choueifaty and Tristan Froidure and Julien Reynier},
  date         = {2013},
  journaltitle = {The Journal of Investment Strategies},
  title        = {Properties of the most diversified portfolio},
  number       = {2},
  pages        = {119--131},
  url          = {https://www.risk.net/journal-of-investment-strategies/2255764/properties-of-the-most-diversified-portfolio},
  volume       = {1},
  abstract     = {This article expands upon Toward Maximum Diversification by Choueifaty and Coignard [2008]. We present new mathematical properties of the Diversification Ratio and Most Diversified Portfolio (MDP), and investigate the optimality of the MDP in a mean-variance framework. We also introduce a set of Portfolio Invariance Properties, providing the basic rules an unbiased portfolio construction process should respect.

The MDP is then compared in light of these rules to popular methodologies (equal weights, equal risk contribution, minimum variance), and their performance is investigated over the past decade, using the MSCI World as reference universe. We believe that the results obtained in this article show that the MDP is a strong candidate for being the un-diversifiable portfolio, and as such delivers investors with the full benefit of the equity premium.},
  groups       = {Invest_Diversif},
  owner        = {zkgst0c},
  timestamp    = {2020-02-25 21:29},
}

@Article{Sorensen-et-al-2018,
  author         = {Sorensen, Eric and Barnes, Mark and Alonso, Nick and Qian, Edward},
  date           = {2018-03-31},
  journaltitle   = {The Journal of Portfolio Management},
  title          = {Not all factor exposures are created equal},
  doi            = {10.3905/jpm.2018.44.4.039},
  issn           = {0095-4918},
  number         = {4},
  pages          = {39--45},
  volume         = {44},
  abstract       = {Approaches to quantitative equity investing have evolved markedly. Thirty years ago, the focus was on alpha generation, but with the recent decade acceptance of smart (alternative) beta, the focus is turning to transparent methods of construction for factor investing. In this article, the authors present an approach for evaluating various methods of portfolio construction that lead to the same factor exposures. Four portfolios are of interest: factor weighted, cap weighted, equal weighted, and risk parity weighted. The authors compare these portfolios based on standard performance statistics as well as new metrics of value-added, such as performance participation rates and portfolio sector concentrations. The results indicate that once the desired factor exposure is achieved, it is beneficial to build the portfolio with the most desirable characteristics in terms of diversification.},
  day            = {31},
  f1000-projects = {QuantInvest},
  groups         = {Invest_SmartBeta},
  timestamp      = {2020-02-25 21:32},
}

@Article{Bardoscia-et-al-2019,
  author         = {Bardoscia, Marco and {d'Arienzo}, Daniele and Marsili, Matteo and Volpati, Valerio},
  date           = {2019},
  journaltitle   = {SSRN e-Print},
  title          = {Lost in Diversification},
  doi            = {10.2139/ssrn.3323440},
  issn           = {1556-5068},
  url            = {https://ssrn.com/abstract=3323440},
  abstract       = {As financial instruments grow in complexity more and more information is neglected by risk optimization practices. This brings down a curtain of opacity on the origination of risk, that has been one of the main culprits in the 2007-2008 global financial crisis. We discuss how the loss of transparency may be quantified in bits, using information theoretic concepts. We find that i) financial transformations imply large information losses, ii) portfolios are more information sensitive than individual stocks only if fundamental analysis is sufficiently informative on the co-movement of assets, that iii) securitisation, in the relevant range of parameters, yields assets that are less information sensitive than the original stocks and that iv) when diversification (or securitisation) is at its best (i.e. when assets are uncorrelated) information losses are maximal. We also address the issue of whether pricing schemes can be introduced to deal with information losses. This is relevant for the transmission of incentives to gather information on the risk origination side. Within a simple mean variance scheme, we find that market incentives are not generally sufficient to make information harvesting sustainable.},
  f1000-projects = {QuantInvest},
  groups         = {Invest_Diversif},
  timestamp      = {2020-02-25 21:34},
}

@Article{Bennyhoff-2009,
  author               = {Bennyhoff, Donald G.},
  date                 = {2009-02},
  journaltitle         = {The Journal of Investing},
  title                = {Time Diversification and Horizon-Based Asset Allocations},
  doi                  = {10.3905/joi.2009.18.1.045},
  issn                 = {1068-0896},
  number               = {1},
  pages                = {45--52},
  volume               = {18},
  abstract             = {Time diversification, the concept that investments in stocks are less risky over longer periods than shorter ones, has been the subject of spirited debate for decades. Over the last few years the growing acceptance of life cycle investment products, such as target retirement mutual funds, has renewed interest in this topic. The objective of this article is not to prove or disprove time diversification, but to evaluate whether the concept must be valid for a horizon-based asset allocation framework to be viable and appropriate. Our findings suggest that there is little evidence to support the notion that time moderates the perceived volatility inherent in risky assets. However, we would expect the risk/reward relationships of the past to prevail in the future, and if that is the case, a longer investment horizon may support a willingness and ability to assume the greater uncertainty of equity-centric asset allocations. This may be true particularly for younger investors for whom the allocation to human capital and the risk posed by the erosion of purchasing power by inflation can reasonably be assumed to be greatest.},
  citeulike-article-id = {14322263},
  citeulike-linkout-0  = {http://dx.doi.org/10.3905/joi.2009.18.1.045},
  groups               = {Human_Capital, Invest_Diversif},
  posted-at            = {2017-03-29 07:02:30},
  timestamp            = {2020-02-25 21:35},
}

@Article{Dees-et-al-2019,
  author         = {Dees, Bruno Scalzo and Stankovic, Ljubisa and Constantinides, Anthony G. and Mandic, Danilo P.},
  date           = {2019-10-12},
  journaltitle   = {arXiv e-Print},
  title          = {Portfolio Cuts: A Graph-Theoretic Framework to Diversification},
  url            = {https://arxiv.org/abs/1910.05561},
  urldate        = {2019-10-24},
  abstract       = {Investment returns naturally reside on irregular domains, however, standard multivariate portfolio optimization methods are agnostic to data structure. To this end, we investigate ways for domain knowledge to be conveniently incorporated into the analysis, by means of graphs. Next, to relax the assumption of the completeness of graph topology and to equip the graph model with practically relevant physical intuition, we introduce the portfolio cut paradigm. Such a graph-theoretic portfolio partitioning technique is shown to allow the investor to devise robust and tractable asset allocation schemes, by virtue of a rigorous graph framework for considering smaller, computationally feasible, and economically meaningful clusters of assets, based on graph cuts. In turn, this makes it possible to fully utilize the asset returns covariance matrix for constructing the portfolio, even without the requirement for its inversion. The advantages of the proposed framework over traditional methods are demonstrated through numerical simulations based on real-world price data.},
  day            = {12},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-25 21:35},
}

@Article{Sebastian-Gebbie-2019,
  author         = {Sebastian, Ann and Gebbie, Tim},
  date           = {2019-10-12},
  journaltitle   = {arXiv e-Print},
  title          = {Systematic Asset Allocation using Flexible Views for South African Markets},
  url            = {https://arxiv.org/abs/1910.05555},
  urldate        = {2019-10-24},
  abstract       = {We implement a systematic asset allocation model using the Historical Simulation with Flexible Probabilities (HS-FP) framework developed by Meucci. The HS-FP framework is a flexible non-parametric estimation approach that considers future asset class behavior to be conditional on time and market environments, and derives a forward looking distribution that is consistent with this view while remaining close as possible to the prior distribution. The framework derives the forward looking distribution by applying unequal time and state conditioned probabilities to historical observations of asset class returns. This is achieved using relative entropy to find estimates with the least distortion to the prior distribution. Here, we use the HS-FP framework on South African financial market data for asset allocation purposes; by estimating expected returns, correlations and volatilities that are better represented through the measured market cycle. We demonstrated a range of state variables that can be useful towards understanding market environments. Concretely, we compare the out-of-sample performance for a specific configuration of the HS-FP model relative to classic Mean Variance Optimization(MVO) and Equally Weighted (EW) benchmark models. The framework displays low probability of backtest overfitting and the out-of-sample net returns and Sharpe ratio point estimates of the HS-FP model outperforms the benchmark models. However, the results are inconsistent when training windows are varied, the Sharpe ratio is seen to be inflated, and the method does not demonstrate statistically significant out-performance on a gross and net basis.},
  day            = {12},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-25 21:36},
}

@Article{Barkhagen-et-al-2019,
  author         = {Barkhagen, Mathias and Fleming, Brian and Quiles, Sergio Garcia and Gondzio, Jacek and Kalcsics, Joerg and Kroeske, Jens and Sabanis, Sotirios and Staal, Arne},
  date           = {2019-06-03},
  journaltitle   = {arXiv e-Print},
  title          = {Optimising portfolio diversification and dimensionality},
  url            = {https://arxiv.org/abs/1906.00920},
  urldate        = {2019-10-02},
  abstract       = {A new framework for portfolio diversification is introduced which goes beyond the classical mean-variance approach and portfolio allocation strategies such as risk parity. It is based on a novel concept called portfolio dimensionality that connects diversification to the non-Gaussianity of portfolio returns and can typically be defined in terms of the ratio of risk measures which are homogenous functions of equal degree. The latter arises naturally due to our requirement that diversification measures should be leverage invariant. We introduce this new framework and argue the benefits relative to existing measures of diversification in the literature, before addressing the question of optimizing diversification or, equivalently, dimensionality. Maximising portfolio dimensionality leads to highly non-trivial optimization problems with objective functions which are typically non-convex and potentially have multiple local optima. Two complementary global optimization algorithms are thus presented. For problems of moderate size and more akin to asset allocation problems, a deterministic Branch and Bound algorithm is developed, whereas for problems of larger size a stochastic global optimization algorithm based on Gradient Langevin Dynamics is given. We demonstrate analytically and through numerical experiments that the framework reflects the desired properties often discussed in the literature.},
  day            = {3},
  f1000-projects = {QuantInvest},
  groups         = {Invest_Diversif},
  timestamp      = {2020-02-25 21:41},
}

@Article{Meucci-et-al-2015,
  author       = {Attilio Meucci and Alberto Santangelo and Romain Deguest},
  date         = {2015},
  journaltitle = {Risk Magazine},
  title        = {Risk budgeting and diversification based on optimised uncorrelated factors},
  url          = {https://www.risk.net/risk-management/2433224/risk-budgeting-and-diversification-based-on-optimised-uncorrelated-factors},
  abstract     = {We measure the contributions to risk of a set of factors, strategies, or investments, based on "Minimum-Torsion Bets", namely a set of uncorrelated factors, optimized to closely track the factors used to allocate the portfolio. We then introduce a novel definition of contributions to risk, which generalizes the "marginal contributions to risk", traditionally used in banks for risk budgeting and in asset management to build risk parity strategies.

The Minimum-Torsion Bets allow us to also introduce a natural diversification score, the Effective Number of Minimum-Torsion Bets, which we use to measure and manage diversification.

We discuss the advantages of the Minimum-Torsion Bets over the traditional approach to diversification based on marginal contributions to risk. We present two case studies, a security-based investment in the stocks of the S\&P 500, and a factor-based investment in the five Fama-French factors.},
  groups       = {Risk_Budgeting, Invest_Risk, Invest_Diversif},
  owner        = {zkgst0c},
  timestamp    = {2020-02-25 21:43},
}

@Article{Jennings-Payne-2016,
  author               = {Jennings, William W. and Payne, Brian C.},
  date                 = {2016-03},
  journaltitle         = {Financial Analysts Journal},
  title                = {Fees Eat Diversification's Lunch},
  doi                  = {10.2469/faj.v72.n2.1},
  issn                 = {0015-198X},
  number               = {2},
  pages                = {31--40},
  volume               = {72},
  abstract             = {Although diversification is often spoken of as the only free lunch in investing, the authors show that it is not free and that it must be considered in light of its costs. They also show that fees on diversifying asset classes are high relative to their risk-adjusted diversification benefit, with the more exotic asset classes carrying higher price tags. Because there is meaningful cross-sectional variation, fees need to be considered when making strategic asset allocation decisions.},
  citeulike-article-id = {14150315},
  citeulike-linkout-0  = {http://dx.doi.org/10.2469/faj.v72.n2.1},
  groups               = {Invest_Diversif},
  owner                = {cristi},
  posted-at            = {2016-10-02 16:52:21},
  timestamp            = {2020-02-25 21:45},
}

@Article{Kritzman-2015,
  author               = {Kritzman, Mark},
  date                 = {2015-01},
  journaltitle         = {Financial Analysts Journal},
  title                = {What Practitioners Need to Know ... About Time Diversification (corrected)},
  doi                  = {10.2469/faj.v71.n1.4},
  issn                 = {0015-198X},
  number               = {1},
  pages                = {29--34},
  volume               = {71},
  abstract             = {Although an investor may be less likely to lose money over a long horizon than over a short horizon, the magnitude of a potential loss increases with the length of the investment horizon.},
  citeulike-article-id = {14514123},
  citeulike-linkout-0  = {http://dx.doi.org/10.2469/faj.v71.n1.4},
  groups               = {Invest_Diversif},
  posted-at            = {2018-01-09 16:47:05},
  timestamp            = {2020-02-25 21:46},
}

@Article{Chollete-et-al-2011,
  author       = {Chollete, L. and de la Pena, V. and Lu, C.},
  date         = {2011},
  journaltitle = {Journal of Banking and Finance},
  title        = {International Diversification: A Copula Approach},
  number       = {2},
  pages        = {403--417},
  url          = {https://www.sciencedirect.com/science/article/pii/S0378426610003298},
  volume       = {35},
  abstract     = {The viability of international diversification involves balancing benefits and costs. This balance hinges on the degree of asset dependence. In light of theoretical research linking diversification and dependence, we examine international diversification using two measures of dependence: correlations and copulas.

We document several findings.

First, dependence has increased over time.

Second, we find evidence of asymmetric dependence or downside risk in Latin America, but less in the G5. The results indicate very little downside risk in East Asia.

Third, East Asian and Latin American returns exhibit some correlation complexity. Interestingly, the regions with maximal dependence or worst diversification do not command large returns. Our results suggest international limits to diversification. They are also consistent with a possible tradeoff between international diversification and systemic risk.},
  groups       = {Diversification_Measure, Asymm_Dependence},
  owner        = {zkgst0c},
  timestamp    = {2020-02-25 21:47},
}

@Article{Carmichael-et-al-2015a,
  author         = {Carmichael, Benoit and Koumou, Gilles and Moran, Kevin},
  date           = {2015-09},
  journaltitle   = {SSRN e-Print},
  title          = {A New Formulation of Maximum Diversification Indexation Using Rao's Quadratic Entropy},
  url            = {https://ssrn.com/abstract=2923220},
  abstract       = {This paper proposes a new formulation of the Maximum Diversification indexation strategy based on Rao Quadratic Entropy (RQE). It clarifies the investment problem underlying the Most Diversified Portfolio (MDP) formed with this strategy, identifies the source of the MDP out-of-sample performance, and suggests dimensions along which this performance can be improved. We show that these potential improvements are quantitatively important and are robust to portfolio turnover, portfolio risk, estimation window, and covariance matrix estimation.},
  f1000-projects = {QuantInvest},
  groups         = {Diversified_Invest, Invest_Diversif},
  timestamp      = {2020-02-25 21:52},
}

@Conference{Lee-2013,
  author    = {Wai Lee},
  booktitle = {Second Annual Inside Indexing Conference},
  date      = {2013},
  title     = {Risk Based Asset Allocation},
  url       = {https://pdfs.semanticscholar.org/6101/ee13a5ac3a2387441351be7ffd03b6a8a9d1.pdf},
  abstract  = {In recent years, we have witnessed an alarmingly large and growing amount of literature on portfolio construction approaches focused on risks and diversification rather than estimating expected returns. Numerous simulations, applied to different universes, have been documented in support of these approaches based on their apparent outperformance versus passive market-capitalization weighting or static, fixed weight portfolios. Many studies attribute the better performance of these risk-based asset allocation approaches to superior diversification.

Given the absence of clearly defined investment objective functions behind these approaches as well as the metrics used by these studies to evaluate ex-post performance, we put these approaches into the same context of mean-variance efficiency in an attempt to understand their theoretical underpinnings. In doing so, we hope to shed some light on what these approaches attempt to achieve and on the characteristics of the investment universe, if indeed these approaches are meant to approximate meanvariance efficiency.

Rather than adding to the already large collection of simulation results, we use some simple examples to compare and contrast the portfolio and risk characteristics of these approaches. We also reiterate that any portfolio that deviates from the market capitalization-weighted portfolio is an active portfolio.

Finally, we conclude there is no theory to predict, ex-ante, that any of these riskbased approaches should outperform.},
  groups    = {Diversification_Measure},
  owner     = {zkgst0c},
  timestamp = {2020-02-25 21:52},
}

@Article{Cesarone-et-al-2019,
  author         = {Cesarone, Francesco and Scozzari, Andrea and Tardella, Fabio},
  date           = {2019-07-25},
  journaltitle   = {Journal of Global Optimization},
  title          = {An optimization-diversification approach to portfolio selection},
  doi            = {10.1007/s10898-019-00809-7},
  issn           = {0925-5001},
  urldate        = {2019-09-10},
  abstract       = {The classical approaches to optimal portfolio selection call for finding a feasible portfolio that optimizes a risk measure, or a gain measure, or a combination thereof by means of a utility function or of a performance measure. However, the optimization approach tends to amplify the estimation errors on the parameters required by the model, such as expected returns and covariances. For this reason, the Risk Parity model, a novel risk diversification approach to portfolio selection, has been recently theoretically developed and used in practice, mainly for the case of the volatility risk measure. Here we first provide new theoretical results for the Risk Parity approach for general risk measures. Then we propose a novel framework for portfolio selection that combines the diversification and the optimization approaches through the global solution of a hard nonlinear mixed integer or pseudo Boolean problem. For the latter problem we propose an efficient and accurate Multi-Greedy heuristic that extends the classical single-threaded greedy approach to a multiple-threaded setting. Finally, we provide empirical results on real-world data showing that the diversified optimal portfolios are only slightly suboptimal in-sample with respect to optimal portfolios, and generally show improved out-of-sample performance with respect to their purely diversified or purely optimized counterparts.},
  day            = {25},
  f1000-projects = {QuantInvest},
  groups         = {Invest_Diversif},
  timestamp      = {2020-02-25 21:53},
}

@Article{Darnell-2009,
  author       = {Max Darnell},
  date         = {2009},
  journaltitle = {SSRN e-Print},
  title        = {Did Diversification Fail?},
  url          = {https://www.firstquadrant.com/system/files/2009_10_Did_Diversification_Fail_0.pdf},
  abstract     = {Many investors had felt that by spreading their investments across many asset classes - by investing in a wide array of betas - that they would avoid the risk of an across-theboard decline in their investments. They thought that they had avoided the problem associated with putting all their eggs in one basket as the adage advises. When most assets did fall together in largely simultaneous fashion in the midst of the recent credit crisis, investors rated diversification a failure, and cried out in frustration that correlations had all converged on one. Diversification failed this year, 1 was the title of a New York Times article in the business section in November last year. In another, more recent article,2 one of the large university endowments explained that diversification had failed to protect its asset values. This sentiment was, and is, entirely common. If their eggs were all in different baskets, then it would appear that they were somehow all tied together sharing a common fate when their fates were assumed to have been independent of one another instead. There are several aspects of this that are wrong. Diversification didn't fail; the metaphor of eggs in different baskets doesn't accurately capture the purpose of diversification; and those weren't betas that they diversified across},
  groups       = {Invest_Diversif},
  owner        = {zkgst0c},
  timestamp    = {2020-02-25 21:54},
}

@InCollection{deCarvalho-et-al-2017a,
  author               = {{de Carvalho}, Raul L. and Lu, Xiao and Soupe, Francois and Dugnolle, Patrick},
  booktitle            = {Factor Investing},
  date                 = {2017},
  title                = {Diversify and Purify Factor Premiums in Equity Markets},
  doi                  = {10.1016/b978-1-78548-201-4.50004-0},
  isbn                 = {9781785482014},
  pages                = {73--97},
  publisher            = {Elsevier},
  abstract             = {In this chapter, we consider the question of how to improve the efficacy of strategies designed to capture factor premiums in equity markets and, in particular, from the value, quality, low-risk and momentum factors. We consider a number of portfolio construction approaches designed to capture factor premiums with the appropriate levels of risk controls aiming at increasing information ratios. We show that information ratios can be increased by targeting constant volatility (CV) over time, hedging market beta (HB) and hedging exposures to the size factor, i.e. neutralizing biases in the market capitalization of stocks used in factor strategies. With regard to the neutralization of sector exposures, we find this to be of particular importance for the value and low-risk factors. Finally, we look at the added value of shorting stocks in factor strategies. We find that with few exceptions the contributions to performance from the short leg are inferior to those from the long leg. Thus, long-only strategies can be efficient alternatives to capture these factor premiums. Finally, we find that factor premiums tend to have fatter tails than what could be expected from a Gaussian distribution of returns, but that skewness is not significantly negative in most cases.},
  citeulike-article-id = {14499092},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/b978-1-78548-201-4.50004-0},
  groups               = {Invest_Factor, Invest_Risk, Factor_Types, Invest_Diversif},
  posted-at            = {2017-12-08 00:48:48},
  timestamp            = {2020-02-25 21:55},
}

@Article{DeMiguel-et-al-2009a,
  author       = {DeMiguel, V. and Garlappi, L. and Uppal, R.},
  date         = {2009},
  journaltitle = {Review of Financial Studies},
  title        = {Optimal versus Naive Diversification: how Inefficient is the 1/N Portfolio Strategy},
  doi          = {10.1093/rfs/hhm075},
  pages        = {1915--1953},
  url          = {Optimal versus Naive Diversification: how Inefficient is the 1/N
Portfolio Strategy},
  volume       = {22},
  abstract     = {We evaluate the out-of-sample performance of the sample-based mean-variance model, and its extensions designed to reduce estimation error, relative to the naive 1/N portfolio. Of the 14 models we evaluate across seven empirical datasets, none is consistently better than the 1/N rule in terms of Sharpe ratio, certainty-equivalent return, or turnover, which indicates that, out of sample, the gain from optimal diversification is more than offset by estimation error. Based on parameters calibrated to the US equity market, our analytical results and simulations show that the estimation window needed for the sample-based mean-variance strategy and its extensions to outperform the 1/N benchmark is around 3000 months for a portfolio with 25 assets and about 6000 months for a portfolio with 50 assets. This suggests that there are still many miles to go before the gains promised by optimal portfolio choice can actually be realized out of sample.},
  groups       = {Invest_Diversif},
  owner        = {zkgst0c},
  timestamp    = {2020-02-25 21:55},
}

@Article{Domian-et-al-2007,
  author               = {Domian, Dale L. and Louton, David A. and Racine, Marie D.},
  date                 = {2007-11},
  journaltitle         = {Financial Review},
  title                = {Diversification in Portfolios of Individual Stocks: 100 Stocks Are Not Enough},
  doi                  = {10.1111/j.1540-6288.2007.00183.x},
  issn                 = {0732-8516},
  number               = {4},
  pages                = {557--570},
  volume               = {42},
  abstract             = {We examine returns and ending wealth in portfolios selected from 1,000 large U.S. stocks over a 20-year holding period. Shortfall risk, the possibility of ending wealth being below a target, is a useful metric for long horizon investors and is consistent with the Safety First criterion. Density functions obtained from simulations illustrate that shortfall risk reduction continues as portfolio size is increased, even above 100 stocks. A slightly lower risk can be achieved in small portfolios by diversifying across industries, but a greater reduction is obtained by simply increasing the number of stocks.},
  citeulike-article-id = {1948681},
  citeulike-linkout-0  = {http://dx.doi.org/10.1111/j.1540-6288.2007.00183.x},
  citeulike-linkout-1  = {http://www.ingentaconnect.com/content/bpl/fire/2007/00000042/00000004/art00004},
  day                  = {1},
  groups               = {Invest_Diversif},
  owner                = {cristi},
  posted-at            = {2016-11-07 04:12:43},
  publisher            = {Blackwell Publishing Inc},
  timestamp            = {2020-02-25 21:56},
}

@Article{Fouquau-et-al-2018,
  author         = {Fouquau, Julien and Kharoubi, Cecile and Spieser, Philippe},
  date           = {2018},
  journaltitle   = {The Journal of Risk},
  title          = {International and temporal diversifications: the best of both worlds?},
  doi            = {10.21314/{JOR}.2018.382},
  issn           = {1465-1211},
  url            = {https://www.risk.net/journal-of-risk/5472731/international-and-temporal-diversifications-the-best-of-both-worlds},
  urldate        = {2019-05-30},
  f1000-projects = {QuantInvest},
  groups         = {Invest_Diversif},
  timestamp      = {2020-02-25 21:57},
}

@Article{Hallerbach-2017,
  author               = {Hallerbach, Winfried G.},
  date                 = {2017-07},
  journaltitle         = {The Journal of Wealth Management},
  title                = {If You Have Said A, You Must Also Say B: Calculating Diversified Asset Returns},
  doi                  = {10.3905/jwm.2017.20.2.076},
  issn                 = {1534-7524},
  number               = {2},
  pages                = {76--81},
  volume               = {20},
  abstract             = {The bottom-up route to portfolio diversification is clear: Combining individual assets into a portfolio will lower portfolio risk (especially when correlations are low). The top-down route of evaluating individual assets from the perspective of the diversified portfolio is widely applied in risk budgeting but is neglected in return attributions. Consequently, many investors evaluate individual assets on the basis of their undiversified returns instead of including the diversification benefits they offer. This perspective biases the evaluation of high-volatility/low-correlation assets in the portfolio. In this note, we highlight the importance of evaluating diversified returns and show how we can calculate these returns. We illustrate the method for a U.S. asset portfolio.},
  citeulike-article-id = {14402574},
  citeulike-linkout-0  = {http://dx.doi.org/10.3905/jwm.2017.20.2.076},
  groups               = {Invest_Diversif},
  posted-at            = {2017-07-30 07:46:31},
  timestamp            = {2020-02-25 21:58},
}

@Article{Humphrey-et-al-2015,
  author               = {Humphrey, Jacquelyn E. and Benson, Karen L. and Low, Rand K. Y. and Lee, Wei-Lun},
  date                 = {2015-11},
  journaltitle         = {Pacific-Basin Finance Journal},
  title                = {Is diversification always optimal?},
  doi                  = {10.1016/j.pacfin.2015.09.003},
  issn                 = {0927-538X},
  pages                = {521--532},
  volume               = {35},
  abstract             = {Should retirement savers diversify across many funds or consolidate into one fund? We examine Australian retirement savings. Theoretically, diversification across funds is the optimal strategy. With real-world short-selling constraints, investment in a single fund is optimal. Finance theory and recent literature suggest that investors should diversify their retirement savings across a number of funds. However, the Australian government encourages investors to consolidate retirement savings into just one fund. Using a number of optimization techniques, we investigate which of these two actions would result in the best outcome for investors in terms of risk and return. We find that in the majority of cases investors would be better off not diversifying their holdings; mainly because superannuation funds cannot be short sold. Consolidation therefore does appear to be the optimal strategy for the average superannuation investor.},
  citeulike-article-id = {14160270},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.pacfin.2015.09.003},
  groups               = {Invest_Diversif},
  owner                = {zkgst0c},
  posted-at            = {2016-10-12 20:57:58},
  timestamp            = {2020-02-25 21:59},
}

@Article{Jacobs-et-al-2014,
  author               = {Jacobs, Heiko and M uller, Sebastian and Weber, Martin},
  date                 = {2014-06},
  journaltitle         = {Journal of Financial Markets},
  title                = {How should individual investors diversify? An empirical evaluation of alternative asset allocation policies},
  doi                  = {10.1016/j.finmar.2013.07.004},
  issn                 = {1386-4181},
  pages                = {62--85},
  volume               = {19},
  abstract             = {For global equity diversification, prominent Markowitz extensions do not outperform several heuristic weighting schemes (1/N heuristic, market value-weighting and GDP-weighting). Comparing the different heuristic stock weighting schemes, the value-weighted heuristic performs worse than the GDP-weighted global stock portfolio. Diversification gains in the asset allocation context are mainly driven by a well-balanced allocation over different asset classes. Consistent with global equity diversification, Markowitz-based optimization methods do not add significant value when allocating across different asset classes. This paper evaluates numerous diversification strategies as a possible remedy against widespread costly investment mistakes of individual investors. Our results reveal that a very broad range of simple heuristic allocation schemes offers similar diversification gains as well-established or recently developed portfolio optimization approaches. This holds true for both international diversification in the stock market and diversification over different asset classes. We thus suggest easy-to-implement allocation guidelines for individual investors.},
  citeulike-article-id = {13987903},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.finmar.2013.07.004},
  groups               = {Invest_Diversif},
  owner                = {cristi},
  posted-at            = {2016-03-25 08:36:39},
  timestamp            = {2020-02-25 21:59},
}

@Article{McKay-et-al-2017,
  author               = {McKay, Shawn and Shapiro, Robert and Thomas, Ric},
  date                 = {2017-11},
  journaltitle         = {Financial Analysts Journal},
  title                = {What Free Lunch? The Costs of Overdiversification},
  doi                  = {10.2469/faj.v74.n1.2},
  issn                 = {0015-198X},
  pages                = {1--15},
  abstract             = {Institutional investors, charged with outperforming a policy benchmark, often allocate to external active managers in order to hit their return objective. The challenge is to do so without overdiversifying the plan. Hiring too many managers can significantly reduce active risk, leaving the plan with high fees and limited ability to outperform a policy benchmark. We review the number of external investment strategies held by the largest US public and corporate pension funds. Our analysis shows that most large pension funds are overdiversified, allowing us to suggest a simpler framework for moving forward.},
  citeulike-article-id = {14485296},
  citeulike-linkout-0  = {http://dx.doi.org/10.2469/faj.v74.n1.2},
  day                  = {20},
  groups               = {BenchmarkInvest, Invest_Diversif},
  posted-at            = {2017-11-28 18:26:03},
  timestamp            = {2020-02-25 22:00},
}

@Article{Page-Taborsky-2011,
  author       = {Sebastien Page and Mark A. Taborsky},
  date         = {2011},
  journaltitle = {The Journal of Portfolio Management},
  title        = {The myth of diversification: risk factors versus asset classes},
  number       = {4},
  pages        = {1--2},
  url          = {https://jpm.pm-research.com/content/36/1/26},
  volume       = {37},
  abstract     = {In our New Normal world, regime shifts in economic conditions will continue to cause significant challenges for risk management and portfolio construction. On average, correlations across risk factors are lower than correlations across asset classes, and risk factor correlations tend to be more robust to regime shifts. Risk factors provide a flexible language with which investors may express their forward-looking economic views, adapt to regime shifts and diversify their portfolios accordingly.},
  groups       = {Invest_Diversif},
  owner        = {zkgst0c},
  timestamp    = {2020-02-25 22:01},
}

@Article{Pittman-et-al-2019,
  author         = {Pittman, Sam and Singh, Amneet and Srinivasan, Sangeetha},
  date           = {2019-09-07},
  journaltitle   = {The Journal of Wealth Management},
  title          = {Diversification benefits, where art thou?},
  doi            = {10.3905/jwm.2019.1.081},
  issn           = {1534-7524},
  pages          = {jwm.2019.1.081},
  urldate        = {2019-09-28},
  abstract       = {Following the global financial crisis, a portfolio concentrated in US large cap equity and aggregate fixed income has provided higher returns than diversified portfolios through 2019. Such a prolonged experience causes investors to question the benefits of diversification. This leads us to use a longer history of data across 15 asset classes to understand the historical benefits of diversifying a portfolio with international equity, real assets, and below investment grade fixed income. Our results portray the frequency and magnitude of risk-adjusted return improvement coming from different diversifying asset classes over five-year holding periods. We find that certain asset classes, such as below investment grade fixed income, regularly improve risk-adjusted return of the portfolio, while other asset classes like commodities improve risk-adjusted returns less frequently. Further, we observe that some asset classes do not deliver meaningful risk-adjusted return improvements in the presence of other asset classes. Our conclusion is that investors should continue to build diversified portfolios, but in doing so they should consider that some asset classes more consistently improved risk-adjusted returns than others.},
  day            = {7},
  f1000-projects = {QuantInvest},
  groups         = {Invest_Diversif},
  timestamp      = {2020-02-25 22:01},
}

@Article{Pourbabaee-et-al-2016,
  author               = {Pourbabaee, Farzad and Kwak, Minsuk and Pirvu, Traian A.},
  date                 = {2016-09},
  journaltitle         = {Quantitative Finance},
  title                = {Risk minimization and portfolio diversification},
  doi                  = {10.1080/14697688.2015.1115891},
  number               = {9},
  pages                = {1325--1332},
  volume               = {16},
  citeulike-article-id = {14150627},
  citeulike-linkout-0  = {http://dx.doi.org/10.1080/14697688.2015.1115891},
  citeulike-linkout-1  = {http://www.tandfonline.com/doi/abs/10.1080/14697688.2015.1115891},
  day                  = {1},
  groups               = {Invest_Diversif},
  owner                = {cristi},
  posted-at            = {2016-10-03 01:18:37},
  publisher            = {Routledge},
  timestamp            = {2020-02-25 22:02},
}

@Article{Vandenbroucke-2019,
  author         = {Vandenbroucke, Jurgen},
  date           = {2019-05-09},
  journaltitle   = {The Journal of Investing},
  title          = {Adaptive Portfolios and the Power of Diversification},
  url            = {https://joi.iijournals.com/content/early/2019/05/09/joi.2019.1.089},
  urldate        = {2019-05-09},
  abstract       = {The article gives a qualitative description of an advisory or discretionary investment process that manages the emotional aspect of investing. Portfolios are adaptive, meaning they automatically adjust their allocation in response to changing market conditions. The adjustments are model-based and transparent, and align in terms of frequency and magnitude with the investor’s emotionality. The process looks beyond the risk-focused paradigm in relation to investor profiling, product positioning, and portfolio construction. First, investor profiles distinguish between the attitude toward risk and the attitude toward loss. Second, products differentiate in terms of variance and in terms of skewness. Finally, adaptive portfolios represent a client centric combination of products that lifts the power of diversification to a higher level and ultimately contributes to long term buy-and-hold investor behavior.},
  day            = {9},
  f1000-projects = {QuantInvest},
  groups         = {Invest_Diversif},
  publisher      = {Institutional Investor Journals Umbrella},
  timestamp      = {2020-02-25 22:03},
}

@Article{Adachi-Trendafilov-2017,
  author               = {Adachi, Kohei and Trendafilov, NickolayT},
  date                 = {2017},
  journaltitle         = {Advances in Data Analysis and Classification},
  title                = {Sparsest factor analysis for clustering variables: a matrix decomposition approach},
  doi                  = {10.1007/s11634-017-0284-z},
  pages                = {1--27},
  abstract             = {We propose a new procedure for sparse factor analysis (FA) such that each variable loads only one common factor. Thus, the loading matrix has a single nonzero element in each row and zeros elsewhere. Such a loading matrix is the sparsest possible for certain number of variables and common factors. For this reason, the proposed method is named sparsest FA (SSFA). It may also be called FA-based variable clustering, since the variables loading the same common factor can be classified into a cluster. In SSFA, all model parts of FA (common factors, their correlations, loadings, unique factors, and unique variances) are treated as fixed unknown parameter matrices and their least squares function is minimized through specific data matrix decomposition. A useful feature of the algorithm is that the matrix of common factor scores is re-parameterized using QR decomposition in order to efficiently estimate factor correlations. A simulation study shows that the proposed procedure can exactly identify the true sparsest models. Real data examples demonstrate the usefulness of the variable clustering performed by SSFA.},
  citeulike-article-id = {14433260},
  citeulike-linkout-0  = {http://dx.doi.org/10.1007/s11634-017-0284-z},
  citeulike-linkout-1  = {http://link.springer.com/article/10.1007/s11634-017-0284-z},
  groups               = {Sparse factor analysis, Clustering and network analysis},
  posted-at            = {2017-09-17 20:19:21},
  publisher            = {Springer Berlin Heidelberg},
  timestamp            = {2020-02-27 03:51},
}

@Article{Aghabozorgi-et-al-2015,
  author               = {Aghabozorgi, Saeed and Seyed Shirkhorshidi, Ali and Ying Wah, Teh},
  date                 = {2015-10},
  journaltitle         = {Information Systems},
  title                = {Time-series clustering - A decade review},
  doi                  = {10.1016/j.is.2015.04.007},
  issn                 = {0306-4379},
  pages                = {16--38},
  volume               = {53},
  abstract             = {Anatomy of time-series clustering is revealed by introducing its 4 main component. Research works in each of the four main components are reviewed in detail and compared. Analysis of research works published in the last decade. Enlighten new paths for future works for time-series clustering and its components. Clustering is a solution for classifying enormous data when there is not any early knowledge about classes. With emerging new concepts like cloud computing and big data and their vast applications in recent years, research works have been increased on unsupervised solutions like clustering algorithms to extract knowledge from this avalanche of data. Clustering time-series data has been used in diverse scientific areas to discover patterns which empower data analysts to extract valuable information from complex and massive datasets. In case of huge datasets, using supervised classification solutions is almost impossible, while clustering can solve this problem using un-supervised approaches. In this research work, the focus is on time-series data, which is one of the popular data types in clustering problems and is broadly used from gene expression data in biology to stock market analysis in finance. This review will expose four main components of time-series clustering and is aimed to represent an updated investigation on the trend of improvements in efficiency, quality and complexity of clustering time-series approaches during the last decade and enlighten new paths for future works.},
  citeulike-article-id = {14168675},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.is.2015.04.007},
  keywords             = {pdf},
  posted-at            = {2016-10-19 19:24:13},
  timestamp            = {2020-02-27 03:51},
}

@InCollection{Alaiz-et-al-2014,
  author               = {Alaz, Carlos M and Fernandez, Angela and Gala, Yvonne and Dorronsoro, Jose R},
  booktitle            = {Intelligent Data Engineering and Automated Learning - IDEAL 2014},
  date                 = {2014},
  title                = {Kernel K-Means Low Rank Approximation for Spectral Clustering and Diffusion Maps},
  doi                  = {10.1007/978-3-319-10840-7\_30},
  editor               = {Corchado, Emilio and Lozano, JoseA and Quintian, Hector and Yin, Hujun},
  pages                = {239--246},
  publisher            = {Springer International Publishing},
  series               = {Lecture Notes in Computer Science},
  volume               = {8669},
  abstract             = {Spectral Clustering and Diffusion Maps are currently the leading methods for advanced clustering or dimensionality reduction. However, they require the eigenanalysis of a sample's graph Laplacian L, something very costly for moderately sized samples and prohibitive for very large ones. We propose to build a low rank approximation to L using essentially the centroids obtained applying kernel K-means over the similarity matrix. We call this approach kernel KASP (kKASP) as it follows the KASP procedure of Yan et al. but coupling centroid selection with the local geometry defined by the similarity matrix. As we shall see, kKASP's reconstructions are competitive with KASP's ones, particularly in the low rank range.},
  citeulike-article-id = {14212409},
  citeulike-linkout-0  = {http://dx.doi.org/10.1007/978-3-319-10840-730},
  citeulike-linkout-1  = {http://link.springer.com/chapter/10.1007/978-3-319-10840-730},
  owner                = {cristi},
  posted-at            = {2016-11-21 22:14:56},
  timestamp            = {2020-02-27 03:51},
}

@Article{Albatineh-NiewiadomskaBugaj-2011,
  author         = {Albatineh, Ahmed N. and Niewiadomska-Bugaj, Magdalena},
  date           = {2011-07},
  journaltitle   = {Journal of Classification},
  title          = {MCS: A method for finding the number of clusters},
  doi            = {10.1007/s00357-010-9069-1},
  issn           = {0176-4268},
  number         = {2},
  pages          = {184--209},
  volume         = {28},
  abstract       = {This paper proposes a maximum clustering similarity (MCS) method for determining the number of clusters in a data set by studying the behavior of similarity indices comparing two (of several) clustering methods. The similarity between the two clusterings is calculated at the same number of clusters, using the indices of Rand (R), Fowlkes and Mallows (FM), and Kulczynski (K) each corrected for chance agreement. The number of clusters at which the index attains its maximum is a candidate for the optimal number of clusters. The proposed method is applied to simulated bivariate normal data, and further extended for use in circular data. Its performance is compared to the criteria discussed in Tibshirani, Walther, and Hastie (2001). The proposed method is not based on any distributional or data assumption which makes it widely applicable to any type of data that can be clustered using at least two clustering algorithms.},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 03:51},
}

@Article{Ando-Bai-2017,
  author               = {Ando, Tomohiro and Bai, Jushan},
  date                 = {2017-06},
  journaltitle         = {Journal of the American Statistical Association},
  title                = {Clustering Huge Number of Financial Time Series: A Panel Data Approach With High-Dimensional Predictors and Factor Structures},
  doi                  = {10.1080/01621459.2016.1195743},
  number               = {519},
  pages                = {1182--1198},
  volume               = {112},
  abstract             = {AbstractThis article introduces a new procedure for clustering a large number of financial time series based on high-dimensional panel data with grouped factor structures. The proposed method attempts to capture the level of similarity of each of the time series based on sensitivity to observable factors as well as to the unobservable factor structure. The proposed method allows for correlations between observable and unobservable factors and also allows for cross-sectional and serial dependence and heteroscedasticities in the error structure, which are common in financial markets. In addition, theoretical properties are established for the procedure. We apply the method to analyze the returns for over 6000 international stocks from over 100 financial markets. The empirical analysis quantifies the extent to which the U.S. subprime crisis spilled over to the global financial markets. Furthermore, we find that nominal classifications based on either listed market, industry, country or region are insufficient to characterize the heterogeneity of the global financial markets. Supplementary materials for this article are available online.},
  citeulike-article-id = {14431349},
  citeulike-linkout-0  = {http://dx.doi.org/10.1080/01621459.2016.1195743},
  citeulike-linkout-1  = {http://www.tandfonline.com/doi/abs/10.1080/01621459.2016.1195743},
  day                  = {10},
  groups               = {Clustering and network analysis, Scenario_TimeSeries},
  posted-at            = {2017-09-16 16:57:06},
  publisher            = {Taylor \& Francis},
  timestamp            = {2020-02-27 03:51},
}

@Article{Arakelian-Karlis-2014,
  author               = {Arakelian, Veni and Karlis, Dimitris},
  date                 = {2014-01},
  journaltitle         = {Communications in Statistics - Simulation and Computation},
  title                = {Clustering Dependencies Via Mixtures of Copulas},
  doi                  = {10.1080/03610918.2012.752832},
  number               = {7},
  pages                = {1644--1661},
  volume               = {43},
  abstract             = {The use of mixture models for clustering purposes has been considerably increased the last years primarily due to the existence of efficient computational methods that facilitate estimation. Nowadays, there are several clustering procedures based on mixtures for certain types of data. On the other hand, copulas are becoming very popular models to model dependencies as one of their appealing properties is the separation of the marginal properties of the data from the dependence properties. The purpose of this article is to put together the two distinct ideas, namely mixtures and copulas, so as to use mixtures of copulas aiming at using them for clustering with respect to the dependence properties of the data. This is accomplished by considering finite mixture of different copulas to represent different dependence structures. We provide properties of the derived models along with the description of an estimation method using an EM algorithm based on the standard approach for mixture models. Using daily returns from major stock markets, we illustrate the potential of our method.},
  citeulike-article-id = {14151134},
  citeulike-linkout-0  = {http://dx.doi.org/10.1080/03610918.2012.752832},
  citeulike-linkout-1  = {http://www.tandfonline.com/doi/abs/10.1080/03610918.2012.752832},
  day                  = {1},
  groups               = {Networks and investment management},
  owner                = {cristi},
  posted-at            = {2016-10-03 20:23:53},
  publisher            = {Taylor and Francis},
  timestamp            = {2020-02-27 03:51},
}

@Article{Arbelaitz-et-al-2013,
  author               = {Arbelaitz, Olatz and Gurrutxaga, Ibai and Muguerza, Javier and Perez, Jesus M. and Perona, Inigo},
  date                 = {2013-01},
  journaltitle         = {Pattern Recognition},
  title                = {An extensive comparative study of cluster validity indices},
  doi                  = {10.1016/j.patcog.2012.07.021},
  issn                 = {0031-3203},
  number               = {1},
  pages                = {243--256},
  volume               = {46},
  abstract             = {The validation of the results obtained by clustering algorithms is a fundamental part of the clustering process. The most used approaches for cluster validation are based on internal cluster validity indices. Although many indices have been proposed, there is no recent extensive comparative study of their performance. In this paper we show the results of an experimental work that compares 30 cluster validity indices in many different environments with different characteristics. These results can serve as a guideline for selecting the most suitable index for each possible application and provide a deep insight into the performance differences between the currently available indices. We compare 30 cluster validity indices (CVIs) in 720 synthetic and 20 real datasets. We use a new comparison methodology and three clustering algorithms: k-means, Ward and Average-linkage. The CVI performance drops dramatically when noise is present or clusters overlap. Statistical tests suggest a division of three groups of CVIs.},
  citeulike-article-id = {11208386},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.patcog.2012.07.021},
  posted-at            = {2017-11-22 22:50:31},
  timestamp            = {2020-02-27 03:51},
}

@Article{Augustynski-LaskosGrabowski-2018,
  author         = {Augustynski, Iwo and Laskos-Grabowski, Pawel},
  date           = {2018},
  journaltitle   = {Econometrics},
  title          = {Clustering Macroeconomic Time Series},
  doi            = {10.15611/eada.2018.2.06},
  issn           = {1507-3866},
  number         = {2},
  pages          = {74--88},
  volume         = {22},
  abstract       = {The data mining technique of time series clustering is well established in many fields. However, as an unsupervised learning method, it requires making choices that are nontrivially influenced by the nature of the data involved. The aim of this paper is to verify usefulness of the time series clustering method for macroeconomics research, and to develop the most suitable methodology. By extensively testing various possibilities, we arrive at a choice of a dissimilarity measure (compression-based dissimilarity measure, or CDM) which is particularly suitable for clustering macroeconomic variables. We check that the results are stable in time and reflect large-scale phenomena such as crises. We also successfully apply our findings to analysis of national economies, specifically to identifying their structural relations.},
  f1000-projects = {QuantInvest},
  groups         = {Invest_Network, ML_ClustTimeSrs},
  timestamp      = {2020-02-27 03:51},
}

@Article{Balzer-et-al-2018,
  author         = {Balzer, Laura B. and Zheng, Wenjing and van der Laan, Mark J. and Petersen, Maya L.},
  date           = {2018-01-01},
  journaltitle   = {Statistical Methods in Medical Research},
  title          = {A new approach to hierarchical data analysis: Targeted maximum likelihood estimation for the causal effect of a cluster-level exposure.},
  doi            = {10.1177/0962280218774936},
  pages          = {962280218774936},
  abstract       = {We often seek to estimate the impact of an exposure naturally occurring or randomly assigned at the cluster-level. For example, the literature on neighborhood determinants of health continues to grow. Likewise, community randomized trials are applied to learn about real-world implementation, sustainability, and population effects of interventions with proven individual-level efficacy. In these settings, individual-level outcomes are correlated due to shared cluster-level factors, including the exposure, as well as social or biological interactions between individuals. To flexibly and efficiently estimate the effect of a cluster-level exposure, we present two targeted maximum likelihood estimators (TMLEs). The first TMLE is developed under a non-parametric causal model, which allows for arbitrary interactions between individuals within a cluster. These interactions include direct transmission of the outcome (i.e. contagion) and influence of one individual's covariates on another's outcome (i.e. covariate interference). The second TMLE is developed under a causal sub-model assuming the cluster-level and individual-specific covariates are sufficient to control for confounding. Simulations compare the alternative estimators and illustrate the potential gains from pairing individual-level risk factors and outcomes during estimation, while avoiding unwarranted assumptions. Our results suggest that estimation under the sub-model can result in bias and misleading inference in an observational setting. Incorporating working assumptions during estimation is more robust than assuming they hold in the underlying causal model. We illustrate our approach with an application to HIV prevention and treatment.},
  day            = {1},
  f1000-projects = {QuantInvest},
  groups         = {Estim_MaxLikelihood, Data_Explo_Analysis},
  pmid           = {29921160},
  timestamp      = {2020-02-27 03:51},
}

@Article{Basalto-et-al-2007,
  author               = {Basalto, Nicolas and Bellotti, Roberto and De Carlo, Francesco and Facchi, Paolo and Pantaleo, Ester and Pascazio, Saverio},
  date                 = {2007-06},
  journaltitle         = {Physica A: Statistical Mechanics and its Applications},
  title                = {Hausdorff clustering of financial time series},
  doi                  = {10.1016/j.physa.2007.01.011},
  issn                 = {0378-4371},
  number               = {2},
  pages                = {635--644},
  volume               = {379},
  abstract             = {A clustering procedure is introduced based on the Hausdorff distance as a similarity measure between clusters of elements. The method is applied to the financial time series of the Dow Jones industrial average (DJIA) index to find companies that share a similar behavior. Comparisons are made with other linkage algorithms.},
  citeulike-article-id = {14148581},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.physa.2007.01.011},
  groups               = {Networks and investment management},
  owner                = {cristi},
  posted-at            = {2016-09-28 19:44:58},
  timestamp            = {2020-02-27 03:51},
}

@Article{Bastos-Caiado-2014,
  author               = {Bastos, Joao A. and Caiado, Jorge},
  date                 = {2014-12},
  journaltitle         = {Quantitative Finance},
  title                = {Clustering financial time series with variance ratio statistics},
  doi                  = {10.1080/14697688.2012.726736},
  number               = {12},
  pages                = {2121--2133},
  volume               = {14},
  abstract             = {This study introduces a new distance measure for clustering financial time series based on variance ratio test statistics. The proposed metric attempts to assess the level of interdependence of time series from the point of view of return predictability. Simulation results show that this metric aggregates time series according to their serial dependence structure better than a metric based on the sample autocorrelations. An empirical application of this approach to international stock market returns is presented. The results suggest that this metric discriminates stock markets reasonably well according to size and the level of development. Furthermore, despite the substantial evolution of individual variance ratio statistics, the clustering pattern remains fairly stable across different time periods.},
  citeulike-article-id = {14316690},
  citeulike-linkout-0  = {http://dx.doi.org/10.1080/14697688.2012.726736},
  citeulike-linkout-1  = {http://www.tandfonline.com/doi/abs/10.1080/14697688.2012.726736},
  day                  = {2},
  groups               = {Networks and investment management},
  posted-at            = {2017-03-23 08:25:58},
  publisher            = {Routledge},
  timestamp            = {2020-02-27 03:51},
}

@Article{Berthold-Hoppner-2016,
  author               = {Berthold, Michael R. and Hoppner, Frank},
  date                 = {2016-01-10},
  journaltitle         = {arXiv e-Print},
  title                = {On Clustering Time Series Using Euclidean Distance and Pearson Correlation},
  eprint               = {1601.02213},
  eprinttype           = {arXiv},
  url                  = {https://arxiv.org/abs/1601.02213},
  abstract             = {For time series comparisons, it has often been observed that z-score normalized Euclidean distances far outperform the unnormalized variant. In this paper we show that a z-score normalized, squared Euclidean Distance is, in fact, equal to a distance based on Pearson Correlation. This has profound impact on many distance-based classification or clustering methods. In addition to this theoretically sound result we also show that the often used k-Means algorithm formally needs a mod ification to keep the interpretation as Pearson correlation strictly valid. Experimental results demonstrate that in many cases the standard k-Means algorithm generally produces the same results.},
  citeulike-article-id = {13904331},
  citeulike-linkout-0  = {http://arxiv.org/abs/1601.02213},
  citeulike-linkout-1  = {http://arxiv.org/pdf/1601.02213},
  day                  = {10},
  posted-at            = {2017-11-23 21:43:08},
  timestamp            = {2020-02-27 03:51},
}

@Article{Bien-Tibshirani-2011,
  author               = {Bien, Jacob and Tibshirani, Robert},
  date                 = {2011-09},
  journaltitle         = {Journal of the American Statistical Association},
  title                = {Hierarchical Clustering With Prototypes via Minimax Linkage},
  doi                  = {10.1198/jasa.2011.tm10183},
  number               = {495},
  pages                = {1075--1084},
  volume               = {106},
  abstract             = {Agglomerative hierarchical clustering is a popular class of methods for understanding the structure of a dataset. The nature of the clustering depends on the choice of linkage, that is, on how one measures the distance between clusters. In this article we investigate minimax linkage, a recently introduced but little-studied linkage. Minimax linkage is unique in naturally associating a prototype chosen from the original dataset with every interior node of the dendrogram. These prototypes can be used to greatly enhance the interpretability of a hierarchical clustering. Furthermore, we prove that minimax linkage has a number of desirable theoretical properties; for example, minimax-linkage dendrograms cannot have inversions (unlike centroid linkage) and is robust against certain perturbations of a dataset. We provide an efficient implementation and illustrate minimax linkage's strengths as a data analysis and visualization tool on a study of words from encyclopedia articles and on a dataset of images of human faces.},
  citeulike-article-id = {14148576},
  citeulike-linkout-0  = {http://dx.doi.org/10.1198/jasa.2011.tm10183},
  citeulike-linkout-1  = {http://www.tandfonline.com/doi/abs/10.1198/jasa.2011.tm10183},
  day                  = {1},
  groups               = {Networks and investment management, Clustering and network analysis},
  owner                = {cristi},
  posted-at            = {2016-09-28 19:30:51},
  publisher            = {Taylor and Francis},
  timestamp            = {2020-02-27 03:51},
}

@Article{Biswas-Biswas-2017,
  author               = {Biswas, Anupam and Biswas, Bhaskar},
  date                 = {2017-04},
  journaltitle         = {Expert Systems with Applications},
  title                = {Defining quality metrics for graph clustering evaluation},
  doi                  = {10.1016/j.eswa.2016.11.011},
  issn                 = {0957-4174},
  pages                = {1--17},
  volume               = {71},
  abstract             = {Evaluation of clustering has significant importance in various applications of expert and intelligent systems. Clusters are evaluated in terms of quality and accuracy. Measuring quality is a unsupervised approach that completely depends on edges, whereas measuring accuracy is a supervised approach that measures similarity between the real clustering and the predicted clustering. Accuracy cannot be measured for most of the real-world networks since real clustering is unavailable. Thus, it will be advantageous from the viewpoint of expert systems to develop a quality metric that can assure certain level of accuracy along with the quality of clustering. In this paper we have proposed a set of three quality metrics for graph clustering that have the ability to ensure accuracy along with the quality. The effectiveness of the metrics has been evaluated on benchmark graphs as well as on real-world networks and compared with existing metrics. Results indicate competency of the suggested metrics while dealing with accuracy, which will definitely improve the decision-making in expert and intelligent systems. We have also shown that our metrics satisfy all of the six quality-related properties.},
  citeulike-article-id = {14447477},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.eswa.2016.11.011},
  posted-at            = {2017-10-08 14:20:33},
  timestamp            = {2020-02-27 03:51},
}

@Article{Blau-2019,
  author         = {Blau, Benjamin M.},
  date           = {2019-01-02},
  journaltitle   = {Journal of Behavioral Finance},
  title          = {Price clustering and investor sentiment},
  doi            = {10.1080/15427560.2018.1431887},
  issn           = {1542-7560},
  number         = {1},
  pages          = {19--30},
  urldate        = {2019-09-01},
  volume         = {20},
  abstract       = {Among the anomalous findings in the finance literature, perhaps the most persistent is the finding that security prices tend to cluster on round pricing increments. The author examines how investor sentiment influences the degree of price clustering. Both univariate and multivariate tests show a contemporaneous correlation between price clustering and investor sentiment. Recognizing the need to make stronger causal inferences, the author conducts 2 additional sets of tests. First, the author uses the technology bubble period as natural experiment and examine the price clustering of technology vis-a-vis nontechnology stocks. Results show that price clustering is markedly higher in tech stocks than in nontech stocks during this period of rising, sector-specific, investor sentiment. Second, the author estimates a vector autoregression process and examines the impulse responses of price clustering to exogenous shocks in investor sentiment. The results from these tests indicate that causation flows from sentiment to clustering instead of the other way around.},
  day            = {2},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 03:51},
}

@Article{Bnouachir-Mkhadri-2019,
  author         = {Bnouachir, Najla and Mkhadri, Abdallah},
  date           = {2020},
  journaltitle   = {Communications in Statistics - Simulation and Computation},
  title          = {Efficient cluster-based portfolio optimization},
  doi            = {10.1080/03610918.2019.1621341},
  issn           = {0361-0918},
  pages          = {In Press1--15},
  urldate        = {2020-01-13},
  abstract       = {The sample mean and covariance matrix of historical data provide a disappointing out-of-sample performance in mean-variance portfolio rules. This poor performance is certainly due to the high estimation error incurred in the optimization model. Our purpose in this article is to find a method that enhances the out-of-sample performance of the portfolio weights. Using hierarchical clustering, we propose an alternative cluster-based portfolio to obtain a sequence of cluster assets. On the basis of Gram-Schmidt orthogonalization, the estimation risk of the data set becomes the sum of the estimations of the clusters in the sequence. The performance of our method and its competitors is compared empirically and via some simulations in high dimension.},
  day            = {3},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 03:51},
}

@Article{Bulo-Pelillo-2017,
  author               = {Bulo, Samuel Rota and Pelillo, Marcello},
  date                 = {2017-10},
  journaltitle         = {European Journal of Operational Research},
  title                = {Dominant-set clustering: A review},
  doi                  = {10.1016/j.ejor.2017.03.056},
  issn                 = {0377-2217},
  number               = {1},
  pages                = {1--13},
  volume               = {262},
  abstract             = {Clustering refers to the process of extracting maximally coherent groups from a set of objects using pairwise, or high-order, similarities. Traditional approaches to this problem are based on the idea of partitioning the input data into a predetermined number of classes, thereby obtaining the clusters as a by-product of the partitioning process. A radically different perspective of the problem consists in providing a formalization of the very notion of a cluster and considering the clustering process as a sequential search of structures in the data adhering to this cluster notion. In this manuscript we review one of the pioneering approaches falling in the latter class of algorithms, which has been proposed in the early 2000s and has been found since then a number of applications in different domains. It is known as dominant set clustering and provides a notion of a cluster (a.k.a. dominant set) that has intriguing links to game-theory, graph-theory and optimization theory. From the game-theoretic perspective, clusters are regarded as equilibria of non-cooperative "clustering" games; in the graph-theoretic context, it can be shown that they generalize the notion of maximal clique to edge-weighted graphs; finally, from an optimization point of view, they can be characterized in terms of solutions to a simplex-constrained, quadratic optimization problem, as well as in terms of an exquisitely combinatorial entity. Besides introducing dominant sets from a theoretical perspective, we will also focus on the related algorithmic issues by reviewing two state-of-the-art methods that are used in the literature to find dominant sets clusters, namely the Replicator Dynamics and the Infection and Immunization Dynamics. Finally, we conclude with an overview of different extensions of the dominant set framework and of applications where dominant sets have been successfully employed.},
  citeulike-article-id = {14500761},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.ejor.2017.03.056},
  posted-at            = {2017-12-11 09:13:21},
  timestamp            = {2020-02-27 03:51},
}

@Article{Cai-et-al-2016,
  author               = {Cai, Fan and Le-Khac, Nhien-An and Kechadi, Tahar},
  date                 = {2016-09},
  journaltitle         = {arXiv e-Print},
  title                = {Clustering Approaches for Financial Data Analysis: a Survey},
  eprint               = {1609.08520},
  eprinttype           = {arXiv},
  url                  = {https://arxiv.org/abs/1609.08520},
  abstract             = {Nowadays, financial data analysis is becoming increasingly important in the business market. As companies collect more and more data from daily operations, they expect to extract useful knowledge from existing collected data to help make reasonable decisions for new customer requests, e.g. user credit category, confidence of expected return, etc. Banking and financial institutes have applied different data mining techniques to enhance their business performance. Among these techniques, clustering has been considered as a significant method to capture the natural structure of data. However, there are not many studies on clustering approaches for financial data analysis. In this paper, we evaluate different clustering algorithms for analysing different financial datasets varied from time series to transactions. We also discuss the advantages and disadvantages of each method to enhance the understanding of inner structure of financial datasets as well as the capability of each clustering method in this context.},
  citeulike-article-id = {14148629},
  citeulike-linkout-0  = {http://arxiv.org/abs/1609.08520},
  citeulike-linkout-1  = {http://arxiv.org/pdf/1609.08520},
  day                  = {4},
  groups               = {Networks and investment management, Invest_Network},
  owner                = {cristi},
  posted-at            = {2016-09-28 21:25:38},
  timestamp            = {2020-02-27 03:51},
}

@Article{Cai-et-al-2017,
  author               = {Cai, Yumei and Cui, Xiaomei and Huang, Qianyun and Sun, Jianqiang},
  date                 = {2017-09},
  journaltitle         = {International Review of Economics \& Finance},
  title                = {Hierarchy, cluster, and time-stable information structure of correlations between international financial markets},
  doi                  = {10.1016/j.iref.2017.07.024},
  issn                 = {1059-0560},
  pages                = {562--573},
  volume               = {51},
  abstract             = {This paper investigates the correlations between 52 financial markets located in different countries or regions from July 2004 through June 2011. By using a correlation matrix time series and a participation frequency method based on the random matrix theory, we show that a time-stable information structure is contained in the correlations between global financial markets. We further find that the information structure is closely associated with global market and global geographical factors, and that each financial index's participation in the global market factor varies over time and presents dynamics. Two patterns, hierarchy and cluster effects, are found to be in the dynamics of the indices' participation in the global market factor. The cluster effect implies a more concentrated participation during the 2008 financial crisis.},
  citeulike-article-id = {14429814},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.iref.2017.07.024},
  groups               = {Clustering and network analysis},
  posted-at            = {2017-09-13 09:33:44},
  timestamp            = {2020-02-27 03:51},
}

@Article{Carlsson-et-al-2016,
  author               = {Carlsson, Gunnar and Memoli, Facundo and Ribeiro, Alejandro and Segarra, Santiago},
  date                 = {2016-07},
  journaltitle         = {arXiv e-Print},
  title                = {Excisive Hierarchical Clustering Methods for Network Data},
  eprint               = {1607.06339},
  eprinttype           = {arXiv},
  url                  = {https://arxiv.org/abs/1607.06339},
  abstract             = {We introduce two practical properties of hierarchical clustering methods for (possibly asymmetric) network data: excisiveness and linear scale preservation. The latter enforces imperviousness to change in units of measure whereas the former ensures local consistency of the clustering outcome. Algorithmically, excisiveness implies that we can reduce computational complexity by only clustering a data subset of interest while theoretically guaranteeing that the same hierarchical outcome would be observed when clustering the whole dataset. Moreover, we introduce the concept of representability, i.e. a generative model for describing clustering methods through the specification of their action on a collection of networks. We further show that, within a rich set of admissible methods, requiring representability is equivalent to requiring both excisiveness and linear scale preservation. Leveraging this equivalence, we show that all excisive and linear scale preserving methods can be factored into two steps: a transformation of the weights in the input network followed by the application of a canonical clustering method. Furthermore, their factorization can be used to show stability of excisive and linear scale preserving methods in the sense that a bounded perturbation in the input network entails a bounded perturbation in the clustering output.},
  citeulike-article-id = {14357923},
  citeulike-linkout-0  = {http://arxiv.org/abs/1607.06339},
  citeulike-linkout-1  = {http://arxiv.org/pdf/1607.06339},
  day                  = {21},
  groups               = {Networks and investment management, Clustering and network analysis},
  posted-at            = {2017-05-16 14:04:03},
  timestamp            = {2020-02-27 03:51},
}

@Article{Carlsson-et-al-2016a,
  author               = {Carlsson, Gunnar and Memoli, Facundo and Ribeiro, Alejandro and Segarra, Santiago},
  date                 = {2016-07},
  journaltitle         = {arXiv e-Print},
  title                = {Hierarchical Clustering of Asymmetric Networks},
  eprint               = {1607.06294},
  eprinttype           = {arXiv},
  url                  = {https://arxiv.org/abs/1607.06294},
  abstract             = {This paper considers networks where relationships between nodes are represented by directed dissimilarities. The goal is to study methods that, based on the dissimilarity structure, output hierarchical clusters, i.e., a family of nested partitions indexed by a connectivity parameter. Our construction of hierarchical clustering methods is built around the concept of admissible methods, which are those that abide by the axioms of value - nodes in a network with two nodes are clustered together at the maximum of the two dissimilarities between them - and transformation - when dissimilarities are reduced, the network may become more clustered but not less. Two particular methods, termed reciprocal and nonreciprocal clustering, are shown to provide upper and lower bounds in the space of admissible methods. Furthermore, alternative clustering methodologies and axioms are considered. In particular, modifying the axiom of value such that clustering in two-node networks occurs at the minimum of the two dissimilarities entails the existence of a unique admissible clustering method.},
  citeulike-article-id = {14357925},
  citeulike-linkout-0  = {http://arxiv.org/abs/1607.06294},
  citeulike-linkout-1  = {http://arxiv.org/pdf/1607.06294},
  day                  = {21},
  groups               = {Networks and investment management, Clustering and network analysis},
  posted-at            = {2017-05-16 14:05:05},
  timestamp            = {2020-02-27 03:51},
}

@Article{Carlsson-et-al-2017,
  author               = {Carlsson, Gunnar and Memoli, Facundo and Ribeiro, Alejandro and Segarra, Santiago},
  date                 = {2017},
  journaltitle         = {IEEE Transactions on Signal and Information Processing over Networks},
  title                = {Admissible Hierarchical Clustering Methods and Algorithms for Asymmetric Networks},
  doi                  = {10.1109/tsipn.2017.2662622},
  issn                 = {2373-776X},
  pages                = {1},
  abstract             = {This paper characterizes hierarchical clustering methods that abide by two previously introduced axioms - thus, denominated admissible methods - and proposes tractable algorithms for their implementation. We leverage the fact that, for asymmetric networks, every admissible method must be contained between reciprocal and nonreciprocal clustering, and describe three families of intermediate methods. Grafting methods exchange branches between dendrograms generated by different admissible methods. The convex combination family combines admissible methods through a convex operation in the space of dendrograms, and thirdly, the semi-reciprocal family clusters nodes that are related by strong cyclic influences in the network. An algorithmic framework for the computation of hierarchical clusters generated by reciprocal and nonreciprocal clustering as well as the grafting, convex combination, and semi-reciprocal families is presented via matrix operations in a dioid algebra. Finally, the introduced clustering methods and algorithms are exemplified through their application to a network describing the interrelation between sectors of the United States (U.S.) economy.},
  citeulike-article-id = {14357921},
  citeulike-linkout-0  = {http://dx.doi.org/10.1109/tsipn.2017.2662622},
  groups               = {Networks and investment management, Clustering and network analysis},
  posted-at            = {2017-05-16 14:02:52},
  timestamp            = {2020-02-27 03:51},
}

@Article{Chamroukhi-et-al-2013,
  author               = {Chamroukhi, Faicel and Same, Allou and Aknin, Patrice and Govaert, Gerard},
  date                 = {2013-12-25},
  journaltitle         = {Proceedings of the 2011 International Joint Conference on Neural Networks (IJCNN)},
  title                = {Model-based clustering with Hidden Markov Model regression for time series with regime changes},
  doi                  = {10.1109/ijcnn.2011.6033590},
  pages                = {2814--2821},
  abstract             = {This paper introduces a novel model-based clustering approach for clustering time series which present changes in regime. It consists of a mixture of polynomial regressions governed by hidden Markov chains. The underlying hidden process for each cluster activates successively several polynomial regimes during time. The parameter estimation is performed by the maximum likelihood method through a dedicated Expectation-Maximization (EM) algorithm. The proposed approach is evaluated using simulated time series and real-world time series issued from a railway diagnosis application. Comparisons with existing approaches for time series clustering, including the stand EM for Gaussian mixtures, K-means clustering, the standard mixture of regression models and mixture of Hidden Markov Models, demonstrate the effectiveness of the proposed approach.},
  booktitle            = {The 2011 International Joint Conference on Neural Networks},
  citeulike-article-id = {14510865},
  citeulike-linkout-0  = {http://arxiv.org/abs/1312.7024},
  citeulike-linkout-1  = {http://arxiv.org/pdf/1312.7024},
  citeulike-linkout-2  = {http://dx.doi.org/10.1109/ijcnn.2011.6033590},
  day                  = {25},
  isbn                 = {978-1-4244-9635-8},
  location             = {San Jose, CA, USA},
  posted-at            = {2018-01-02 02:30:30},
  publisher            = {IEEE},
  timestamp            = {2020-02-27 03:51},
}

@Article{Chamroukhi-Nguyen-2019,
  author         = {Chamroukhi, Faicel and Nguyen, Hien D.},
  date           = {2019-01-18},
  journaltitle   = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  title          = {Model-based clustering and classification of functional data},
  doi            = {10.1002/widm.1298},
  issn           = {1942-4787},
  pages          = {e1298},
  urldate        = {2019-10-09},
  abstract       = {The problem of complex data analysis is a central topic of modern statistical science and learning systems and is becoming of broader interest with the increasing prevalence of high-dimensional data. The challenge is to develop statistical models and autonomous algorithms that are able to acquire knowledge from raw data for exploratory analysis, which can be achieved through clustering techniques or to make predictions of future data via classification (i.e., discriminant analysis) techniques. Latent data models, including mixture model-based approaches are one of the most popular and successful approaches in both the unsupervised context (i.e., clustering) and the supervised one (i.e, classification or discrimination). Although traditionally tools of multivariate analysis, they are growing in popularity when considered in the framework of functional data analysis (FDA). FDA is the data analysis paradigm in which the individual data units are functions (e.g., curves, surfaces), rather than simple vectors. In many areas of application, the analyzed data are indeed often available in the form of discretized values of functions or curves (e.g., time series, waveforms) and surfaces (e.g., 2d-images, spatio-temporal data). This functional aspect of the data adds additional difficulties compared to the case of a classical multivariate (non-functional) data analysis. We review and present approaches for model-based clustering and classification of functional data. We derive well-established statistical models along with efficient algorithmic tools to address problems regarding the clustering and the classification of these high-dimensional data, including their heterogeneity, missing information, and dynamical hidden structure. The presented models and algorithms are illustrated on real-world functional data analysis problems from several application area.},
  day            = {18},
  f1000-projects = {QuantInvest},
  groups         = {ML_ClustTimeSrs},
  timestamp      = {2020-02-27 03:51},
}

@Article{Charrad-et-al-2014,
  author               = {Charrad, Malika and Ghazzali, Nadia and Boiteau, Veronique and Niknafs, Azam},
  date                 = {2014},
  journaltitle         = {Journal of Statistical Software},
  title                = {NbClust: An R Package for Determining theRelevant Number of Clusters in a Data Set},
  abstract             = {Clustering is the partitioning of a set of objects into groups (clusters) so that objects within a group are more similar to each others than objects in dierent groups. Most of the clustering algorithms depend on some assumptions in order to dene the subgroups present in a data set. As a consequence, the resulting clustering scheme requires some sort of evaluation as regards its validity. The evaluation procedure has to tackle dicult problems such as the quality of clusters, the degree with which a clustering scheme ts a specic data set and the optimal number of clusters in a partitioning. In the literature, a wide variety of indices have been proposed to nd the optimal number of clusters in a partitioning of a data set during the clustering process. However, for most of indices proposed in the literature, programs are unavailable to test these indices and compare them. The R package NbClust has been developed for that purpose. It provides 30 indices which determine the number of clusters in a data set and it oers also the best clus- tering scheme from dierent results to the user. In addition, it provides a function to perform k-means and hierarchical clustering with dierent distance measures and aggre- gation methods. Any combination of validation indices and clustering methods can be requested in a single function call. This enables the user to simultaneously evaluate sev- eral clustering schemes while varying the number of clusters, to help determining the most appropriate number of clusters for the data set of interest.},
  citeulike-article-id = {14468583},
  posted-at            = {2017-10-29 20:23:51},
  timestamp            = {2020-02-27 03:51},
}

@Article{Chatziafratis-et-al-2018,
  author         = {Chatziafratis, Vaggos and Niazadeh, Rad and Charikar, Moses},
  date           = {2018-07-03},
  journaltitle   = {Proceedings of Machine Learning Research},
  title          = {Hierarchical Clustering with Structural Constraints},
  url            = {http://proceedings.mlr.press/v80/chatziafratis18a.html},
  urldate        = {2019-09-15},
  abstract       = {Hierarchical clustering is a popular unsupervised data analysis method. For many real-world applications, we would like to exploit prior information about the data that imposes constraints on the clustering hierarchy, and is not captured by the set of features available to the algorithm. This gives rise to the problem of hierarchical clustering with structural constraints. Structural constraints pose major challenges for bottom-up approaches like average/single linkage and even though they can be naturally incorporated into top-down divisive algorithms, no formal guarantees exist on the quality of their output. In this paper, we provide provable approximation guarantees for two simple top-down algorithms, using a recently introduced optimization viewpoint of hierarchical clustering with pairwise similarity information (Dasgupta, 2016). We show how to find good solutions even in the presence of conflicting prior information, by formulating a constraint-based regularization of the objective. Furthemore, we explore a variation of this objective for dissimilarity information (Cohen-Addad et al., 2018) and improve upon current techniques. Finally, we demonstrate our approach on a real dataset for the taxonomy application.},
  day            = {3},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 03:51},
}

@Article{Crawford-Milenkovic-2018,
  author         = {Crawford, Joseph and Milenkovic, Tijana},
  date           = {2018-05-08},
  journaltitle   = {PLOS ONE},
  title          = {ClueNet: Clustering a temporal network based on topological similarity rather than denseness.},
  doi            = {10.1371/journal.pone.0195993},
  number         = {5},
  pages          = {e0195993},
  volume         = {13},
  abstract       = {Network clustering is a very popular topic in the network science field. Its goal is to divide (partition) the network into groups (clusters or communities) of "topologically related" nodes, where the resulting topology-based clusters are expected to "correlate" well with node label information, i.e., metadata, such as cellular functions of genes/proteins in biological networks, or age or gender of people in social networks. Even for static data, the problem of network clustering is complex. For dynamic data, the problem is even more complex, due to an additional dimension of the data-their temporal (evolving) nature. Since the problem is computationally intractable, heuristic approaches need to be sought. Existing approaches for dynamic network clustering (DNC) have drawbacks. First, they assume that nodes should be in the same cluster if they are densely interconnected within the network. We hypothesize that in some applications, it might be of interest to cluster nodes that are topologically similar to each other instead of or in addition to requiring the nodes to be densely interconnected. Second, they ignore temporal information in their early steps, and when they do consider this information later on, they do so implicitly. We hypothesize that capturing temporal information earlier in the clustering process and doing so explicitly will improve results. We test these two hypotheses via our new approach called ClueNet. We evaluate ClueNet against six existing DNC methods on both social networks capturing evolving interactions between individuals (such as interactions between students in a high school) and biological networks capturing interactions between biomolecules in the cell at different ages. We find that ClueNet is superior in over 83\% of all evaluation tests. As more real-world dynamic data are becoming available, DNC and thus ClueNet will only continue to gain importance.},
  day            = {8},
  f1000-projects = {QuantInvest},
  pmcid          = {PMC5940177},
  pmid           = {29738568},
  timestamp      = {2020-02-27 03:51},
}

@Article{Dantas-Oliveira-2018,
  author         = {Dantas, Tiago Mendes and Oliveira, Fernando Luiz Cyrino},
  date           = {2018-10},
  journaltitle   = {International Journal of Forecasting},
  title          = {Improving time series forecasting: An approach combining bootstrap aggregation, clusters and exponential smoothing},
  doi            = {10.1016/j.ijforecast.2018.05.006},
  issn           = {0169-2070},
  number         = {4},
  pages          = {748--761},
  volume         = {34},
  abstract       = {Some recent papers have demonstrated that combining bagging (bootstrap aggregating) with exponential smoothing methods can produce highly accurate forecasts and improve the forecast accuracy relative to traditional methods. We therefore propose a new approach that combines the bagging, exponential smoothing and clustering methods. The existing methods use bagging to generate and aggregate groups of forecasts in order to reduce the variance. However, none of them consider the effect of covariance among the group of forecasts, even though it could have a dramatic impact on the variance of the group, and therefore on the forecast accuracy. The proposed approach, referred to here as Bagged Cluster ETS, aims to reduce the covariance effect by using partitioning around medoids (PAM) to produce clusters of similar forecasts, then selecting several forecasts from each cluster to create a group with a reduced variance. This approach was tested on various different time series sets from the M3 and CIF 2016 competitions. The empirical results have shown a substantial reduction in the forecast error, considering sMAPE and MASE.},
  f1000-projects = {QuantInvest},
  groups         = {FrcstQWIM_TimeSrs, Scenario_TimeSeries},
  timestamp      = {2020-02-27 03:56},
}

@Article{DeLuca-Zuccolotto-2016,
  author               = {{De Luca}, Giovanni and Zuccolotto, Paola},
  date                 = {2016-01},
  journaltitle         = {Statistics and Risk Modeling},
  title                = {A double clustering algorithm for financial time series based on extreme events},
  doi                  = {10.1515/strm-2015-0026},
  issn                 = {2193-1402},
  number               = {0},
  volume               = {0},
  abstract             = {This paper is concerned with a procedure for financial time series clustering, aimed at creating groups of time series characterized by similar behavior with regard to extreme events. The core of our proposal is a double clustering procedure: the former is based on the lower tail dependence of all the possible pairs of time series, the latter on the upper tail dependence. Tail dependence coefficients are estimated with copula functions. The final goal is to exploit the two clustering solutions in an algorithm designed to create a portfolio that maximizes the probability of joint positive extreme returns while minimizing the risk of joint negative extreme returns. In financial crisis scenarios, such a portfolio is expected to outperform portfolios generated by the traditional methods. We describe the results of a simulation study and, finally, we apply the procedure to a dataset composed of the 50 assets included in the EUROSTOXX index.},
  citeulike-article-id = {14150079},
  citeulike-linkout-0  = {http://dx.doi.org/10.1515/strm-2015-0026},
  day                  = {20},
  groups               = {Networks and investment management, [nbkcbu3:]},
  owner                = {cristi},
  posted-at            = {2016-10-01 20:49:20},
  timestamp            = {2020-02-27 03:56},
}

@Article{dePrado-2020a,
  author         = {{de Prado}, Marcos Lopez},
  date           = {2020},
  journaltitle   = {SSRN e-Print},
  title          = {Clustering},
  doi            = {10.2139/ssrn.3512998},
  issn           = {1556-5068},
  url            = {https://www.ssrn.com/abstract=3512998},
  urldate        = {2020-01-19},
  abstract       = {Many problems in finance require the clustering of variables or observations. Despite its usefulness, clustering is almost never taught in Econometrics courses. In this seminar we review two general clustering approaches: partitional and hierarchical.},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 03:56},
}

@Article{Dias-et-al-2015,
  author       = {Dias, Jose G. and Vermunt, Jeroen K. and Ramos, Sofia},
  date         = {2015},
  journaltitle = {European Journal of Operational Research},
  title        = {Clustering financial time series: New insights from an extended hidden Markov model},
  doi          = {10.1016/j.ejor.2014.12.041},
  number       = {3},
  pages        = {852--864},
  url          = {https://www.sciencedirect.com/science/article/abs/pii/S0377221714010595},
  volume       = {243},
  abstract     = {In recent years, large amounts of financial data have become available for analysis. We propose exploring returns from 21 European stock markets by model-based clustering of regime switching models. These econometric models identify clusters of time series with similar dynamic patterns and moreover allow relaxing assumptions of existing approaches, such as the assumption of conditional Gaussian returns.

The proposed model handles simultaneously the heterogeneity across stock markets and over time, i.e., time-constant and time-varying discrete latent variables capture unobserved heterogeneity between and within stock markets, respectively. The results show a clear distinction between two groups of stock markets, each one characterized by different regime switching dynamics that correspond to different expected return-risk patterns.

We identify three regimes: the so-called bull and bear regimes, as well as a stable regime with returns close to 0, which turns out to be the most frequently occurring regime. This is consistent with stylized facts in financial econometrics.},
  groups       = {Networks and investment management, Clustering and network analysis, Scenario_TimeSeries},
  keywords     = {Data mining; Hidden Markov model; Stock indexes; Latent class model; Regime-switching model;},
  owner        = {zkgst0c},
  timestamp    = {2020-02-27 03:56},
}

@Article{DiLascio-et-al-2018,
  author         = {Di Lascio, F. Marta L. and Giammusso, Davide and Puccetti, Giovanni},
  date           = {2018-11},
  journaltitle   = {Journal of banking \& finance},
  title          = {A clustering approach and a rule of thumb for risk aggregation},
  doi            = {10.1016/j.jbankfin.2018.07.002},
  issn           = {0378-4266},
  pages          = {236--248},
  volume         = {96},
  abstract       = {Abstract The problem of establishing reliable estimates or bounds for the (T)VaR of a joint risk portfolio is a relevant subject in connection with the computation of total economic capital in the Basel regulatory framework for the finance sector as well as with the Solvency regulations for the insurance sector. In the computation of total economic capital, a financial institution faces a considerable amount of model uncertainty related to the estimation of the interdependence amongst the marginal risks. In this paper, we propose to apply a clustering procedure in order to partition a risk portfolio into independent subgroups of positively dependent risks. Based on available data, the portfolio partition so obtained can be statistically validated and allows for a reduction of capital and the corresponding model uncertainty. We illustrate the proposed methodology in a simulation study and two case studies considering an Operational and a Market Risk portfolio. A rule of thumb stems from the various examples proposed: in a mathematical model where the risk portfolio is split into independent subsets with comonotonic dependence within, the smallest VaR-based capital estimate (at the high regulatory probability levels typically used) is produced by assuming that the infinite-mean risks are comonotonic and the finite-mean risks are independent. The largest VaR estimate is instead generated by obtaining the maximum number of independent infinite-mean sums.},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 03:56},
}

@Article{Ding-et-al-2017b,
  author         = {Ding, Jiajun and He, Xiongxiong and Yuan, Junqing and Jiang, Bo},
  date           = {2017-08-02},
  journaltitle   = {Soft Computing},
  title          = {Automatic clustering based on density peak detection using generalized extreme value distribution},
  doi            = {10.1007/s00500-017-2748-7},
  issn           = {1432-7643},
  number         = {9},
  pages          = {1--20},
  volume         = {22},
  abstract       = {Density peaks clustering (DPC) algorithm is able to get a satisfactory result with the help of artificial selecting the clustering centers, but such selection can be hard for a large amount of clustering tasks or the data set with a complex decision diagram. The purpose of this paper is to propose an automatic clustering approach without human intervention. Inspired by the visual selection rule of DPC, the judgment index which equals the lower value within density and distance (after normalization) is proposed for selecting the clustering centers. The judgment index approximately follows the generalized extreme value (GEV) distribution, and each clustering center judgment index is much higher. Hence, it is reasonable that the points are selected as clustering centers if their judgment indices are larger than the upper quantile of GEV. This proposed method is called density peaks clustering based on generalized extreme value distribution (DPC-GEV). Furthermore, taking the computational complexity into account, an alternative method based on density peak detection using Chebyshev inequality (DPC-CI) is also given. Experiments on both synthetic and real-world data sets show that DPC-GEV and DPC-CI can achieve the same accuracy as DPC on most data sets but consume much less time.},
  day            = {2},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 03:56},
}

@Article{Durante-et-al-2013,
  author               = {Durante, Fabrizio and Pappada, Roberta and Torelli, Nicola},
  date                 = {2013-12},
  journaltitle         = {Advances in Data Analysis and Classification},
  title                = {Clustering of financial time series in risky scenarios},
  doi                  = {10.1007/s11634-013-0160-4},
  issn                 = {1862-5347},
  number               = {4},
  pages                = {359--376},
  volume               = {8},
  abstract             = {A methodology is presented for clustering financial time series according to the association in the tail of their distribution. The procedure is based on the calculation of suitable pairwise conditional Spearman's correlation coefficients extracted from the series. The performance of the method has been tested via a simulation study. As an illustration, an analysis of the components of the Italian FTSE-MIB is presented. The results could be applied to construct financial portfolios that can manage to reduce the risk in case of simultaneous large losses in several markets.},
  citeulike-article-id = {14150074},
  citeulike-linkout-0  = {http://dx.doi.org/10.1007/s11634-013-0160-4},
  day                  = {22},
  groups               = {Networks and investment management, Scenario generation, Scenario_Market, Scenario_TimeSeries},
  owner                = {cristi},
  posted-at            = {2016-10-01 20:45:04},
  timestamp            = {2020-02-27 03:56},
}

@Article{Durante-et-al-2015,
  author               = {Durante, Fabrizio and Pappada, Roberta and Torelli, Nicola},
  date                 = {2015},
  journaltitle         = {Statistical Papers},
  title                = {Clustering of time series via non-parametric tail dependence estimation},
  doi                  = {10.1007/s00362-014-0605-7},
  number               = {3},
  pages                = {701--721},
  volume               = {56},
  abstract             = {We present a procedure for clustering time series according to their tail dependence behaviour as measured via a suitable copula-based tail coefficient, estimated in a non-parametric way. Simulation results about the proposed methodology together with an application to financial data are presented showing the usefulness of the proposed approach.},
  citeulike-article-id = {14150076},
  citeulike-linkout-0  = {http://dx.doi.org/10.1007/s00362-014-0605-7},
  citeulike-linkout-1  = {http://link.springer.com/article/10.1007/s00362-014-0605-7},
  groups               = {Networks and investment management},
  owner                = {cristi},
  posted-at            = {2016-10-01 20:45:35},
  publisher            = {Springer Berlin Heidelberg},
  timestamp            = {2020-02-27 03:56},
}

@InCollection{Durante-Pappada-2015,
  author               = {Durante, Fabrizio and Pappada, Roberta},
  booktitle            = {Strengthening Links Between Data Analysis and Soft Computing},
  date                 = {2015},
  title                = {Cluster Analysis of Time Series via Kendall Distribution},
  doi                  = {10.1007/978-3-319-10765-3\_25},
  editor               = {Grzegorzewski, Przemyslaw and Gagolewski, Marek and Hryniewicz, Olgierd and Gil, Mara},
  pages                = {209--216},
  publisher            = {Springer International Publishing},
  series               = {Advances in Intelligent Systems and Computing},
  volume               = {315},
  abstract             = {We present a method to cluster time series according to the calculation of the pairwise Kendall distribution function between them. A case study with environmental data illustrates the introduced methodology.},
  citeulike-article-id = {14150077},
  citeulike-linkout-0  = {http://dx.doi.org/10.1007/978-3-319-10765-325},
  citeulike-linkout-1  = {http://link.springer.com/chapter/10.1007/978-3-319-10765-325},
  groups               = {Networks and investment management},
  owner                = {cristi},
  posted-at            = {2016-10-01 20:46:12},
  timestamp            = {2020-02-27 03:56},
}

@InCollection{Durstewitz-2017e,
  author         = {Durstewitz, Daniel},
  booktitle      = {Advanced data analysis in neuroscience},
  date           = {2017},
  title          = {Clustering and density estimation},
  doi            = {10.1007/978-3-319-59976-2\_5},
  isbn           = {978-3-319-59974-8},
  pages          = {85--103},
  publisher      = {Springer International Publishing},
  series         = {Bernstein series in computational neuroscience},
  abstract       = {In classification approaches as described in Chap. 3, we have a training sample X with known class labels C, and we use this information either to estimate the conditional probabilities p(C = k), or to set up class boundaries (decision surfaces) by some other more direct criterion. In clustering we likewise assume that there is some underlying class structure in the data, just that we don know it and have no access to class labels C for our sample X, so that we have to infer it from X alone. This is also called an unsupervised statistical learning problem. In neurobiology this problem frequently occurs, for instance, when we suspect that neural cells in a brain area from their morphological and/or electrophysiological characteristics into different types, when gene sets cluster in functional pathways, when we believe that neural spiking patterns generated spontaneously in a given area are not arranged along a continuum but come from discrete categories (as possibly indicative of an attractor dynamics, see Chap. 9), or when rodents appear to utilize a discrete set of behavioral patterns or response strategies. In many such circumstances, we may feel that similarities between observations (observed feature sets) speak for an underlying mechanism that produces discrete types, but how could we extract such apparent structure and characterize it more formally? In fact, we may not just search for one such specific partition but may aim for a hierarchically nested set of partitions, that is, classes may split into subclasses and so on, as is the case with many natural categories and biological taxonomies. For instance, at a superordinate level, we may group cortical cell types into pyramidal cells and interneurons, which then in turn would split into several subclasses (like fast-spiking, bursting, etc.).},
  f1000-projects = {QuantInvest},
  issn           = {2520-{159X}},
  timestamp      = {2020-02-27 03:56},
}

@Article{Farrokhnia-Karimi-2016,
  author               = {Farrokhnia, Maryam and Karimi, Sadegh},
  date                 = {2016-01},
  journaltitle         = {Analytica Chimica Acta},
  title                = {Variable selection in multivariate calibration based on clustering of variable concept},
  doi                  = {10.1016/j.aca.2015.11.002},
  issn                 = {0003-2670},
  pages                = {70--81},
  volume               = {902},
  abstract             = {A new and efficient variable selection based on clustering of variable concept has been suggested for PLS. Selection the most useful variable is simple and straightforward. CLoVA concept can be used as alternative instead of using interval based variable selections for PLS. Analyses of different data sets indicate the superiority of CLoVA respect to available variable selection algorithms. Recently we have proposed a new variable selection algorithm, based on clustering of variable concept (CLoVA) in classification problem. With the same idea, this new concept has been applied to a regression problem and then the obtained results have been compared with conventional variable selection strategies for PLS. The basic idea behind the clustering of variable is that, the instrument channels are clustered into different clusters via clustering algorithms. Then, the spectral data of each cluster are subjected to PLS regression. Different real data sets (Cargill corn, Biscuit dough, ACE QSAR, Soy, and Tablet) have been used to evaluate the influence of the clustering of variables on the prediction performances of PLS. Almost in the all cases, the statistical parameter especially in prediction error shows the superiority of CLoVA-PLS respect to other variable selection strategies. Finally the synergy clustering of variable (sCLoVA-PLS), which is used the combination of cluster, has been proposed as an efficient and modification of CLoVA algorithm. The obtained statistical parameter indicates that variable clustering can split useful part from redundant ones, and then based on informative cluster; stable model can be reached.},
  citeulike-article-id = {14071205},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.aca.2015.11.002},
  owner                = {zkgst0c},
  posted-at            = {2016-06-17 22:22:46},
  timestamp            = {2020-02-27 03:56},
}

@Article{Fenn-et-al-2012,
  author               = {Fenn, Daniel J. and Porter, Mason A. and Mucha, Peter J. and McDonald, Mark and Williams, Stacy and Johnson, Neil F. and Jones, Nick S.},
  date                 = {2012-10-01},
  journaltitle         = {Quantitative Finance},
  title                = {Dynamical clustering of exchange rates},
  doi                  = {10.1080/14697688.2012.668288},
  number               = {10},
  pages                = {1493--1520},
  volume               = {12},
  abstract             = {We use techniques from network science to study correlations in the foreign exchange (FX) market during the period 1991?2008. We consider an FX market network in which each node represents an exchange rate and each weighted edge represents a time-dependent correlation between the rates. To provide insights into the clustering of the exchange-rate time series, we investigate dynamic communities in the network. We show that there is a relationship between an exchange rate's functional role within the market and its position within its community and use a node-centric community analysis to track the temporal dynamics of such roles. This reveals which exchange rates dominate the market at particular times and also identifies exchange rates that experienced significant changes in market role. We also use the community dynamics to uncover major structural changes that occurred in the FX market. Our techniques are general and will be similarly useful for investigating correlations in other markets.},
  citeulike-article-id = {14460851},
  citeulike-linkout-0  = {http://dx.doi.org/10.1080/14697688.2012.668288},
  citeulike-linkout-1  = {http://www.tandfonline.com/doi/abs/10.1080/14697688.2012.668288},
  day                  = {1},
  posted-at            = {2017-10-19 00:29:20},
  publisher            = {Routledge},
  timestamp            = {2020-02-27 03:56},
}

@Article{Ferraro-et-al-2019,
  author    = {Maria Brigida Ferraro and Paolo Giordani and Alessio Serafini},
  title     = {{fclust: An R Package for Fuzzy Clustering}},
  doi       = {10.32614/RJ-2019-017},
  url       = {https://doi.org/10.32614/RJ-2019-017},
  abstract  = {Fuzzy clustering methods discover fuzzy partitions where observations can be softly assigned to more than one cluster. The package fclust is a toolbox for fuzzy clustering in the R programming language. It not only implements the widely used fuzzy k-means (FkM) algorithm, but also many FkM variants. Fuzzy cluster similarity measures, cluster validity indices and cluster visualization tools are also offered. In the current version, all the functions are rewritten in the C++ language allowing their application in large-size problems. Moreover, new fuzzy relational clustering algorithms for partitioning qualitative/mixed data are provided together with an improved version of the so-called Gustafson-Kessel algorithm to avoid singularity in the cluster covariance matrices. Finally, it is now possible to automatically select the number of clusters by means of the available fuzzy cluster validity indices.},
  journal   = {{The R Journal}},
  timestamp = {2020-02-27 03:56},
  year      = {2019},
}

@Article{Ferreira-Zhao-2015,
  author               = {Ferreira, Leonardo N. and Zhao, Liang},
  date                 = {2015-08-19},
  journaltitle         = {Information Sciences},
  title                = {Time Series Clustering via Community Detection in Networks},
  doi                  = {10.1016/j.ins.2015.07.046},
  issn                 = {0020-0255},
  pages                = {227--242},
  volume               = {326},
  abstract             = {In this paper, we propose a technique for time series clustering using community detection in complex networks. Firstly, we present a method to transform a set of time series into a network using different distance functions, where each time series is represented by a vertex and the most similar ones are connected. Then, we apply community detection algorithms to identify groups of strongly connected vertices (called a community) and, consequently, identify time series clusters. Still in this paper, we make a comprehensive analysis on the influence of various combinations of time series distance functions, network generation methods and community detection techniques on clustering results. Experimental study shows that the proposed network-based approach achieves better results than various classic or up-to-date clustering techniques under consideration. Statistical tests confirm that the proposed method outperforms some classic clustering algorithms, such as k-medoids, diana, median-linkage and centroid-linkage in various data sets. Interestingly, the proposed method can effectively detect shape patterns presented in time series due to the topological structure of the underlying network constructed in the clustering process. At the same time, other techniques fail to identify such patterns. Moreover, the proposed method is robust enough to group time series presenting similar pattern but with time shifts and/or amplitude variations. In summary, the main point of the proposed method is the transformation of time series from time-space domain to topological domain. Therefore, we hope that our approach contributes not only for time series clustering, but also for general time series analysis tasks.},
  citeulike-article-id = {14042013},
  citeulike-linkout-0  = {http://arxiv.org/abs/1508.04757},
  citeulike-linkout-1  = {http://arxiv.org/pdf/1508.04757},
  citeulike-linkout-2  = {http://dx.doi.org/10.1016/j.ins.2015.07.046},
  day                  = {19},
  posted-at            = {2018-01-02 02:20:04},
  timestamp            = {2020-02-27 03:56},
}

@Article{Fop-Murphy-2017,
  author         = {Fop, Michael and Murphy, Thomas Brendan},
  date           = {2017-07-02},
  journaltitle   = {arXiv e-Print},
  title          = {Variable Selection Methods for Model-based Clustering},
  url            = {https://arxiv.org/abs/1707.00306},
  abstract       = {Model-based clustering is a popular approach for clustering multivariate data which has seen applications in numerous fields. Nowadays, high-dimensional data are more and more common and the model-based clustering approach has adapted to deal with the increasing dimensionality. In particular, the development of variable selection techniques has received a lot of attention and research effort in recent years. Even for small size problems, variable selection has been advocated to facilitate the interpretation of the clustering results. This review provides a summary of the methods developed for variable selection in model-based clustering. Existing R packages implementing the different methods are indicated and illustrated in application to two data analysis examples.},
  day            = {2},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 03:56},
}

@Article{Fragkiskos-Bauman-2018,
  author               = {Fragkiskos, Apollon and Bauman, Evgeny},
  date                 = {2018},
  journaltitle         = {SSRN e-Print},
  title                = {Factor Based Clustering},
  url                  = {https://ssrn.com/abstract=3089985},
  abstract             = {We propose a novel approach to cluster funds based on their factor exposures. The approach uses investment returns as input data and calculates similarity scores across funds, which are then used to form clusters. The derived clusters avoid common pitfalls that correlation based or other cluster methods fall into. They can be used as peer group alternatives to what vendors provide or to further refine existing categories that might be too obscure to make sense of. When tested against long/short equity funds, we find that we can form clusters with relatively high levels of stability across time.},
  citeulike-article-id = {14516008},
  citeulike-linkout-0  = {https://papers.ssrn.com/sol3/papers.cfm?abstractid=3089985},
  groups               = {Invest_Network},
  posted-at            = {2018-01-12 20:49:21},
  timestamp            = {2020-02-27 03:56},
}

@InCollection{Gagolewski-et-al-2016a,
  author               = {Gagolewski, Marek and Cena, Anna and Bartoszuk, Maciej},
  booktitle            = {Modeling Decisions for Artificial Intelligence},
  date                 = {2016},
  title                = {Hierarchical Clustering via Penalty-Based Aggregation and the Genie Approach},
  doi                  = {10.1007/978-3-319-45656-0\_16},
  editor               = {Torra, Vicenc and Narukawa, Yasuo and Navarro-Arribas, Guillermo and Yanez, Cristina},
  pages                = {191--202},
  publisher            = {Springer International Publishing},
  series               = {Lecture Notes in Computer Science},
  volume               = {9880},
  abstract             = {The paper discusses a generalization of the nearest centroid hierarchical clustering algorithm. A first extension deals with the incorporation of generic distance-based penalty minimizers instead of the classical aggregation by means of centroids. Due to that the presented algorithm can be applied in spaces equipped with an arbitrary dissimilarity measure (images, DNA sequences, etc.). Secondly, a correction preventing the formation of clusters of too highly unbalanced sizes is applied: just like in the recently introduced Genie approach, which extends the single linkage scheme, the new method averts a chosen inequity measure (e.g., the Gini-, de Vergottini-, or Bonferroni-index) of cluster sizes from raising above a predefined threshold. Numerous benchmarks indicate that the introduction of such a correction increases the quality of the resulting clusterings significantly.},
  citeulike-article-id = {14468579},
  citeulike-linkout-0  = {http://dx.doi.org/10.1007/978-3-319-45656-016},
  citeulike-linkout-1  = {http://link.springer.com/chapter/10.1007/978-3-319-45656-016},
  posted-at            = {2017-10-29 19:59:00},
  timestamp            = {2020-02-27 03:56},
}

@Article{Gan-2013,
  author               = {Gan, Guojun},
  date                 = {2013-11},
  journaltitle         = {Insurance: Mathematics and Economics},
  title                = {Application of data clustering and machine learning in variable annuity valuation},
  doi                  = {10.1016/j.insmatheco.2013.09.021},
  issn                 = {0167-6687},
  number               = {3},
  pages                = {795--801},
  volume               = {53},
  abstract             = {We study the pricing of a large portfolio of VA policies. A clustering method is used to select representative policies. A machine learning method is used to estimate the guarantee value. The proposed method performs well in terms of accuracy and speed. The valuation of variable annuity guarantees has been studied extensively in the past four decades. However, almost all the studies focus on the valuation of guarantees embedded in a single variable annuity contract. How to efficiently price the guarantees for a large portfolio of variable annuity contracts has not received enough attention. This paper fills the gap by introducing a novel method based on data clustering and machine learning to price the guarantees for a large portfolio of variable annuity contracts. Our test results show that this method performs very well in terms of accuracy and speed.},
  citeulike-article-id = {13934351},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.insmatheco.2013.09.021},
  groups               = {Networks and investment management, Machine learning and investment strategies, Annuities, ML_InvestSelect},
  owner                = {cristi},
  posted-at            = {2016-03-29 11:15:25},
  timestamp            = {2020-02-27 03:56},
}

@Article{Garvey-Madhavan-2019,
  author         = {Garvey, Gerald and Madhavan, Ananth},
  date           = {2019-09-09},
  journaltitle   = {The Journal of Financial Data Science},
  title          = {Reconstructing Emerging and Developed Markets Using Hierarchical Clustering},
  url            = {https://jfds.pm-research.com/content/early/2019/09/08/jfds.2019.1.014},
  urldate        = {2019-09-10},
  day            = {9},
  f1000-projects = {QuantInvest},
  publisher      = {Institutional Investor Journals Umbrella},
  timestamp      = {2020-02-27 03:56},
}

@Article{Gates-Ahn-2017,
  author               = {Gates, Alexander J. and Ahn, Yong-Yeol},
  date                 = {2017-01},
  journaltitle         = {arXiv e-Print},
  title                = {The Impact of Random Models on Clustering Similarity},
  eprint               = {1701.06508},
  eprinttype           = {arXiv},
  url                  = {https://arxiv.org/abs/1701.06508},
  abstract             = {Clustering is a central approach for unsupervised learning. After clustering is applied, the most fundamental analysis is to quantitatively compare clusterings. Such comparisons are crucial for the evaluation of clustering methods as well as other tasks such as consensus clustering. It is often argued that, in order to establish a baseline, clustering similarity should be assessed in the context of a random ensemble of clusterings. The prevailing assumption for the random clustering ensemble is the permutation model in which the number and sizes of clusters are fixed. However, this assumption does not necessarily hold in practice; for example, while multiple runs of K-means clustering returns clusterings with a fixed number of clusters, the cluster size distribution varies greatly. Here, we derive corrected variants of two clustering similarity measures (the Rand index and Mutual Information) in the context of two random clustering ensembles in which the number and sizes of clusters vary. In addition, we study the impact of one-sided comparisons in the scenario with a reference clustering. The consequences of different random models are illustrated using synthetic examples, handwriting recognition, and gene expression data. We demonstrate that the choice of random model can have a drastic impact on results and argue that the choice should be carefully justified.},
  citeulike-article-id = {14262970},
  citeulike-linkout-0  = {http://arxiv.org/abs/1701.06508},
  citeulike-linkout-1  = {http://arxiv.org/pdf/1701.06508},
  day                  = {23},
  posted-at            = {2017-01-26 15:41:14},
  timestamp            = {2020-02-27 03:56},
}

@Article{Ghoshdastidar-et-al-2018,
  author         = {Ghoshdastidar, Debarghya and Perrot, Michael and von Luxburg, Ulrike},
  date           = {2018-11-02},
  journaltitle   = {arXiv e-Print},
  title          = {Foundations of Comparison-Based Hierarchical Clustering},
  url            = {https://arxiv.org/abs/1811.00928},
  urldate        = {2019-12-18},
  abstract       = {We address the classical problem of hierarchical clustering, but in a framework where one does not have access to a representation of the objects or their pairwise similarities. Instead, we assume that only a set of comparisons between objects is available, that is, statements of the form "objects ii and jj are more similar than objects kk and ll." Such a scenario is commonly encountered in crowdsourcing applications. The focus of this work is to develop comparison-based hierarchical clustering algorithms that do not rely on the principles of ordinal embedding. We show that single and complete linkage are inherently comparison-based and we develop variants of average linkage. We provide statistical guarantees for the different methods under a planted hierarchical partition model. We also empirically demonstrate the performance of the proposed approaches on several datasets.},
  day            = {2},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 03:56},
}

@Article{Goswami-et-al-2017,
  author         = {Goswami, Saptarsi and Das, Amit Kumar and Chakrabarti, Amlan and Chakraborty, Basabi},
  date           = {2017-08},
  journaltitle   = {Expert Systems with Applications},
  title          = {A feature cluster taxonomy based feature selection technique},
  doi            = {10.1016/j.eswa.2017.01.044},
  issn           = {0957-4174},
  pages          = {76--89},
  volume         = {79},
  abstract       = {Feature subset selection is basically an optimization problem for choosing the most important features from various alternatives in order to facilitate classification or mining problems. Though lots of algorithms have been developed so far, none is considered to be the best for all situations and researchers are still trying to come up with better solutions. In this work, a flexible and user-guided feature subset selection algorithm, named as FCTFS (Feature Cluster Taxonomy based Feature Selection) has been proposed for selecting suitable feature subset from a large feature set. The proposed algorithm falls under the genre of clustering based feature selection techniques in which features are initially clustered according to their intrinsic characteristics following the filter approach. In the second step the most suitable feature is selected from each cluster to form the final subset following a wrapper approach. The two stage hybrid process lowers the computational cost of subset selection, especially for large feature data sets. One of the main novelty of the proposed approach lies in the process of determining optimal number of feature clusters. Unlike currently available methods, which mostly employ a trial and error approach, the proposed method characterises and quantifies the feature clusters according to the quality of the features inside the clusters and defines a taxonomy of the feature clusters. The selection of individual features from a feature cluster can be done judiciously considering both the relevancy and redundancy according to user intention and requirement. The algorithm has been verified by simulation experiments with different bench mark data set containing features ranging from 10 to more than 800 and compared with other currently used feature selection algorithms. The simulation results prove the superiority of our proposal in terms of model performance, flexibility of use in practical problems and extendibility to large feature sets. Though the current proposal is verified in the domain of unsupervised classification, it can be easily used in case of supervised classification.},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 03:56},
}

@Article{Grun-2018,
  author         = {Grun, Bettina},
  date           = {2018-07-05},
  journaltitle   = {arXiv e-Print},
  title          = {Model-based Clustering},
  url            = {https://arxiv.org/abs/1807.01987},
  abstract       = {Mixture models extend the toolbox of clustering methods available to the data analyst. They allow for an explicit definition of the cluster shapes and structure within a probabilistic framework and exploit estimation and inference techniques available for statistical models in general. In this chapter an introduction to cluster analysis is provided, model-based clustering is related to standard heuristic clustering methods and an overview on different ways to specify the cluster model is given. Post-processing methods to determine a suitable clustering, infer cluster distribution characteristics and validate the cluster solution are discussed. The versatility of the model-based clustering approach is illustrated by giving an overview on the different areas of applications.},
  day            = {5},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 03:56},
}

@Article{Guenther-Lee-2016,
  author               = {Guenther, John and Lee, Herbert K. H.},
  date                 = {2016},
  journaltitle         = {Applied Mathematics},
  title                = {Cluster Search Algorithm for Finding Multiple Optima},
  doi                  = {10.4236/am.2016.77067},
  issn                 = {2152-7385},
  number               = {07},
  pages                = {736--752},
  volume               = {07},
  abstract             = {The black box functions found in computer experiments often result in multimodal optimization programs. Optimization that focuses on a single best optimum may not achieve the goal of getting the best answer for the purposes of the experiment. This paper builds upon an algorithm introduced in [1] that is successful for finding multiple optima within the input space of the objective function. Here we introduce an alternative cluster search algorithm for finding these optima, making use of clustering. The cluster search algorithm has several advantages over the earlier algorithm. It gives a forward view of the optima that are present in the input space so the user has a preview of what to expect as the optimization process continues. It employs pattern search, in many instances, closer to the minimum's location in input space, saving on simulator point computations. At termination, this algorithm does not need additional verification that a minimum is a duplicate of a previously found minimum, which also saves on simulator point computations. Finally, it finds minima that can be "hidden" by close larger minima.},
  citeulike-article-id = {14497899},
  citeulike-linkout-0  = {http://dx.doi.org/10.4236/am.2016.77067},
  posted-at            = {2017-12-06 19:43:47},
  timestamp            = {2020-02-27 03:56},
}

@Article{GuijoRubio-et-al-2018,
  author         = {Guijo-Rubio, David and Duran-Rosal, Antonio Manuel and Gutierrez, Pedro Antonio and Troncoso, Alicia and Hervas-Martinez, Cesar},
  date           = {2018-10-27},
  journaltitle   = {arXiv e-Print},
  title          = {Time series clustering based on the characterisation of segment typologies},
  url            = {https://arxiv.org/abs/1810.11624},
  abstract       = {Time series clustering is the process of grouping time series with respect to their similarity or characteristics. Previous approaches usually combine a specific distance measure for time series and a standard clustering method. However, these approaches do not take the similarity of the different subsequences of each time series into account, which can be used to better compare the time series objects of the dataset. In this paper, we propose a novel technique of time series clustering based on two clustering stages. In a first step, a least squares polynomial segmentation procedure is applied to each time series, which is based on a growing window technique that returns different-length segments. Then, all the segments are projected into same dimensional space, based on the coefficients of the model that approximates the segment and a set of statistical features. After mapping, a first hierarchical clustering phase is applied to all mapped segments, returning groups of segments for each time series. These clusters are used to represent all time series in the same dimensional space, after defining another specific mapping process. In a second and final clustering stage, all the time series objects are grouped. We consider internal clustering quality to automatically adjust the main parameter of the algorithm, which is an error threshold for the segmenta- tion. The results obtained on 84 datasets from the UCR Time Series Classification Archive have been compared against two state-of-the-art methods, showing that the performance of this methodology is very promising.},
  day            = {27},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 03:56},
}

@Article{Gu-Louca-2019,
  author         = {Gu, Ariel and Louca, Christodoulos},
  date           = {2019},
  journaltitle   = {SSRN e-Print},
  title          = {What drives new mutual fund clustering?},
  doi            = {10.2139/ssrn.3409491},
  issn           = {1556-5068},
  url            = {https://www.ssrn.com/abstract=3409491},
  urldate        = {2019-11-28},
  abstract       = {Over time, the aggregate new mutual fund volume is considerably larger in markets. The naive explanation that new fund volume correlates with the economic environment is incomplete because active mutual funds, on average, underperform. However, we demonstrate that fund families exploit IPO-related investment opportunities, which correlate cross-sectionally for economy-wide reasons, more by creating new funds than by using existing funds. This naturally leads to new fund volume clustering that is correlated with the economic environment. In addition, consistent with fund families strategically exploiting the economic environment, new funds with access to IPO offerings outperform and attract higher investment flows.},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 03:56},
}

@InCollection{Gupta-Chatterjee-2018,
  author               = {Gupta, Kartikay and Chatterjee, Niladri},
  booktitle            = {Information and Communication Technology for Intelligent Systems (ICTIS 2017) - Volume 2},
  date                 = {2018},
  title                = {Financial Time Series Clustering},
  doi                  = {10.1007/978-3-319-63645-0\_16},
  editor               = {Satapathy, Suresh C. and Joshi, Amit},
  pages                = {146--156},
  publisher            = {Springer International Publishing},
  series               = {Smart Innovation, Systems and Technologies},
  volume               = {84},
  abstract             = {Financial time series clustering finds application in forecasting, noise reduction and enhanced index tracking. The central theme in all the available clustering algorithms is the dissimilarity measure employed by the algorithm. The dissimilarity measures, applicable in financial domain, as used or suggested in past researches, are correlation based dissimilarity measure, temporal correlation based dissimilarity measure and dynamic time wrapping (DTW) based dissimilarity measure. One shortcoming of these dissimilarity measures is that they do not take into account the lead or lag existing between the returns of different stocks which changes with time. Mostly, such stocks with high value of correlation at some lead or lag belong to the same cluster (or sector). The present paper, proposes two new dissimilarity measures which show superior clustering results as compared to past measures when compared over 3 data sets comprising of 526 companies.},
  citeulike-article-id = {14435138},
  citeulike-linkout-0  = {http://dx.doi.org/10.1007/978-3-319-63645-016},
  citeulike-linkout-1  = {http://link.springer.com/chapter/10.1007/978-3-319-63645-016},
  groups               = {FrcstQWIM_TimeSrs},
  posted-at            = {2017-09-21 00:23:24},
  timestamp            = {2020-02-27 03:56},
}

@Article{Han-Ge-2020,
  author         = {Han, Jingti and Ge, Zhipeng},
  date           = {2020-01},
  journaltitle   = {Expert systems with applications},
  title          = {Effect of dimensionality reduction on stock selection with cluster analysis in different market situations},
  doi            = {10.1016/j.eswa.2020.113226},
  issn           = {0957-4174},
  pages          = {113226},
  url            = {https://linkinghub.elsevier.com/retrieve/pii/{S095741742030052X}},
  urldate        = {2020-01-23},
  abstract       = {Dimensionality reduction is inevitable in stock selection with cluster analysis. Considering relations among dimensionality reduction, noise trading, and market situations, we empirically investigate the effect of dimensionality-reduction methods-principal component analysis, stacked autoencoder, and stacked restricted Boltzmann machine-on stock selection with cluster analysis in different market situations. Based on the index fluctuation, the market is divided into sideways and trend situations. For the CSI 100 and Nikkei 225 constituent stocks, experimental results show that: (1) in sideways situations, dimensionality reduction hardly improves the performance of stock selection with cluster analysis; (2) the advantage of dimensionality reduction is mainly reflected in trend situations, but whether it is in an up or down trend depends on the market analyzed. More importantly, according to the above findings and assuming that the dimensionality-reduction effect will continue, we propose a rotation strategy with and without dimensionality reduction. The results of experiments show that the proposed rotation strategy outperforms the stock market indices as well as the stock-selection strategies based on dimensionality reduction and cluster analysis. These findings offer practical insights into how dimensionality reduction can be efficiently used for stock selection.},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 03:56},
}

@Article{Hennig-et-al-2018,
  author         = {Hennig, Christian and Viroli, Cinzia and Anderlucci, Laura},
  date           = {2018-06-27},
  journaltitle   = {arXiv e-Print},
  title          = {Quantile-based clustering},
  url            = {https://arxiv.org/abs/1806.10403},
  abstract       = {A new cluster analysis method, K-quantiles clustering, is introduced. K-quantiles clustering can be computed by a simple greedy algorithm in the style of the classical Lloyd's algorithm for K-means. It can be applied to large and high-dimensional datasets. It allows for within-cluster skewness and internal variable scaling based on within-cluster variation. Different versions allow for different levels of parsimony and computational efficiency. Although K-quantiles clustering is conceived as nonparametric, it can be connected to a fixed partition model of generalized asymmetric Laplace-distributions. The consistency of K-quantiles clustering is proved, and it is shown that K-quantiles clusters correspond to well separated mixture components in a nonparametric mixture. In a simulation, K-quantiles clustering is compared with a number of popular clustering methods with good results. A high-dimensional microarray dataset is clustered by K-quantiles.},
  day            = {27},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 03:56},
}

@Article{Hofmeyr-2017,
  author               = {Hofmeyr, David P.},
  date                 = {2017-08-01},
  journaltitle         = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  title                = {Clustering by Minimum Cut Hyperplanes},
  doi                  = {10.1109/tpami.2016.2609929},
  issn                 = {0162-8828},
  number               = {8},
  pages                = {1547--1560},
  volume               = {39},
  abstract             = {Minimum normalised graph cuts are highly effective ways of partitioning unlabeled data, having been made popular by the success of spectral clustering. This work presents a novel method for learning hyperplane separators which minimise this graph cut objective, when data are embedded in Euclidean space. The optimisation problem associated with the proposed method can be formulated as a sequence of univariate subproblems, in which the optimal hyperplane orthogonal to a given vector is determined. These subproblems can be solved in log-linear time, by exploiting the trivial factorisation of the exponential function. Experimentation suggests that the empirical runtime of the overall algorithm is also log-linear in the number of data. Asymptotic properties of the minimum cut hyperplane, both for a finite sample, and for an increasing sample assumed to arise from an underlying probability distribution are discussed. In the finite sample case the minimum cut hyperplane converges to the maximum margin hyperplane as the scaling parameter is reduced to zero. Applying the proposed methodology, both for fixed scaling, and the large margin asymptotes, is shown to produce high quality clustering models in comparison with state-of-the-art clustering algorithms in experiments using a large collection of benchmark datasets.},
  citeulike-article-id = {14486419},
  citeulike-linkout-0  = {http://dx.doi.org/10.1109/tpami.2016.2609929},
  day                  = {1},
  posted-at            = {2017-11-30 19:03:59},
  timestamp            = {2020-02-27 03:56},
}

@InCollection{Hofmeyr-Pavlidis-2015,
  author               = {Hofmeyr, David and Pavlidis, Nicos},
  booktitle            = {IEEE Symposium Series on Computational Intelligence},
  date                 = {2015-12},
  title                = {Maximum Clusterability Divisive Clustering},
  doi                  = {10.1109/ssci.2015.116},
  isbn                 = {978-1-4799-7560-0},
  location             = {Cape Town, South Africa},
  pages                = {780--786},
  publisher            = {IEEE},
  abstract             = {The notion of cluster ability is often used to determine how strong the cluster structure within a set of data is, as well as to assess the quality of a clustering model. In multivariate applications, however, the cluster ability of a data set can be obscured by irrelevant or noisy features. We study the problem of finding low dimensional projections which maximise the cluster ability of a data set. In particular, we seek low dimensional representations of the data which maximise the quality of a binary partition. We use this bi-partitioning recursively to generate high quality clustering models. We illustrate the improvement over standard dimension reduction and clustering techniques, and evaluate our method in experiments on real and simulated data sets.},
  citeulike-article-id = {14486417},
  citeulike-linkout-0  = {http://dx.doi.org/10.1109/ssci.2015.116},
  posted-at            = {2017-11-30 19:02:58},
  timestamp            = {2020-02-27 03:56},
}

@InProceedings{Holst-et-al-2019,
  author         = {Holst, Anders and Bae, Juhee and Karlsson, Alexander and Bouguelia, Mohamed-Rafik},
  booktitle      = {Proceedings of the Workshop on Interactive Data Mining - WIDM'19},
  date           = {2019-02-15},
  title          = {Interactive clustering for exploring multiple data streams at different time scales and granularity},
  doi            = {10.1145/3304079.3310286},
  isbn           = {9781450362962},
  location       = {New York, New York, USA},
  pages          = {1--7},
  publisher      = {ACM Press},
  url            = {http://dl.acm.org/citation.cfm?doid=3304079.3310286},
  urldate        = {2019-09-06},
  abstract       = {We approach the problem of identifying and interpreting clusters over different time scales and granularity in multivariate time series data. We extract statistical features over a sliding window of each time series, and then use a Gaussian mixture model to identify clusters which are then projected back on the data streams. The human analyst can then further analyze this projection and adjust the size of the sliding window and the number of clusters in order to capture the different types of clusters over different time scales. We demonstrate the effectiveness of our approach in two different application scenarios: (1) fleet management and (2) district heating, wherein each scenario, several different types of meaningful clusters can be identified when varying over these dimensions.},
  day            = {15},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 03:56},
}

@Article{Hong-et-al-2017,
  author               = {Hong, Dezhi and Gu, Quanquan and Whitehouse, Kamin},
  date                 = {2017},
  journaltitle         = {Proceedings of Machine Learning Research},
  title                = {High-dimensional Time Series Clustering via Cross-Predictability},
  url                  = {http://proceedings.mlr.press/v54/hong17a.html},
  abstract             = {The key to time series clustering is how to characterize the similarity between any two time series. In this paper, we explore a new similarity metric called "cross-predictability": the degree to which a future value in each time series is predicted by past values of the others. However, it is challenging to estimate such cross-predictability among time series in the high-dimensional regime, where the number of time series is much larger than the length of each time series. We address this challenge with a sparsity assumption: only time series in the same cluster have significant cross-predictability with each other. We demonstrate that this approach is computationally attractive, and provide a theoretical proof that the proposed algorithm will identify the correct clustering structure with high probability under certain conditions. To the best of our knowledge, this is the first practical high-dimensional time series clustering algorithm with a provable guarantee. We evaluate with experiments on both synthetic data and real-world data, and results indicate that our method can achieve more than 80\% clustering accuracy on real-world data, which is 20\% higher than the state-of-art baselines.},
  citeulike-article-id = {14435135},
  groups               = {Predictability_FinInfo, ML_ClustTimeSrs},
  posted-at            = {2017-09-20 23:56:17},
  timestamp            = {2020-02-27 03:56},
}

@Article{Horel-et-al-2019,
  author         = {Horel, Enguerrand and Giesecke, Kay and Storchan, Victor and Chittar, Naren},
  date           = {2019},
  journaltitle   = {arXiv e-Print},
  title          = {Explainable Clustering and Application to Wealth Management Compliance},
  url            = {https://arxiv.org/abs/1909.13381},
  urldate        = {2019-09-07},
  abstract       = {Many applications from the financial industry successfully leverage clustering algorithms to reveal meaningful patterns among a vast amount of unstructured financial data. However, these algorithms suffer from a lack of interpretability that is required both at a business and regulatory level. In order to overcome this issue, we propose a novel two-steps method to explain clusters. A classifier is first trained to predict the clusters labels, then the Single Feature Introduction Test (SFIT) method is run on the model to identify the statistically significant features that characterise each cluster. We describe a real wealth management compliance use-case that highlights the necessity of such an interpretable clustering method. We illustrate the performance of our method through an experiment on financial ratios of U.S. companies.},
  day            = {23},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 03:56},
}

@Article{Huang-et-al-2013,
  author               = {Huang, Hanwen and Liu, Yufeng and Yuan, Ming and Marron, J. S.},
  date                 = {2013},
  journaltitle         = {arXiv e-Print},
  title                = {Statistical Significance of Clustering using Soft Thresholding},
  eprint               = {1305.5879},
  eprinttype           = {arXiv},
  url                  = {https://arxiv.org/abs/1305.5879},
  abstract             = {Clustering methods have led to a number of important discoveries in bioinformatics and beyond. A major challenge in their use is determining which clusters represent important underlying structure, as opposed to spurious sampling artifacts. This challenge is especially serious, and very few methods are available when the data are very high in dimension. Statistical Significance of Clustering (SigClust) is a recently developed cluster evaluation tool for high dimensional low sample size data. An important component of the SigClust approach is the very definition of a single cluster as a subset of data sampled from a multivariate Gaussian distribution. The implementation of SigClust requires the estimation of the eigenvalues of the covariance matrix for the null multivariate Gaussian distribution. We show that the original eigenvalue estimation can lead to a test that suffers from severe inflation of type-I error, in the important case where there are huge single spikes in the eigenvalues. This paper addresses this critical challenge using a novel likelihood based soft thresholding approach to estimate these eigenvalues which leads to a much improved SigClust. These major improvements in SigClust performance are shown by both theoretical work and an extensive simulation study. Applications to some cancer genomic data further demonstrate the usefulness of these improvements.},
  citeulike-article-id = {14444684},
  citeulike-linkout-0  = {http://arxiv.org/abs/1305.5879},
  citeulike-linkout-1  = {http://arxiv.org/pdf/1305.5879},
  day                  = {29},
  posted-at            = {2017-10-03 17:02:54},
  timestamp            = {2020-02-27 03:56},
  year                 = {2013},
}

@Article{Huang-et-al-2017a,
  author               = {Huang, Jinlong and Zhu, Qingsheng and Yang, Lijun and Cheng, Dongdong and Wu, Quanwang},
  date                 = {2017},
  journaltitle         = {Machine Learning},
  title                = {QCC: a novel clustering algorithm based on Quasi-Cluster Centers},
  doi                  = {10.1007/s10994-016-5608-2},
  number               = {3},
  pages                = {337--357},
  volume               = {106},
  abstract             = {Cluster analysis aims at classifying objects into categories on the basis of their similarity and has been widely used in many areas such as pattern recognition and image processing. In this paper, we propose a novel clustering algorithm called QCC mainly based on the following ideas: the density of a cluster center is the highest in its K nearest neighborhood or reverse K nearest neighborhood, and clusters are divided by sparse regions. Besides, we define a novel concept of similarity between clusters to solve the complex-manifold problem. In experiments, we compare the proposed algorithm QCC with DBSCAN, DP and DAAP algorithms on synthetic and real-world datasets. Results show that QCC performs the best, and its superiority on clustering non-spherical data and complex-manifold data is especially large.},
  citeulike-article-id = {14334821},
  citeulike-linkout-0  = {http://dx.doi.org/10.1007/s10994-016-5608-2},
  citeulike-linkout-1  = {http://link.springer.com/article/10.1007/s10994-016-5608-2},
  posted-at            = {2017-04-10 01:08:53},
  publisher            = {Springer US},
  timestamp            = {2020-02-27 03:56},
}

@Article{Huang-Ribeiro-2016,
  author               = {Huang, Weiyu and Ribeiro, Alejandro},
  date                 = {2016-10},
  journaltitle         = {arXiv e-Print},
  title                = {Hierarchical Clustering Given Confidence Intervals of Metric Distances},
  eprint               = {1610.04274},
  eprinttype           = {arXiv},
  url                  = {https://arxiv.org/abs/1610.04274},
  abstract             = {This paper considers metric spaces where distances between a pair of nodes are represented by distance intervals. The goal is to study methods for the determination of hierarchical clusters, i.e., a family of nested partitions indexed by a resolution parameter, induced from the given distance intervals of the metric spaces. Our construction of hierarchical clustering methods is based on defining admissible methods to be those methods that abide to the axioms of value - nodes in a metric space with two nodes are clustered together at the convex combination of the distance bounds between them - and transformation - when both distance bounds are reduced, the output may become more clustered but not less. Two admissible methods are constructed and are shown to provide universal upper and lower bounds in the space of admissible methods. Practical implications are explored by clustering moving points via snapshots and by clustering networks representing brain structural connectivity using the lower and upper bounds of the network distance. The proposed clustering methods succeed in identifying underlying clustering structures via the maximum and minimum distances in all snapshots, as well as in differentiating brain connectivity networks of patients from those of healthy controls.},
  citeulike-article-id = {14170685},
  citeulike-linkout-0  = {http://arxiv.org/abs/1610.04274},
  citeulike-linkout-1  = {http://arxiv.org/pdf/1610.04274},
  day                  = {13},
  groups               = {Networks and investment management, Clustering and network analysis},
  posted-at            = {2017-05-16 14:06:50},
  timestamp            = {2020-02-27 03:56},
}

@Article{Isogai-2017,
  author               = {Isogai, Takashi},
  date                 = {2017-05},
  journaltitle         = {Applied Network Science},
  title                = {Dynamic correlation network analysis of financial asset returns with network clustering},
  doi                  = {10.1007/s41109-017-0031-6},
  issn                 = {2364-8228},
  number               = {1},
  volume               = {2},
  abstract             = {In this study, we propose a novel approach to analyze a dynamic correlation network of highly volatile financial asset returns by using a network clustering algorithm to deal with high dimensionality issues. We analyze the dynamic correlation network of selected Japanese stock returns as an empirical study of the correlation dynamics at the market level by applying the proposed method. Two types of network clustering algorithms are employed for the dimensionality reduction. Firstly, several stock groups instead of the existing business sector classification are generated by the hierarchical recursive network clustering of filtered stock returns in order to overcome the high dimensionality problem due to the large number of stocks. The stock returns are then filtered in advance to control for volatility fluctuations that can distort the correlation between stocks. Thus, the correlation network of individual stock returns is transformed into a correlation network of group-based portfolio returns. Secondly, the reduced size of the correlation network is extended to a dynamic one by using a model-based correlation estimation method. A time series of adjacency matrices is created on a daily basis as a dynamic correlation network from the estimation results. Then, the correlation network is summarized into only three representative correlation networks by clustering along the time axis. Some intertemporal comparisons of the dynamic correlation network are conducted by examining the differences between the three sub-period networks. Our dynamic correlation network analysis framework is not limited to stock returns, but can be applied to many other financial and non-financial volatile time series data.},
  citeulike-article-id = {14399609},
  citeulike-linkout-0  = {http://dx.doi.org/10.1007/s41109-017-0031-6},
  day                  = {23},
  groups               = {Networks and investment management, Clustering and network analysis, Invest_Network, Vol_Cluster},
  posted-at            = {2017-07-26 14:35:01},
  timestamp            = {2020-02-27 03:56},
}

@Article{Izakian-et-al-2014,
  author               = {Izakian, Hesam and Pedrycz, Witold and Jamal, Iqbal},
  date                 = {2015-03},
  journaltitle         = {Engineering Applications of Artificial Intelligence},
  title                = {Fuzzy clustering of time series data using dynamic time warping distance},
  doi                  = {10.1016/j.engappai.2014.12.015},
  issn                 = {0952-1976},
  pages                = {235--244},
  volume               = {39},
  abstract             = {Clustering is a powerful vehicle to reveal and visualize structure of data. When dealing with time series, selecting a suitable measure to evaluate the similarities/dissimilarities within the data becomes necessary and subsequently it exhibits a significant impact on the results of clustering. This selection should be based upon the nature of time series and the application itself. When grouping time series based on their shape information is of interest (shape-based clustering), using a Dynamic Time Warping (DTW) distance is a desirable choice. Using stretching or compressing segments of temporal data, DTW determines an optimal match between any two time series. In this way, time series exhibiting similar patterns occurring at different time periods, are considered as being similar. Although DTW is a suitable choice for comparing data with respect to their shape information, calculating the average of a collection of time series (which is required in clustering methods) based on this distance becomes a challenging problem. As the result, employing clustering techniques like K-Means and Fuzzy C-Means (where the cluster centers - prototypes are calculated through averaging the data) along with the DTW distance is a challenging task and may produce unsatisfactory results. In this study, three alternatives for fuzzy clustering of time series using DTW distance are proposed. In the first method, a DTW-based averaging technique proposed in the literature, has been applied to the Fuzzy C-Means clustering. The second method considers a Fuzzy C-Medoids clustering, while the third alternative comes as a hybrid technique, which exploits the advantages of both the Fuzzy C-Means and Fuzzy C-Medoids when clustering time series. Experimental studies are reported over a set of time series coming from the UCR time series database.},
  citeulike-article-id = {14485204},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.engappai.2014.12.015},
  groups               = {Scenario_TimeSeries},
  posted-at            = {2017-11-28 14:19:15},
  timestamp            = {2020-02-27 03:56},
}

@InProceedings{Jha-et-al-2015,
  author         = {Jha, Abhay and Ray, Shubhankar and Seaman, Brian and Dhillon, Inderjit S.},
  booktitle      = {2015 IEEE 31st International Conference on Data Engineering},
  date           = {2015-04-13},
  title          = {Clustering to forecast sparse time-series data},
  doi            = {10.1109/{ICDE}.2015.7113385},
  isbn           = {978-1-4799-7964-6},
  pages          = {1388--1399},
  publisher      = {IEEE},
  url            = {http://ieeexplore.ieee.org/document/7113385/},
  urldate        = {2019-09-11},
  abstract       = {Forecasting accurately is essential to successful inventory planning in retail. Unfortunately, there is not always enough historical data to forecast items individually- this is particularly true in e-commerce where there is a long tail of low selling items, and items are introduced and phased out quite frequently, unlike physical stores. In such scenarios, it is preferable to forecast items in well-designed groups of similar items, so that data for different items can be pooled together to fit a single model. In this paper, we first discuss the desiderata for such a grouping and how it differs from the traditional clustering problem. We then describe our approach which is a scalable local search heuristic that can naturally handle the constraints required in this setting, besides being capable of producing solutions competitive with well-known clustering algorithms. We also address the complementary problem of estimating similarity, particularly in the case of new items which have no past sales. Our solution is to regress the sales profile of items against their semantic features, so that given just the semantic features of a new item we can predict its relation to other items, in terms of as yet unobserved sales. Our experiments demonstrate both the scalability of our approach and implications for forecast accuracy.},
  day            = {13},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 04:01},
}

@Article{Jiang-et-al-2014a,
  author               = {Jiang, Kening and Li, Duan and Gao, Jianjun and Yu, Jeffrey X.},
  date                 = {2014},
  journaltitle         = {IFAC Proceedings Volumes},
  title                = {Factor Model Based Clustering Approach for Cardinality Constrained Portfolio Selection},
  doi                  = {10.3182/20140824-6-za-1003.00663},
  issn                 = {1474-6670},
  number               = {3},
  pages                = {10713--10718},
  volume               = {47},
  abstract             = {Portfolio selection concerns identifying an optimal composition of various risky assets and their corresponding holding amounts such that the corresponding investment strategy strikes a balance between maximizing the expected investment return and minimizing investment risk. While market frictions make full diversification impractical, cardinality constrained mean-variance (CCMV) portfolio selection problem emerges as a natural remedy: Given an asset pool with total n assets and a given cardinality s n, optimally choose s assets from the entire asset pool such as to achieve a mean-variance efficiency. Unfortunately, CCMV has been proved to be NP hard and has been posted in front of optimization society as a long-standing challenge. By invoking structural market information and utilizing fast clustering algorithm for classification, we develop in this paper an effective heuristic scheme to identify approximate solutions for large-scale CCMV problems. More specifically, by constructing grouping constraints generated from factor-model based clustering algorithm and attaching them to the mixed integer programming formulation associated with the CCMV problem, we are able to significantly reduce the computational complexity, thus offering a fast algorithm with relatively high quality solution.},
  citeulike-article-id = {14321944},
  citeulike-linkout-0  = {http://dx.doi.org/10.3182/20140824-6-za-1003.00663},
  groups               = {Networks and investment management, Clustering and network analysis, Network_Invest, PortfOptim_Network, Invest_Network},
  posted-at            = {2017-03-28 18:07:33},
  timestamp            = {2020-02-27 04:01},
}

@Article{Ji-et-al-2018,
  author         = {Ji, Hao and Wang, Hao and Liseo, Brunero},
  date           = {2018-09},
  journaltitle   = {Australian economic papers},
  title          = {Portfolio Diversification Strategy Via Tail-Dependence Clustering and ARMA-GARCH Vine Copula Approach},
  doi            = {10.1111/1467-8454.12126},
  issn           = {0004-900X},
  number         = {3},
  pages          = {265--283},
  urldate        = {2019-12-04},
  volume         = {57},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 04:01},
}

@Article{Joe-Sang-2016,
  author               = {Joe, Harry and Sang, Peijun},
  date                 = {2016-05},
  journaltitle         = {Computational Statistics \& Data Analysis},
  title                = {Multivariate models for dependent clusters of variables with conditional independence given aggregation variables},
  doi                  = {10.1016/j.csda.2015.12.001},
  issn                 = {0167-9473},
  pages                = {114--132},
  volume               = {97},
  abstract             = {A general multivariate distributional approach, with conditional independence given aggregation variables, is presented to combine group-based submodels when variables are naturally divided into several non-overlapping groups. When the distributions are all multivariate Gaussian, the dependence among different groups is parsimonious based on conditional independence given linear combinations of variables in each group. For the case of multivariate t distributions in each group, a grouped t distribution is obtained. The approach can be extended so that the copula for each group is based on a skew-t distribution, and an application of this is given to financial returns of stocks in several different sectors. Another example of the modeling approach is given with variables separated into groups based on their units of measurements.},
  citeulike-article-id = {14219060},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.csda.2015.12.001},
  groups               = {Data_Independence},
  owner                = {cristi},
  posted-at            = {2016-12-02 15:44:07},
  timestamp            = {2020-02-27 04:01},
}

@Article{Jung-Chang-2016,
  author               = {Jung, Sean S. and Chang, Woojin},
  date                 = {2016-11},
  journaltitle         = {Physica A: Statistical Mechanics and its Applications},
  title                = {Clustering stocks using partial correlation coefficients},
  doi                  = {10.1016/j.physa.2016.06.094},
  issn                 = {0378-4371},
  pages                = {410--420},
  volume               = {462},
  abstract             = {Correlation analyses are conducted on Korean stock market. Agglomerative hierarchical clustering is performed based on correlation matrices. Each cluster consists of firms from multiple business sectors. A partial correlation analysis is performed on the Korean stock market (KOSPI). The difference between Pearson correlation and the partial correlation is analyzed and it is found that when conditioned on the market return, Pearson correlation coefficients are generally greater than those of the partial correlation, which implies that the market return tends to drive up the correlation between stock returns. A clustering analysis is then performed to study the market structure given by the partial correlation analysis and the members of the clusters are compared with the Global Industry Classification Standard (GICS). The initial hypothesis is that the firms in the same GICS sector are clustered together since they are in a similar business and environment. However, the result is inconsistent with the hypothesis and most clusters are a mix of multiple sectors suggesting that the traditional approach of using sectors to determine the proximity between stocks may not be sufficient enough to diversify a portfolio.},
  citeulike-article-id = {14149947},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.physa.2016.06.094},
  groups               = {Networks and investment management, Clustering and network analysis, Invest_Network},
  owner                = {cristi},
  posted-at            = {2016-10-01 13:50:28},
  timestamp            = {2020-02-27 04:01},
}

@Article{Kamalzadeh-et-al-2019,
  author         = {Kamalzadeh, Hossein and Ahmadi, Abbas and Mansour, Saeed},
  date           = {2019-12-05},
  journaltitle   = {arXiv e-Print},
  title          = {Clustering Time-Series by a Novel Slope-Based Similarity Measure Considering Particle Swarm Optimization},
  url            = {https://arxiv.org/abs/1912.02405},
  urldate        = {2019-12-22},
  abstract       = {Recently there has been an increase in the studies on time-series data mining specifically time-series clustering due to the vast existence of time-series in various domains. The large volume of data in the form of time-series makes it necessary to employ various techniques such as clustering to understand the data and to extract information and hidden patterns. In the field of clustering specifically, time-series clustering, the most important aspects are the similarity measure used and the algorithm employed to conduct the clustering. In this paper, a new similarity measure for time-series clustering is developed based on a combination of a simple representation of time-series, slope of each segment of time-series, Euclidean distance and the so-called dynamic time warping. It is proved in this paper that the proposed distance measure is metric and thus indexing can be applied. For the task of clustering, the Particle Swarm Optimization algorithm is employed. The proposed similarity measure is compared to three existing measures in terms of various criteria used for the evaluation of clustering algorithms. The results indicate that the proposed similarity measure outperforms the rest in almost every dataset used in this paper.},
  day            = {5},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 04:01},
}

@Article{Kang-et-al-2015,
  author               = {Kang, Zhao and Peng, Chong and Cheng, Qiang},
  date                 = {2015-11},
  journaltitle         = {IEEE Signal Processing Letters},
  title                = {Robust Subspace Clustering via Smoothed Rank Approximation},
  doi                  = {10.1109/lsp.2015.2460737},
  issn                 = {1070-9908},
  number               = {11},
  pages                = {2088--2092},
  volume               = {22},
  abstract             = {Matrix rank minimizing subject to affine constraints arises in many application areas, ranging from signal processing to machine learning. Nuclear norm is a convex relaxation for this problem which can recover the rank exactly under some restricted and theoretically interesting conditions. However, for many real-world applications, nuclear norm approximation to the rank function can only produce a result far from the optimum. To seek a solution of higher accuracy than the nuclear norm, in this letter, we propose a rank approximation based on Logarithm-Determinant. We consider using this rank approximation for subspace clustering application. Our framework can model different kinds of errors and noise. Effective optimization strategy is developed with theoretical guarantee to converge to a stationary point. The proposed method gives promising results on face clustering and motion segmentation tasks compared to the state-of-the-art subspace clustering algorithms.},
  citeulike-article-id = {14351210},
  citeulike-linkout-0  = {http://dx.doi.org/10.1109/lsp.2015.2460737},
  posted-at            = {2017-05-05 01:31:46},
  timestamp            = {2020-02-27 04:01},
}

@Article{Kang-et-al-2017b,
  author               = {Kang, Zhao and Peng, Chong and Cheng, Qiang},
  date                 = {2017-05},
  journaltitle         = {arXiv e-Print},
  title                = {Twin Learning for Similarity and Clustering: A Unified Kernel Approach},
  eprint               = {1705.00678},
  eprinttype           = {arXiv},
  url                  = {https://arxiv.org/abs/1705.00678},
  abstract             = {Many similarity-based clustering methods work in two separate steps including similarity matrix computation and subsequent spectral clustering. However, similarity measurement is challenging because it is usually impacted by many factors, e.g., the choice of similarity metric, neighborhood size, scale of data, noise and outliers. Thus the learned similarity matrix is often not suitable, let alone optimal, for the subsequent clustering. In addition, nonlinear similarity often exists in many real world data which, however, has not been effectively considered by most existing methods. To tackle these two challenges, we propose a model to simultaneously learn cluster indicator matrix and similarity information in kernel spaces in a principled way. We show theoretical relationships to kernel k-means, k-means, and spectral clustering methods. Then, to address the practical issue of how to select the most suitable kernel for a particular clustering task, we further extend our model with a multiple kernel learning ability. With this joint model, we can automatically accomplish three subtasks of finding the best cluster indicator matrix, the most accurate similarity relations and the optimal combination of multiple kernels. By leveraging the interactions between these three subtasks in a joint framework, each subtask can be iteratively boosted by using the results of the others towards an overall optimal solution. Extensive experiments are performed to demonstrate the effectiveness of our method.},
  citeulike-article-id = {14351185},
  citeulike-linkout-0  = {http://arxiv.org/abs/1705.00678},
  citeulike-linkout-1  = {http://arxiv.org/pdf/1705.00678},
  day                  = {3},
  posted-at            = {2017-05-04 22:18:58},
  timestamp            = {2020-02-27 04:01},
}

@Article{Kauffmann-et-al-2019,
  author         = {Kauffmann, Jacob and Esders, Malte and Montavon, Grégoire and Samek, Wojciech and Müller, Klaus-Robert},
  date           = {2019-06-18},
  journaltitle   = {arXiv e-Print},
  title          = {From Clustering to Cluster Explanations via Neural Networks},
  url            = {https://arxiv.org/abs/1906.07633},
  urldate        = {2019-10-11},
  abstract       = {A wealth of algorithms have been developed to extract natural cluster structure in data. Identifying this structure is desirable but not always sufficient: We may also want to understand why the data points have been assigned to a given cluster. Clustering algorithms do not offer a systematic answer to this simple question. Hence we propose a new framework that can, for the first time, explain cluster assignments in terms of input features in a comprehensive manner. It is based on the novel theoretical insight that clustering models can be rewritten as neural networks, or 'neuralized'. Predictions of the obtained networks can then be quickly and accurately attributed to the input features. Several showcases demonstrate the ability of our method to assess the quality of learned clusters and to extract novel insights from the analyzed data and representations.},
  day            = {18},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 04:01},
}

@Article{Kawamoto-Kabashima-2017a,
  author               = {Kawamoto, Tatsuro and Kabashima, Yoshiyuki},
  date                 = {2017-06},
  journaltitle         = {Scientific Reports},
  title                = {Cross-validation estimate of the number of clusters in a network},
  doi                  = {10.1038/s41598-017-03623-x},
  issn                 = {2045-2322},
  number               = {1},
  volume               = {7},
  abstract             = {Network science investigates methodologies that summarise relational data to obtain better interpretability. Identifying modular structures is a fundamental task, and assessment of the coarse-grain level is its crucial step. Here, we propose principled, scalable, and widely applicable assessment criteria to determine the number of clusters in modular networks based on the leave-one-out cross-validation estimate of the edge prediction error.},
  citeulike-article-id = {14449671},
  citeulike-linkout-0  = {http://dx.doi.org/10.1038/s41598-017-03623-x},
  day                  = {12},
  posted-at            = {2017-10-12 23:55:55},
  timestamp            = {2020-02-27 04:01},
}

@Article{Kazor-Hering-2015,
  author               = {Kazor, Karen and Hering, AmandaS},
  date                 = {2015},
  journaltitle         = {Journal of Agricultural, Biological, and Environmental Statistics},
  title                = {Assessing the Performance of Model-Based Clustering Methods in Multivariate Time Series with Application to Identifying Regional Wind Regimes},
  doi                  = {10.1007/s13253-015-0203-8},
  number               = {2},
  pages                = {192--217},
  volume               = {20},
  abstract             = {The desire to group observations generated from multivariate time series is common in many applications with the goal to distinguish not only between differences in the means of individual variables but also changes in their covariances and in the temporal dependence of observations. In this analysis, we compare ten model-based clustering methods in terms of their ability to identify such features under four scenarios in which data are simulated with varying levels of variable and temporal dependence. To consider these methods in a realistic environment, we focus our analysis on wind data, where observations are often strongly correlated in time, and the dependence of variables is known to vary across different regional weather patterns. In particular, we assess each method's performance when applied to wind data simulated under a realistic two-regime Markov-switching vector autoregressive (VAR) model with a diurnally varying mean. A Gaussian mixture model and a basic Markov-switching model outperform the other methods considered in terms of misclassification rates and number of clusters identified. These two methods and an additional Markov-switching VAR model are then applied to one year of averaged hourly wind data from twenty meteorological stations, and we find that the methods can identify very different features in the data. Supplementary materials accompanying this paper appear on-line.},
  citeulike-article-id = {14014328},
  citeulike-linkout-0  = {http://dx.doi.org/10.1007/s13253-015-0203-8},
  citeulike-linkout-1  = {http://link.springer.com/article/10.1007/s13253-015-0203-8},
  owner                = {cristi},
  posted-at            = {2016-04-17 17:30:34},
  publisher            = {Springer US},
  timestamp            = {2020-02-27 04:01},
}

@Article{Khaleghi-et-al-2016,
  author               = {Khaleghi, Azadeh and Ryabko, Daniil and Mary, Jeremie and Preux, Philippe},
  date                 = {2016},
  journaltitle         = {The Journal of Machine Learning Research},
  title                = {Consistent algorithms for clustering time series},
  pages                = {1--32},
  url                  = {http://jmlr.org/papers/v17/khaleghi16a.html},
  volume               = {17},
  abstract             = {The problem of clustering is considered for the case where every point is a time series. The time series are either given in one batch (offline setting), or they are allowed to grow with time and new time series can be added along the way (online setting). We propose a natural notion of consistency for this problem, and show that there are simple, computationally efficient algorithms that are asymptotically consistent under extremely weak assumptions on the distributions that generate the data. The notion of consistency is as follows. A clustering algorithm is called consistent if it places two time series into the same cluster if and only if the distribution that generates them is the same. In the considered framework the time series are allowed to be highly dependent, and the dependence can have arbitrary form. If the number of clusters is known, the only assumption we make is that the (marginal) distribution of each time series is stationary ergodic. No parametric, memory or mixing assumptions are made. When the number of clusters is unknown, stronger assumptions are provably necessary, but it is still possible to devise nonparametric algorithms that are consistent under very general conditions. The theoretical findings of this work are illustrated with experiments on both synthetic and real data.},
  citeulike-article-id = {14148585},
  groups               = {Networks and investment management, Clustering and network analysis, ML_ClustTimeSrs},
  owner                = {cristi},
  posted-at            = {2016-09-28 19:51:44},
  timestamp            = {2020-02-27 04:01},
}

@Article{Kou-et-al-2014,
  author               = {Kou, Gang and Peng, Yi and Wang, Guoxun},
  date                 = {2014-08},
  journaltitle         = {Information Sciences},
  title                = {Evaluation of clustering algorithms for financial risk analysis using MCDM methods},
  doi                  = {10.1016/j.ins.2014.02.137},
  issn                 = {0020-0255},
  pages                = {1--12},
  volume               = {275},
  abstract             = {The evaluation of clustering algorithms is intrinsically difficult because of the lack of objective measures. Since the evaluation of clustering algorithms normally involves multiple criteria, it can be modeled as a multiple criteria decision making (MCDM) problem. This paper presents an MCDM-based approach to rank a selection of popular clustering algorithms in the domain of financial risk analysis. An experimental study is designed to validate the proposed approach using three MCDM methods, six clustering algorithms, and eleven cluster validity indices over three real-life credit risk and bankruptcy risk data sets. The results demonstrate the effectiveness of MCDM methods in evaluating clustering algorithms and indicate that the repeated-bisection method leads to good 2-way clustering solutions on the selected financial risk data sets.},
  citeulike-article-id = {14435123},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.ins.2014.02.137},
  posted-at            = {2017-09-20 22:42:34},
  timestamp            = {2020-02-27 04:01},
}

@Article{Kozdoba-Mannor-2016,
  author               = {Kozdoba, Mark and Mannor, Shie},
  date                 = {2016-09},
  journaltitle         = {arXiv e-Print},
  title                = {Clustering Time Series and the Surprising Robustness of HMMs},
  eprint               = {1605.02531},
  eprinttype           = {arXiv},
  url                  = {https://arxiv.org/abs/1605.02531},
  abstract             = {Suppose that we are given a time series where consecutive samples are believed to come from a probabilistic source, that the source changes from time to time and that the total number of sources is fixed. Our objective is to estimate the distributions of the sources. A standard approach to this problem is to model the data as a hidden Markov model (HMM). However, since the data often lacks the Markov or the stationarity properties of an HMM, one can ask whether this approach is still suitable or perhaps another approach is required. In this paper we show that a maximum likelihood HMM estimator can be used to approximate the source distributions in a much larger class of models than HMMs. Specifically, we propose a natural and fairly general non-stationary model of the data, where the only restriction is that the sources do not change too often. Our main result shows that for this model, a maximum-likelihood HMM estimator produces the correct second moment of the data, and the results can be extended to higher moments.},
  citeulike-article-id = {14357388},
  citeulike-linkout-0  = {http://arxiv.org/abs/1605.02531},
  citeulike-linkout-1  = {http://arxiv.org/pdf/1605.02531},
  day                  = {14},
  groups               = {Clustering and network analysis, NonStatry_FinTimeSrs},
  posted-at            = {2017-05-15 17:42:58},
  timestamp            = {2020-02-27 04:01},
}

@InCollection{Lemieux-et-al-2014,
  author               = {Lemieux, Victoria and Rahmdel, Payam S. and Walker, Rick and Wong, B. L. William and Flood, Mark},
  booktitle            = {Proceedings of the International Workshop on Data Science for Macro-Modeling},
  date                 = {2014},
  title                = {Clustering Techniques And Their Effect on Portfolio Formation and Risk Analysis},
  doi                  = {10.1145/2630729.2630749},
  isbn                 = {978-1-4503-3012-1},
  location             = {Snowbird, UT, USA},
  publisher            = {ACM},
  series               = {DSMM'14},
  abstract             = {This paper explores the application of three different portfolio formation rules using standard clustering techniques---K-means, K-mediods, and hierarchical---to a large financial data set (16 years of daily CRSP stock data) to determine how the choice of clustering technique may affect analysts' perceptions of the riskiness of different portfolios in the context of a prototype visual analytics system designed for financial stability monitoring. We use a two-phased experimental approach with visualizations to explore the effects of the different clustering techniques. The choice of clustering technique matters. There is significant variation among techniques, resulting in different "pictures"of the riskiness of the same underlying data when plotted to the visual analytics tool. This sensitivity to clustering methodolgy has the potential to mislead analysts about the riskiness of portfolios. We conclude that further research into the implications of portfolio formation rules is needed, and that visual analytics tools should not limit analysts to a single clustering technique, but instead should provide the facility to explore the data using different techniques.},
  address              = {New York, NY, USA},
  citeulike-article-id = {14148038},
  citeulike-linkout-0  = {http://portal.acm.org/citation.cfm?id=2630749},
  citeulike-linkout-1  = {http://dx.doi.org/10.1145/2630729.2630749},
  groups               = {Networks and investment management, Clustering and network analysis, Network_Invest},
  owner                = {cristi},
  posted-at            = {2016-09-28 18:10:37},
  timestamp            = {2020-02-27 04:01},
}

@Article{Leon-et-al-2017,
  author               = {Leon, Diego and Aragon, Arbey and Sandoval, Javier and Hernandez, German and Arevalo, Andres and Nino, Jaime},
  date                 = {2017},
  journaltitle         = {Procedia Computer Science},
  title                = {Clustering algorithms for Risk-Adjusted Portfolio Construction},
  doi                  = {10.1016/j.procs.2017.05.185},
  issn                 = {1877-0509},
  pages                = {1334--1343},
  volume               = {108},
  abstract             = {This paper presents the performance of seven portfolios created using clustering analysis techniques to sort out assets into categories and then applying classical optimization inside every cluster to select best assets inside each asset category. The proposed clustering algorithms are tested constructing portfolios and measuring their performances over a two month dataset of 1-minute asset returns from a sample of 175 assets of the Russell 1000 index. A three-week sliding window is used for model calibration, leaving an out of sample period of five weeks for testing. Model calibration is done weekly. Three different rebalancing periods are tested: every 1, 2 and 4 hours. The results show that all clustering algorithms produce more stable portfolios with similar volatility. In this sense, the portfolios volatilities generated by the clustering algorithms are smaller when compare to the portfolio obtained using classical Mean-Variance Optimization (MVO) over all the dataset. Hierarchical clustering algorithms achieve the best financial performance obtaining an adequate trade-off between accumulated financial returns and the risk-adjusted measure, Omega Ratio, during the out of sample testing period.},
  citeulike-article-id = {14379414},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.procs.2017.05.185},
  groups               = {Networks and investment management, Clustering and network analysis, Network_Invest, PortfOptim_Network, Invest_Network, Vol_Cluster},
  posted-at            = {2017-06-19 22:00:54},
  timestamp            = {2020-02-27 04:01},
}

@InCollection{Leverger-et-al-2019,
  author         = {Leverger, Colin and Malinowski, Simon and Guyet, Thomas and Lemaire, Vincent and Bondu, Alexis and Termier, Alexandre},
  booktitle      = {Intelligent data engineering and automated learning - IDEAL 2019: 20th international conference, manchester, UK, november 14-16, 2019, proceedings, part I},
  date           = {2019},
  title          = {Toward a framework for seasonal time series forecasting using clustering},
  doi            = {10.1007/978-3-030-33607-3\_36},
  editor         = {Yin, Hujun and Camacho, David and Tino, Peter and Tallon-Ballesteros, Antonio J. and Menezes, Ronaldo and Allmendinger, Richard},
  isbn           = {978-3-030-33606-6},
  pages          = {328--340},
  publisher      = {Springer International Publishing},
  series         = {Lecture notes in computer science},
  url            = {http://link.springer.com/10.1007/978-3-030-33607-3\_36},
  urldate        = {2020-01-03},
  volume         = {11871},
  abstract       = {Seasonal behaviours are widely encountered in various applications. For instance, requests on web servers are highly influenced by our daily activities. Seasonal forecasting consists in forecasting the whole next season for a given seasonal time series. It may help a service provider to provision correctly the potentially required resources, avoiding critical situations of over- or under provision. In this article, we propose a generic framework to make seasonal time series forecasting. The framework combines machine learning techniques (1) to identify the typical seasons and (2) to forecast the likelihood of having a season type in one season ahead. We study this framework by comparing the mean squared errors of forecasts for various settings and various datasets. The best setting is then compared to state-of-the-art time series forecasting methods. We show that it is competitive with them.},
  f1000-projects = {QuantInvest},
  issn           = {0302-9743},
  timestamp      = {2020-02-27 04:01},
}

@Article{Liechti-Bonhoeffer-2019,
  author         = {Liechti, Jonas I. and Bonhoeffer, Sebastian},
  date           = {2019-12-09},
  journaltitle   = {arXiv e-Print},
  title          = {A time resolved clustering method revealing longterm structures and their short-term internal dynamics},
  url            = {https://arxiv.org/abs/1912.04261v1},
  urldate        = {2019-12-15},
  abstract       = {The last decades have not only been characterized by an explosive growth of data, but also an increasing appreciation of data as a valuable resource. It's value comes with the ability to extract meaningful patterns that are of economic, societal or scientific relevance. A particular challenge is to identify patterns across time, including patterns that might only become apparent when the temporal dimension is taken into account. Here, we present a novel method that aims to achieve this by detecting dynamic clusters, i.e. structural elements that can be present over prolonged durations. It is based on an adaptive identification of majority overlaps between groups at different time points and allows the accommodation of transient decompositions in otherwise persistent dynamic clusters. As such, our method enables the detection of persistent structural elements with internal dynamics and can be applied to any classifiable data, ranging from social contact networks to arbitrary sets of time stamped feature vectors. It provides a unique tool to study systems with non-trivial temporal dynamics with a broad applicability to scientific, societal and economic data.},
  day            = {9},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 04:01},
}

@Article{Lin-et-al-2018a,
  author         = {Lin, Alexander and Zhang, Yingzhuo and Heng, Jeremy and Allsop, Stephen A. and Tye, Kay M. and Jacob, Pierre E. and Ba, Demba},
  date           = {2018-10-23},
  journaltitle   = {arXiv e-Print},
  title          = {Clustering Time Series with Nonlinear Dynamics: A Bayesian Non-Parametric and Particle-Based Approach},
  url            = {https://arxiv.org/abs/1810.09920},
  abstract       = {We propose a general statistical framework for clustering multiple time series that exhibit nonlinear dynamics into an a-priori-unknown number of sub-groups. Our motivation comes from neuroscience, where an important problem is to identify, within a large assembly of neurons, subsets that respond similarly to a stimulus or contingency. Upon modeling the multiple time series as the output of a Dirichlet process mixture of nonlinear state-space models, we derive a Metropolis-within-Gibbs algorithm for full Bayesian inference that alternates between sampling cluster assignments and sampling parameter values that form the basis of the clustering. The Metropolis step employs recent innovations in particle-based methods. We apply the framework to clustering time series acquired from the prefrontal cortex of mice in an experiment designed to characterize the neural underpinnings of fear.},
  day            = {23},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 04:01},
}

@Article{Lin-Li-2017,
  author               = {Lin, Lin and Li, Jia},
  date                 = {2017},
  journaltitle         = {Journal of Machine Learning Research},
  title                = {Clustering with Hidden Markov Model on Variable Blocks},
  number               = {110},
  pages                = {1--49},
  url                  = {http://jmlr.org/papers/v18/16-342.html},
  volume               = {18},
  abstract             = {Large-scale data containing multiple important rare clusters, even at moderately high dimensions, pose challenges for existing clustering methods. To address this issue, we propose a new mixture model called Hidden Markov Model on Variable Blocks (HMM-VB) and a new mode search algorithm called Modal Baum-Welch (MBW) for mode-association clustering. HMM-VB leverages prior information about chain-like dependence among groups of variables to achieve the effect of dimension reduction. In case such a dependence structure is unknown or assumed merely for the sake of parsimonious modeling, we develop a recursive search algorithm based on BIC to optimize the formation of ordered variable blocks. The MBW algorithm ensures the feasibility of clustering via mode association, achieving linear complexity in terms of the number of variable blocks despite the exponentially growing number of possible state sequences in HMM-VB. In addition, we provide theoretical investigations about the identifiability of HMM-VB as well as the consistency of our approach to search for the block partition of variables in a special case. Experiments on simulated and real data show that our proposed method outperforms other widely used methods.},
  citeulike-article-id = {14509669},
  citeulike-linkout-0  = {http://jmlr.org/papers/v18/16-342.html},
  keywords             = {*file-import-17-12-29},
  posted-at            = {2017-12-29 02:10:57},
  timestamp            = {2020-02-27 04:01},
}

@Article{Lisi-Menardi-2015,
  author               = {Lisi, Francesco and Menardi, Giovanna},
  date                 = {2015},
  journaltitle         = {Electronic Journal of Applied Statistical Analysis},
  title                = {Double clustering for rating mutual funds},
  number               = {1},
  pages                = {44--56},
  url                  = {http://siba-ese.unisalento.it/index.php/ejasa/article/view/13761},
  volume               = {8},
  abstract             = {Due to the increasing proliferation of mutual funds, in-depth evaluation of the available products for portfolio selection purposes is a difficult task. Hence, classiffication schemes giving quick information about which funds are worth to be monitored, are often provided. The aim of this work is to show an application of clustering methods to the mutual funds historical data. Starting from the monthly time series of the Net Asset Values of a specific style-based category, namely the Large Blend US mutual funds, we apply distance-based clustering methods twice on a set of return, risk and performance measures: firstly, with the aim of reducing data dimension, and secondly to cluster funds in homogeneous classes. The adopted procedure claims the feature of producing a partition of funds that are readily interpretable from a financial point of view and it is further possible to rank the identified groups, thus obtaining a rating of funds that turns out to account for different propensities toward the risk exposure.},
  citeulike-article-id = {14367329},
  groups               = {Clustering and network analysis},
  posted-at            = {2017-06-02 22:26:50},
  timestamp            = {2020-02-27 04:01},
}

@Article{Lisi-Otranto-2010,
  author       = {Francesco Lisi and Edoardo Otranto},
  date         = {2010},
  journaltitle = {Mathematical and Statistical Methods for Actuarial Sciences and Finance},
  title        = {Clustering mutual funds by return and risk levels},
  url          = {https://link.springer.com/chapter/10.1007/978-88-470-1481-7_19},
  abstract     = {Mutual funds classifications, often made by rating agencies, are very common and sometimes criticised. In this work, a three-step statistical procedure for mutual funds classification is proposed. In the first step fund time series are characterised in terms of returns. In the second step, a clustering analysis is performed in order to obtain classes of homogeneous funds with respect to the risk levels. In particular, the risk is defined starting from an Asymmetric Threshold-GARCH model aimed to describe minimum, normal and turmoil risk. The third step merges the previous two. An application to 75 European funds belonging to 5 different categories is presented.},
  timestamp    = {2020-02-27 04:01},
}

@Article{Li-Zhu-2016,
  author               = {Li, Jinghua and Zhu, Dunlin},
  date                 = {2016-06},
  journaltitle         = {IET Renewable Power Generation},
  title                = {Combination of moment-matching, Cholesky and clustering methods to approximate discrete probability distribution of multiple wind farms},
  doi                  = {10.1049/iet-rpg.2015.0568},
  issn                 = {1752-1416},
  abstract             = {This study focuses on approximating a reduced discrete probability distribution (RDPD) of wind power from the original discrete probability distribution (ODPD), consisting of a large number of observed original scenarios (OSs), to relieve the burden of solving stochastic programs of wind power generation. The proposed method, namely, the MMCC method, aims to achieve high approximation accuracy and computational efficiency by combining an improved moment-matching (MM) method with the clustering (C) method and the Cholesky decomposition (CD) method. First, the C method is used to reduce the number of OSs by minimising the space distance between the reduced scenarios (RSs) and the OSs. Next, the CD method is used to rectify the correlation of the RSs to satisfy that of the ODPD. Finally, the RS probabilities are optimally determined by the MM method in order to minimise the stochastic features (first four moments and correlation matrix) between the RDPD and the ODPD. Simulations of RDPD approximation for three wind farms with 10, 20, 40, 60, 80, and 100 scenarios were carried out using the Latin hypercube sampling, importance sampling, C, moment-matching-clustering (MMC), and MMCC methods. The results showed that the MMCC method exhibits the best performance in terms of capturing the features of the ODPD.},
  citeulike-article-id = {14171142},
  citeulike-linkout-0  = {http://dx.doi.org/10.1049/iet-rpg.2015.0568},
  day                  = {13},
  owner                = {cristi},
  posted-at            = {2016-10-24 20:11:06},
  timestamp            = {2020-02-27 04:01},
}

@Article{Lorimer-et-al-2017,
  author         = {Lorimer, Tom and Held, Jenny and Stoop, Ruedi},
  date           = {2017-06-28},
  journaltitle   = {Philos Transact A Math Phys Eng Sci},
  title          = {Clustering: how much bias do we need?},
  doi            = {10.1098/rsta.2016.0293},
  number         = {2096},
  volume         = {375},
  abstract       = {Scientific investigations in medicine and beyond increasingly require observations to be described by more features than can be simultaneously visualized. Simply reducing the dimensionality by projections destroys essential relationships in the data. Similarly, traditional clustering algorithms introduce data bias that prevents detection of natural structures expected from generic nonlinear processes. We examine how these problems can best be addressed, where in particular we focus on two recent clustering approaches, Phenograph and Hebbian learning clustering, applied to synthetic and natural data examples. Our results reveal that already for very basic questions, minimizing clustering bias is essential, but that results can benefit further from biased post-processing.This article is part of the themed issue 'Mathematical methods in medicine: neuroscience, cardiology and pathology'.},
  day            = {28},
  f1000-projects = {QuantInvest},
  pmcid          = {PMC5434083},
  pmid           = {28507238},
  timestamp      = {2020-02-27 04:01},
}

@Article{Lu-et-al-2018a,
  author         = {Lu, Ya-Nan and Li, Sai-Ping and Zhong, Li-Xin and Jiang, Xiong-Fei and Ren, Fei},
  date           = {2018-12},
  journaltitle   = {Chaos, Solitons \& Fractals},
  title          = {A clustering-based portfolio strategy incorporating momentum effect and market trend prediction},
  doi            = {10.1016/j.chaos.2018.10.012},
  issn           = {0960-0779},
  pages          = {1--15},
  volume         = {117},
  abstract       = {Abstract The hierarchical clustering algorithm has been proved useful in portfolio investment, which is one of the hottest issues in finance. In our new portfolio strategy, central, peripheral and dispersed portfolios constructed from clusters detected using unweighted and weighted modularity are compared according to their past performances, and the optimal portfolio is used in the investment period only if the market index return predicted by the LR, WMA or BP models is positive to avoid losses when the market drops. Our strategy is tested using the daily data of Chinese A-share market from January 4, 2008 and December 31, 2016, and the average investment return during different moving investment periods and 200 repeated runs is calculated. We find that although incorporating dispersed portfolio into our strategy has no significant effect in raising the investment return, it shows a similar performance as the peripheral portfolio, and the strategy constructed using unweighted modularity generally outperforms its counterpart by using weighted modularity. In addition, the market trend prediction can refine the investment return of our strategy. In brief, the strategy constructed using the BP model and unweighted modularity has the best investment return, which also outperforms the Markowitz portfolio.},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 04:01},
}

@Article{Lu-Wan-2013,
  author               = {Lu, Yonggang and Wan, Yi},
  date                 = {2013-05},
  journaltitle         = {Pattern Recognition},
  title                = {PHA: A fast potential-based hierarchical agglomerative clustering method},
  doi                  = {10.1016/j.patcog.2012.11.017},
  issn                 = {0031-3203},
  number               = {5},
  pages                = {1227--1239},
  volume               = {46},
  abstract             = {A novel potential-based hierarchical agglomerative (PHA) clustering method is proposed. In this method, we first construct a hypothetical potential field of all the data points, and show that this potential field is closely related to nonparametric estimation of the global probability density function of the data points. Then we propose a new similarity metric incorporating both the potential field which represents global data distribution information and the distance matrix which represents local data distribution information. Finally we develop another equivalent similarity metric based on an edge weighted tree of all the data points, which leads to a fast agglomerative clustering algorithm with time complexity O(N2). The proposed PHA method is evaluated by comparing with six other typical agglomerative clustering methods on four synthetic data sets and two real data sets. Experiments show that it runs much faster than the other methods and produces the most satisfying results in most cases. A potential field is used to represent global data distribution information. Both local and global data distribution information are used in the clustering. Designed an efficient hierarchical clustering method based on an edge-weighted tree. The potential field can be viewed as an estimated probability density function. PHA usually produces more satisfying results in much less time than other methods.},
  citeulike-article-id = {14148672},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.patcog.2012.11.017},
  owner                = {cristi},
  posted-at            = {2016-09-28 22:54:06},
  timestamp            = {2020-02-27 04:01},
}

@InCollection{Mackrechi-2016,
  author               = {Makrehchi, Masoud},
  booktitle            = {IEEE/WIC/ACM International Conference on Web Intelligence (WI)},
  date                 = {2016-10},
  title                = {Hierarchical Agglomerative Clustering Using Common Neighbours Similarity},
  doi                  = {10.1109/wi.2016.0093},
  isbn                 = {978-1-5090-4470-2},
  location             = {Omaha, NE, USA},
  pages                = {546--551},
  publisher            = {IEEE},
  abstract             = {Hierarchical clustering has been well-studied in the community of machine learning. Hierarchical clustering algorithms are deterministic, stable, and do not need a pre-determined number of clusters as input. However, they are not scalable for very large data due to their non-linear complexity. In this paper, a new approach is proposed to reduce the complexity of Hierarchical Clustering, improve the purity of the clustering algorithm, and reduce the chaining factor. The proposed method has the following components: (i) A new combination similarity based on common-neighbours of graph theory is proposed, (ii) In every iteration, instead of calculating the centroids for new clusters, new centroids are estimated from centroids in previous iteration, and (iii) In each iteration, instead of merging only one pair of objects, multiple pairs are merged at the same time. In addition to the proposed combination similarity, four well-known methods including centroid-based, group-based, complete-link, and single-link, have been also implemented. All five methods are tested and evaluated using two metrics: purity and imbalance or chaining factor. We show that our proposed algorithm outperforms other classic methods.},
  citeulike-article-id = {14335040},
  citeulike-linkout-0  = {http://dx.doi.org/10.1109/wi.2016.0093},
  posted-at            = {2017-04-10 12:18:55},
  timestamp            = {2020-02-27 04:01},
}

@Article{Marbac-Sedki-2017,
  author         = {Marbac, Matthieu and Sedki, Mohammed},
  date           = {2017-07},
  journaltitle   = {Statistics and Computing},
  title          = {Variable selection for model-based clustering using the integrated complete-data likelihood},
  doi            = {10.1007/s11222-016-9670-1},
  issn           = {0960-3174},
  number         = {4},
  pages          = {1049--1063},
  volume         = {27},
  abstract       = {Variable selection in cluster analysis is important yet challenging. It can be achieved by regularization methods, which realize a trade-off between the clustering accuracy and the number of selected variables by using a lasso-type penalty. However, the calibration of the penalty term can suffer from criticisms. Model selection methods are an efficient alternative, yet they require a difficult optimization of an information criterion which involves combinatorial problems. First, most of these optimization algorithms are based on a suboptimal procedure (e.g. stepwise method). Second, the algorithms are often computationally expensive because they need multiple calls of EM algorithms. Here we propose to use a new information criterion based on the integrated complete-data likelihood. It does not require the maximum likelihood estimate and its maximization appears to be simple and computationally efficient. The original contribution of our approach is to perform the model selection without requiring any parameter estimation. Then, parameter inference is needed only for the unique selected model. This approach is used for the variable selection of a Gaussian mixture model with conditional independence assumed. The numerical experiments on simulated and benchmark datasets show that the proposed method often outperforms two classical approaches for variable selection. The proposed approach is implemented in the R package VarSelLCM available on CRAN.},
  f1000-projects = {QuantInvest},
  groups         = {Estim_MaxLikelihood},
  timestamp      = {2020-02-27 04:01},
}

@Article{Marti-et-al-2016,
  author               = {Marti, Gautier and Andler, Sebastien and Nielsen, Frank and Donnat, Philippe},
  date                 = {2016-03},
  journaltitle         = {arXiv e-Print},
  title                = {Clustering Financial Time Series: How Long is Enough?},
  eprint               = {1603.04017},
  eprinttype           = {arXiv},
  url                  = {https://arxiv.org/abs/1603.04017},
  abstract             = {Researchers have used from 30 days to several years of daily returns as source data for clustering financial time series based on their correlations. This paper sets up a statistical framework to study the validity of such practices. We first show that clustering correlated random variables from their observed values is statistically consistent. Then, we also give a first empirical answer to the much debated question: How long should the time series be? If too short, the clusters found can be spurious; if too long, dynamics can be smoothed out.},
  citeulike-article-id = {13987284},
  citeulike-linkout-0  = {http://arxiv.org/abs/1603.04017},
  citeulike-linkout-1  = {http://arxiv.org/pdf/1603.04017},
  day                  = {13},
  groups               = {Networks and investment management, Clustering and network analysis},
  owner                = {cristi},
  posted-at            = {2016-03-24 14:12:20},
  timestamp            = {2020-02-27 04:01},
}

@Article{Marti-et-al-2016a,
  author               = {Marti, Gautier and Nielsen, Frank and Donnat, Philippe and Andler, Sebastien},
  date                 = {2016-03},
  journaltitle         = {arXiv e-Print},
  title                = {On clustering financial time series: a need for distances between dependent random variables},
  eprint               = {1603.07822},
  eprinttype           = {arXiv},
  url                  = {https://arxiv.org/abs/1603.07822},
  abstract             = {The following working document summarizes our work on the clustering of financial time series. It was written for a workshop on information geometry and its application for image and signal processing. This workshop brought several experts in pure and applied mathematics together with applied researchers from medical imaging, radar signal processing and finance. The authors belong to the latter group. This document was written as a long introduction to further development of geometric tools in financial applications such as risk or portfolio analysis. Indeed, risk and portfolio analysis essentially rely on covariance matrices. Besides that the Gaussian assumption is known to be inaccurate, covariance matrices are difficult to estimate from empirical data. To filter noise from the empirical estimate, Mantegna proposed using hierarchical clustering. In this work, we first show that this procedure is statistically consistent. Then, we propose to use clustering with a much broader application than the filtering of empirical covariance matrices from the estimate correlation coefficients. To be able to do that, we need to obtain distances between the financial time series that incorporate all the available information in these cross-dependent random processes.},
  citeulike-article-id = {14147455},
  citeulike-linkout-0  = {http://arxiv.org/abs/1603.07822},
  citeulike-linkout-1  = {http://arxiv.org/pdf/1603.07822},
  day                  = {25},
  owner                = {cristi},
  posted-at            = {2016-09-27 19:10:13},
  timestamp            = {2020-02-27 04:01},
}

@Article{Marti-et-al-2016c,
  author               = {Marti, Gautier and Andler, Sebastien and Nielsen, Frank and Donnat, Philippe},
  date                 = {2016-10},
  journaltitle         = {arXiv e-Print},
  title                = {Exploring and measuring non-linear correlations: Copulas, Lightspeed Transportation and Clustering},
  eprint               = {1610.09659},
  eprinttype           = {arXiv},
  url                  = {https://arxiv.org/abs/1610.09659},
  abstract             = {We propose a methodology to explore and measure the pairwise correlations that exist between variables in a dataset. The methodology leverages copulas for encoding dependence between two variables, state-of-the-art optimal transport for providing a relevant geometry to the copulas, and clustering for summarizing the main dependence patterns found between the variables. Some of the clusters centers can be used to parameterize a novel dependence coefficient which can target or forget specific dependence patterns. Finally, we illustrate and benchmark the methodology on several datasets. Code and numerical experiments are available online for reproducible research.},
  citeulike-article-id = {14291484},
  citeulike-linkout-0  = {http://arxiv.org/abs/1610.09659},
  citeulike-linkout-1  = {http://arxiv.org/pdf/1610.09659},
  day                  = {30},
  posted-at            = {2017-03-03 18:30:47},
  timestamp            = {2020-02-27 04:01},
}

@InCollection{Marti-et-al-2017a,
  author               = {Marti, Gautier and Nielsen, Frank and Donnat, Philippe and Andler, Sebastien},
  booktitle            = {Computational Information Geometry},
  date                 = {2017},
  title                = {On Clustering Financial Time Series: A Need for Distances Between Dependent Random Variables},
  doi                  = {10.1007/978-3-319-47058-0\_8},
  editor               = {Nielsen, Frank and Critchley, Frank and Dodson, Christopher T. J.},
  pages                = {149--174},
  publisher            = {Springer International Publishing},
  series               = {Signals and Communication Technology},
  abstract             = {This artilce summarizes our work on the clustering of financial time series. It was written for a workshop on information geometry and its application for image and signal processing. This workshop brought several experts in pure and applied mathematics together with applied researchers from medical imaging, radar signal processing and finance. The authors belong to the latter group. This document was written as a long introduction to further development of geometric tools in financial applications such as risk or portfolio analysis. Indeed, risk and portfolio analysis essentially rely on covariance matrices. Besides that the Gaussian assumption is known to be inaccurate, covariance matrices are difficult to estimate from empirical data. To filter noise from the empirical estimate, Mantegna proposed using hierarchical clustering. In this work, we first show that this procedure is statistically consistent. Then, we propose to use clustering with a much broader application than the filtering of empirical covariance matrices from the estimated correlation coefficients. To be able to do that, we need to obtain distances between the financial time series that incorporate all the available information in these cross-dependent random processes.},
  citeulike-article-id = {14324929},
  citeulike-linkout-0  = {http://dx.doi.org/10.1007/978-3-319-47058-08},
  citeulike-linkout-1  = {http://link.springer.com/chapter/10.1007/978-3-319-47058-08},
  groups               = {Clustering and network analysis},
  posted-at            = {2017-03-30 22:23:54},
  timestamp            = {2020-02-27 04:01},
}

@Article{Marti-et-al-2019,
  author               = {Marti, Gautier and Nielsen, Frank and Bihkowski, Mikolaj and Donnat, Philippe},
  date                 = {2019-03},
  journaltitle         = {arXiv e-Print},
  title                = {A review of two decades of correlations, hierarchies, networks and clustering in financial markets},
  eprinttype           = {arXiv},
  url                  = {https:://arxiv.org/abs/1703.00485},
  urldate              = {2018-10-07},
  abstract             = {This document is a preliminary version of an in-depth review on the state of the art of clustering financial time series and the study of correlation networks. This preliminary document is intended for researchers in this field so that they can feedback to allow amendments, corrections and addition of new material unknown to the authors of this review. The aim of the document is to gather in one place the relevant material that can help the researcher in the field to have a bigger picture, the quantitative researcher to play with this alternative modeling of the financial time series, and the decision maker to leverage the insights obtained from these methods. We hope that this document will form a basis for implementation of an open toolbox of standard tools to study correlations, hierarchies, networks and clustering in financial markets. We also plan to maintain pointers to online material and an updated version of this work at www.datagrapple.com/Tech.},
  citeulike-article-id = {14291433},
  citeulike-linkout-0  = {http://arxiv.org/abs/1703.00485},
  citeulike-linkout-1  = {http://arxiv.org/pdf/1703.00485},
  day                  = {1},
  groups               = {Networks and investment management, Clustering and network analysis, Invest_Network},
  posted-at            = {2017-03-03 16:58:42},
  timestamp            = {2020-02-27 04:01},
}

@Article{Micciche-et-al-2005,
  author               = {Micciche, S. and Lillo, F. and Mantegna, R. N.},
  date                 = {2005-09},
  journaltitle         = {Workshop of the International School of Solid State Physics},
  title                = {Correlation Based Hierarchical Clustering in Financial Time Series},
  doi                  = {10.1142/9789812701558\_0037},
  pages                = {327--335},
  abstract             = {Abstract We review a correlation based clustering procedure applied to a portfolio of assets synchronously traded in a financial market. The portfolio considered consists of the set of 500 highly capitalized stocks traded at the New York Stock Exchange during the time period 1987-1998. We show that meaningful economic information can be extracted from correlation matrices.},
  booktitle            = {Complexity, Metastability and Nonextensivity},
  citeulike-article-id = {14150002},
  citeulike-linkout-0  = {http://dx.doi.org/10.1142/97898127015580037},
  citeulike-linkout-1  = {http://adsabs.harvard.edu/cgi-bin/nph-bibquery?bibcode=2005cmn..conf..327M},
  citeulike-linkout-2  = {http://www.worldscientific.com/doi/abs/10.1142/97898127015580037},
  day                  = {1},
  groups               = {Networks and investment management, Clustering and network analysis},
  owner                = {cristi},
  posted-at            = {2016-10-01 16:38:57},
  publisher            = {WORLD SCIENTIFIC},
  timestamp            = {2020-02-27 04:01},
}

@Article{Montero-Vilar-2015,
  author       = {Pablo Montero and Jose A. Vilar},
  date         = {2015},
  journaltitle = {Journal of Statistical Software},
  title        = {TSclust: An R Package for Time Series Clustering},
  url          = {https://www.jstatsoft.org/article/view/v062i01},
  volume       = {62},
  abstract     = {Time series clustering is an active research area with applications in a wide range of fields. One key component in cluster analysis is determining a proper dissimilarity measure between two data objects, and many criteria have been proposed in the literature to assess dissimilarity between two time series. The R package TSclust is aimed to implement a large set of well-established peer-reviewed time series dissimilarity measures, including measures based on raw data, extracted features, underlying parametric models, complexity levels, and forecast behaviors. Computation of these measures allows the user to perform clustering by using conventional clustering algorithms. TSclust also includes a clustering procedure based on p values from checking the equality of generating models, and some utilities to evaluate cluster solutions. The implemented dissimilarity functions are accessible individually for an easier extension and possible use out of the clustering context. The main features of TSclust are described and examples of its use are presented.},
  groups       = {FrcstQWIM_TimeSrs},
  timestamp    = {2020-02-27 04:01},
}

@Article{Muca-et-al-2015,
  author               = {Muca, Markela and Kutrolli, Gleda and Kutrolli, Maksi},
  date                 = {2015},
  journaltitle         = {European Scientific Journal},
  title                = {A proposed algorithm for determining the optimal number of clusters},
  url                  = {https://eujournal.org/index.php/esj/article/view/6756},
  abstract             = {Data clustering is a data exploration technique that allows objects with similar characteristics to be grouped together in order to facilitate their further processing. The K-means algorithm is a popular data-clustering algorithm. However, one of its drawbacks is the requirement for the number of clusters, K, to be specified before the algorithm is applied. This paper first reviews existing methods for selecting the number of clusters for the algorithm. Factors that affect this selection are then discussed and an improvement of the existing k-means algorithm to assist the selection is proposed. The paper concludes with an analysis of the results of using cluster validation referring to some measures that are classified as internal and external indexes to determine the optimal number of clusters for the K-means algorithm. There are applied some stopping criterion referring to those indexes for evaluating a clustering against a gold standard.},
  citeulike-article-id = {14435139},
  posted-at            = {2017-09-21 00:35:42},
  timestamp            = {2020-02-27 04:01},
}

@Article{Murtagh-Contreras-2011,
  author               = {Murtagh, Fionn and Contreras, Pedro},
  date                 = {2011-04},
  journaltitle         = {arXiv e-Print},
  title                = {Methods of Hierarchical Clustering},
  eprint               = {1105.0121},
  eprinttype           = {arXiv},
  url                  = {https://arxiv.org/abs/1105.0121},
  abstract             = {We survey agglomerative hierarchical clustering algorithms and discuss efficient implementations that are available in R and other software environments. We look at hierarchical self-organizing maps, and mixture models. We review grid-based clustering, focusing on hierarchical density-based approaches. Finally we describe a recently developed very efficient (linear time) hierarchical clustering algorithm, which can also be viewed as a hierarchical grid-based algorithm.},
  citeulike-article-id = {10504808},
  citeulike-linkout-0  = {http://arxiv.org/abs/1105.0121},
  citeulike-linkout-1  = {http://arxiv.org/pdf/1105.0121},
  day                  = {30},
  groups               = {Clustering and network analysis},
  owner                = {cristi},
  posted-at            = {2016-09-28 19:36:48},
  timestamp            = {2020-02-27 04:01},
}

@Article{Murtagh-Contreras-2012,
  author               = {Murtagh, Fionn and Contreras, Pedro},
  date                 = {2012-01},
  journaltitle         = {WIREs Data Mining Knowl Discov},
  title                = {Algorithms for hierarchical clustering: an overview},
  doi                  = {10.1002/widm.53},
  number               = {1},
  pages                = {86--97},
  volume               = {2},
  abstract             = {We survey agglomerative hierarchical clustering algorithms and discuss efficient implementations that are available in R and other software environments. We look at hierarchical self-organizing maps, and mixture models. We review grid-based clustering, focusing on hierarchical density-based approaches. Finally, we describe a recently developed very efficient (linear time) hierarchical clustering algorithm, which can also be viewed as a hierarchical grid-based algorithm.},
  citeulike-article-id = {13987836},
  citeulike-linkout-0  = {http://dx.doi.org/10.1002/widm.53},
  day                  = {1},
  groups               = {Networks and investment management, Clustering and network analysis},
  owner                = {cristi},
  posted-at            = {2016-03-25 04:22:46},
  publisher            = {John Wiley and Sons, Inc.},
  timestamp            = {2020-02-27 04:01},
}

@Article{Murtagh-Contreras-2017,
  author               = {Murtagh, Fionn and Contreras, Pedro},
  date                 = {2017-11},
  journaltitle         = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  title                = {Algorithms for hierarchical clustering: an overview, II},
  doi                  = {10.1002/widm.1219},
  issn                 = {1942-4787},
  number               = {6},
  pages                = {e1219--n/a},
  volume               = {7},
  abstract             = {We survey agglomerative hierarchical clustering algorithms and discuss efficient implementations that are available in R and other software environments. We look at hierarchical self-organizing maps and mixture models. We review grid-based clustering, focusing on hierarchical density-based approaches. Finally, we describe a recently developed very efficient (linear time) hierarchical clustering algorithm, which can also be viewed as a hierarchical grid-based algorithm. This review adds to the earlier version, Murtagh F, Contreras P. Algorithms for hierarchical clustering: an overview, Wiley Interdiscip Rev: Data Mining Knowl Discov 2012, 2, 86-97. WIREs Data Mining Knowl Discov 2017, 7:e1219. doi: 10.1002/widm.1219 For further resources related to this article, please visit the WIREs website.},
  citeulike-article-id = {14449844},
  citeulike-linkout-0  = {http://dx.doi.org/10.1002/widm.1219},
  day                  = {1},
  journal              = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  posted-at            = {2017-11-23 20:46:27},
  publisher            = {Wiley Periodicals, Inc},
  timestamp            = {2020-02-27 04:01},
  year                 = {2017},
}

@Article{Murtagh-Legendre-2014,
  author               = {Murtagh, Fionn and Legendre, Pierre},
  date                 = {2014-11},
  journaltitle         = {Journal of Classification},
  title                = {Ward's Hierarchical Agglomerative Clustering Method: Which Algorithms Implement Ward's Criterion?},
  doi                  = {10.1007/s00357-014-9161-z},
  number               = {3},
  pages                = {274--295},
  volume               = {31},
  abstract             = {The Ward error sum of squares hierarchical clustering method has been very widely used since its first description by Ward in a 1963 publication. It has also been generalized in various ways. Two algorithms are found in the literature and software, both announcing that they implement the Ward clustering method. When applied to the same distance matrix, they produce different results. One algorithm preserves Ward's criterion, the other does not. Our survey work and case studies will be useful for all those involved in developing software for data analysis using Ward's hierarchical clustering method.},
  citeulike-article-id = {14482080},
  citeulike-linkout-0  = {http://dx.doi.org/10.1007/s00357-014-9161-z},
  citeulike-linkout-1  = {http://link.springer.com/article/10.1007/s00357-014-9161-z},
  posted-at            = {2017-11-23 20:38:36},
  publisher            = {Springer US},
  timestamp            = {2020-02-27 04:01},
  year                 = {2014},
}

@Article{Narabin-Boongasame-2018,
  author       = {Santit Narabin and Laor Boongasame},
  date         = {2018},
  journaltitle = {International Conference on Big Data and Artificial Intelligence},
  title        = {A Cluster Analysis of Mutual Funds Data},
  url          = {https://ieeexplore.ieee.org/abstract/document/8546679},
  abstract     = {The factors of clustering mutual fund (such as Net Asset Value (NAV)) do not direct to both return and risk of mutual funds which they are important factors for investors. This research helps an investor can estimate profit and loss rate of the mutual fund in his/her portfolio by using the net asset value change ratios (NAVCR). Then, both the NAVCR and value of each mutual fund will be used for clustering. For building a portfolio, the mutual funds could be selected from the diversified groups in order to reduce risk. The mutual fund data at different times from the set for the fiscal year 2010 - 2017 are used. The results of our analysis show that our models offer significantly better performance than the portfolio management model derived from the random portfolio management.},
  timestamp    = {2020-02-27 04:01},
}

@Article{Nguyen-et-al-2017a,
  author         = {Nguyen, Hien D. and McLachlan, Geoffrey J. and Orban, Pierre and Bellec, Pierre and Janke, Andrew L.},
  date           = {2017-04},
  journaltitle   = {Neural Computation},
  title          = {Maximum Pseudolikelihood Estimation for Model-Based Clustering of Time Series Data.},
  doi            = {10.1162/{NECO\_a\_00938}},
  number         = {4},
  pages          = {990--1020},
  volume         = {29},
  abstract       = {Mixture of autoregressions (MoAR) models provide a model-based approach to the clustering of time series data. The maximum likelihood (ML) estimation of MoAR models requires evaluating products of large numbers of densities of normal random variables. In practical scenarios, these products converge to zero as the length of the time series increases, and thus the ML estimation of MoAR models becomes infeasible without the use of numerical tricks. We propose a maximum pseudolikelihood (MPL) estimation approach as an alternative to the use of numerical tricks. The MPL estimator is proved to be consistent and can be computed with an EM (expectation-maximization) algorithm. Simulations are used to assess the performance of the MPL estimator against that of the ML estimator in cases where the latter was able to be calculated. An application to the clustering of time series data arising from a resting state fMRI experiment is presented as a demonstration of the methodology.},
  f1000-projects = {QuantInvest},
  groups         = {Estim_MaxLikelihood},
  pmid           = {28095191},
  timestamp      = {2020-02-27 04:01},
}

@Article{Nie-2017,
  author               = {Nie, Chun-Xiao},
  date                 = {2017-06},
  journaltitle         = {Chaos, Solitons and Fractals},
  title                = {Dynamics of cluster structure in financial correlation matrix},
  doi                  = {10.1016/j.chaos.2017.05.039},
  issn                 = {0960-0779},
  abstract             = {The relationship between the correlation dimension of the financial market and the cluster structure in the correlation coefficient matrix is studied. Empirical results based on model data show that the clearer cluster structure corresponds to a smaller dimension. We use the algorithm to verify the relationship between the cluster structure of the real market data and the correlation dimension. The correlation dimensions in the financial market are calculated and used as a measure to study the cluster structure in the correlation coefficient matrix. First, based on the existing model, we present a toy model. Using the model-generated data, we find that the clearer cluster structure corresponds to a smaller dimension. It implies that the correlation dimension can be used as a measure of the cluster structure in the correlation coefficient matrix. Finally, we use the algorithm to compute the clusters in the real market and verify the previous empirical evidence. The results show that the cluster structure in the financial correlation coefficient matrix may change with time. The correlation dimension is smaller after the financial crisis, indicating that the cluster structure is clearer after the financial crisis.},
  citeulike-article-id = {14384085},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.chaos.2017.05.039},
  groups               = {Networks and investment management, Clustering and network analysis},
  posted-at            = {2017-06-28 18:36:06},
  timestamp            = {2020-02-27 04:01},
}

@Article{Nie-2017a,
  author               = {Nie, Chun-Xiao},
  date                 = {2017-09},
  journaltitle         = {Physica A: Statistical Mechanics and its Applications},
  title                = {Cluster structure in the correlation coefficient matrix can be characterized by abnormal eigenvalues},
  doi                  = {10.1016/j.physa.2017.09.066},
  issn                 = {0378-4371},
  abstract             = {n a large number of previous studies, the researchers found that some of the eigenvalues of the financial correlation matrix were greater than the predicted values of the random matrix theory (). Here, we call these eigenvalues as anomalous eigenvalues. In order to reveal the hidden meaning of these anomalous eigenvalues, we study the toy model with cluster structure and find that these eigenvalues are related to the cluster structure of the correlation coefficient matrix. In this paper, model-based experiments show that in most cases, the number of anomalous eigenvalues of the correlation matrix is equal to the number of clusters. In addition, empirical studies show that the sum of the anomalous eigenvalues is related to the clarity of the cluster structure and is negatively correlated with the correlation dimension.},
  citeulike-article-id = {14444598},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.physa.2017.09.066},
  posted-at            = {2017-10-03 13:16:43},
  timestamp            = {2020-02-27 04:01},
}

@Article{Nieminen-et-al-2013,
  author               = {Nieminen, Paavo and Polonen, Ilkka and Sipola, Tuomo},
  date                 = {2013-10},
  journaltitle         = {Journal of Informetrics},
  title                = {Research literature clustering using diffusion maps},
  doi                  = {10.1016/j.joi.2013.08.004},
  issn                 = {1751-1577},
  number               = {4},
  pages                = {874--886},
  volume               = {7},
  abstract             = {Adapting knowledge discovery process to scientometrics. Diffusion map-based clustering framework introduced to scientometrics. Structure of data mining literature revealed as a case study. We apply the knowledge discovery process to the mapping of current topics in a particular field of science. We are interested in how articles form clusters and what are the contents of the found clusters. A framework involving web scraping, keyword extraction, dimensionality reduction and clustering using the diffusion map algorithm is presented. We use publicly available information about articles in high-impact journals. The method should be of use to practitioners or scientists who want to overview recent research in a field of science. As a case study, we map the topics in data mining literature in the year 2011.},
  citeulike-article-id = {14212401},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.joi.2013.08.004},
  owner                = {cristi},
  posted-at            = {2016-11-21 22:04:14},
  timestamp            = {2020-02-27 04:01},
}

@Article{Ning-et-al-2015,
  author               = {Ning, Cathy and Xu, Dinghai and Wirjanto, Tony S.},
  date                 = {2015-03},
  journaltitle         = {Journal of Banking and Finance},
  title                = {Is volatility clustering of asset returns asymmetric?},
  doi                  = {10.1016/j.jbankfin.2014.11.016},
  issn                 = {0378-4266},
  pages                = {62--76},
  volume               = {52},
  abstract             = {We investigate the structure of volatility clustering of asset returns. We employ copula-based univariate time-series models and realized kernel volatility. We find that volatility clustering is highly nonlinear and strongly asymmetric. Our paper is the first one that models and finds asymmetric nonlinear volatility clustering. This finding is important in asset pricing, derivatives pricing, and risk management. Volatility clustering is a well-known stylized feature of financial asset returns. This paper investigates asymmetric pattern in volatility clustering by employing a univariate copula approach of Chen and Fan (2006). Using daily realized kernel volatilities constructed from high frequency data from stock and foreign exchange markets, we find evidence that volatility clustering is highly nonlinear and strongly asymmetric in that clusters of high volatility occur more often than clusters of low volatility. To the best of our knowledge, this paper is the first one to address and uncover this phenomenon. In particular, the asymmetry in volatility clustering is found to be more pronounced in the stock markets than in the foreign exchange markets. Further, the volatility clusters are shown to remain persistent for over a month and asymmetric across different time periods. Our findings have important implications for risk management. A simulation study indicates that models which accommodate asymmetric volatility clustering can significantly improve the out-of-sample forecasts of Value-at-Risk},
  citeulike-article-id = {14313629},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.jbankfin.2014.11.016},
  groups               = {Vol_Cluster},
  posted-at            = {2017-03-19 03:08:58},
  timestamp            = {2020-02-27 04:01},
}

@Article{Otranto-Gargano-2015,
  author               = {Otranto, Edoardo and Gargano, Romana},
  date                 = {2015},
  journaltitle         = {Advances in Data Analysis and Classification},
  title                = {Financial clustering in presence of dominant markets},
  doi                  = {10.1007/s11634-014-0189-z},
  number               = {3},
  pages                = {315--339},
  volume               = {9},
  abstract             = {Clustering financial time series is a recent topic of statistical literature with important fields of applications, in particular portfolio composition and risk evaluation. The risk is generally linked to the volatility of the asset, but its level of predictability also plays a basic role in investment decisions. In particular, the classification of a certain asset could be linked to its dependence on the volatility of a dominant market: movements in the volatility of the dominant market can provide similar movements in the volatility of the asset and its predictability would depend on the strength of this dependence. Working in a model based framework, we base the classification of the volatility of an asset not only on its volatility level, but also on the presence of spillover effects from a dominant market, such as the US one, and on the similarity of the dynamics of the asset and the dominant market. The method is carried out using an extended version of the Multiplicative Error Model and is applied to a set of European assets, also performing a historical simulation experiment.},
  citeulike-article-id = {14014267},
  citeulike-linkout-0  = {http://dx.doi.org/10.1007/s11634-014-0189-z},
  citeulike-linkout-1  = {http://link.springer.com/article/10.1007/s11634-014-0189-z},
  groups               = {Networks and investment management, Clustering and network analysis, Vol_Cluster},
  owner                = {cristi},
  posted-at            = {2016-04-17 16:15:18},
  publisher            = {Springer Berlin Heidelberg},
  timestamp            = {2020-02-27 04:01},
}

@InProceedings{Palupi-et-al-2019,
  author         = {Palupi, Irma and Wahyudi, Bambang Ari and Indwiarti, Indwiarti},
  booktitle      = {7th International Conference on Information and Communication Technology (ICoICT)},
  date           = {2019-07-24},
  title          = {The clustering algorithms approach for decision efficiency in investment portfolio diversification},
  doi            = {10.1109/{ICoICT}.2019.8835314},
  isbn           = {978-1-5386-8052-0},
  pages          = {1--6},
  publisher      = {IEEE},
  url            = {https://ieeexplore.ieee.org/document/8835314/},
  urldate        = {2019-12-03},
  abstract       = {This paper performs a clustering algorithm for portfolio investment diversification. The clustering process is applied to choose the preferred assets among hundreds of assets provided in the market under the related features. This work experimentally provides four features as the coordinate of assets that are mean, variance, skewness, and kurtosis of the returns. The used data is the daily close price of 175 assets in Indonesian exchange (IDX). We perform 4 clustering algorithms to locate the assets that have the same similarity into the same cluster. Since the four features are being considered, the spherical-shape of data is difficult to observe. In fact, the portfolio return of all algorithm's outcome show the Agglomerative and DBScan algorithm yield higher performance evaluation with no dominant asset included. Assets representatives from each cluster are determined to be in the portfolio formation. By using the portfolio theory by Markowitz, i.e Mean-variance optimization (MVO) and Sharp ration optimization, the proportion of contained assets are computed, and we test their performance by applying the formation into the market data a month after as a testing data. Several interesting results are provided.},
  day            = {24},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 04:04},
}

@Article{Paparrizos-Gravano-2017,
  author               = {Paparrizos, John and Gravano, Luis},
  date                 = {2017-06},
  journaltitle         = {ACM Transactions on Database Systems},
  title                = {Fast and Accurate Time-Series Clustering},
  doi                  = {10.1145/3044711},
  issn                 = {0362-5915},
  number               = {2},
  volume               = {42},
  abstract             = {The proliferation and ubiquity of temporal data across many disciplines has generated substantial interest in the analysis and mining of time series. Clustering is one of the most popular data-mining methods, not only due to its exploratory power but also because it is often a preprocessing step or subroutine for other techniques. In this article, we present k-Shape and k-MultiShapes (k-MS), two novel algorithms for time-series clustering. k-Shape and k-MS rely on a scalable iterative refinement procedure. As their distance measure, k-Shape and k-MS use shape-based distance (SBD), a normalized version of the cross-correlation measure, to consider the shapes of time series while comparing them. Based on the properties of SBD, we develop two new methods, namely ShapeExtraction (SE) and MultiShapesExtraction (MSE), to compute cluster centroids that are used in every iteration to update the assignment of time series to clusters. k-Shape relies on SE to compute a single centroid per cluster based on all time series in each cluster. In contrast, k-MS relies on MSE to compute multiple centroids per cluster to account for the proximity and spatial distribution of time series in each cluster. To demonstrate the robustness of SBD, k-Shape, and k-MS, we perform an extensive experimental evaluation on 85 datasets against state-of-the-art distance measures and clustering methods for time series using rigorous statistical analysis. SBD, our efficient and parameter-free distance measure, achieves similar accuracy to Dynamic Time Warping (DTW), a highly accurate but computationally expensive distance measure that requires parameter tuning. For clustering, we compare k-Shape and k-MS against scalable and non-scalable partitional, hierarchical, spectral, density-based, and shapelet-based methods, with combinations of the most competitive distance measures. k-Shape outperforms all scalable methods in terms of accuracy. Furthermore, k-Shape also outperforms all non-scalable approaches, with one exception, namely k-medoids with DTW, which achieves similar accuracy. However, unlike k-Shape, this approach requires tuning of its distance measure and is significantly slower than k-Shape. k-MS performs similarly to k-Shape in comparison to rival methods, but k-MS is significantly more accurate than k-Shape. Beyond clustering, we demonstrate the effectiveness of k-Shape to reduce the search space of one-nearest-neighbor classifiers for time series. Overall, SBD, k-Shape, and k-MS emerge as domain-independent, highly accurate, and efficient methods for time-series comparison and clustering with broad applications.},
  citeulike-article-id = {14435136},
  citeulike-linkout-0  = {http://portal.acm.org/citation.cfm?id=3086510.3044711},
  citeulike-linkout-1  = {http://dx.doi.org/10.1145/3044711},
  groups               = {Scenario_TimeSeries},
  location             = {New York, NY, USA},
  posted-at            = {2017-09-21 00:10:48},
  publisher            = {ACM},
  timestamp            = {2020-02-27 04:04},
}

@Article{Park-2020,
  author         = {Park, Jinwoo},
  date           = {2020-01-09},
  journaltitle   = {arXiv e-Print},
  title          = {Clustering Approaches for Global Minimum Variance Portfolio},
  url            = {https://arxiv.org/abs/2001.02966v2},
  urldate        = {2020-01-17},
  abstract       = {The only input to attain the portfolio weights of global minimum variance portfolio (GMVP) is the covariance matrix of returns of assets being considered for investment. Since the population covariance matrix is not known, investors use historical data to estimate it. Even though sample covariance matrix is an unbiased estimator of the population covariance matrix, it includes a great amount of estimation error especially when the number of observed data is not much bigger than number of assets. As it is difficult to estimate the covariance matrix with high dimensionality all at once, clustering stocks is proposed to come up with covariance matrix in two steps: firstly, within a cluster and secondly, between clusters. It decreases the estimation error by reducing the number of features in the data matrix. The motivation of this dissertation is that the estimation error can still remain high even after clustering, if a large amount of stocks is clustered together in a single group. This research proposes to utilize a bounded clustering method in order to limit the maximum cluster size. The result of experiments shows that not only the gap between in-sample volatility and out-of-sample volatility decreases, but also the out-of-sample volatility gets reduced. It implies that we need a bounded clustering algorithm so that maximum clustering size can be precisely controlled to find the best portfolio performance.},
  day            = {9},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 04:04},
}

@Article{Pasteris-et-al-2018,
  author         = {Pasteris, Stephen and Vitale, Fabio and Gentile, Claudio and Herbster, Mark},
  date           = {2018-04-09},
  journaltitle   = {Proceedings of Machine Learning Research},
  title          = {On Similarity Prediction and Pairwise Clustering},
  url            = {http://proceedings.mlr.press/v83/pasteris18a.html},
  urldate        = {2019-09-15},
  abstract       = {We consider the problem of clustering a finite set of items from pairwise similarity information. Unlike what is done in the literature on this subject, we do so in a passive learning setting, and with no specific constraints on the cluster shapes other than their size. We investigate the problem in different settings: i. an online setting, where we provide a tight characterization of the prediction complexity in the mistake bound model, and ii. a standard stochastic batch setting, where we give tight upper and lower bounds on the achievable generalization error. Prediction performance is measured both in terms of the ability to recover the similarity function encoding the hidden clustering and in terms of how well we classify each item within the set. The proposed algorithms are time efficient.},
  day            = {9},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 04:04},
}

@InCollection{Patel-Thakral-2016,
  author               = {Patel, K. M. Archana and Thakral, Prateek},
  booktitle            = {International Conference on Communication and Signal Processing (ICCSP)},
  date                 = {2016-04},
  title                = {The best clustering algorithms in data mining},
  doi                  = {10.1109/iccsp.2016.7754534},
  isbn                 = {978-1-5090-0396-9},
  location             = {Melmaruvathur, Tamilnadu, India},
  pages                = {2042--2046},
  publisher            = {IEEE},
  abstract             = {In data mining, Clustering is the most popular, powerful and commonly used unsupervised learning technique. It is a way of locating similar data objects into clusters based on some similarity. Clustering algorithms can be categorized into seven groups, namely Hierarchical clustering algorithm, Density-based clustering algorithm, Partitioning clustering algorithm, Graph-based algorithm, Grid-based algorithm, Model-based clustering algorithm and Combinational clustering algorithm. These clustering algorithms give different result according to the conditions. Some clustering techniques are better for large data set and some gives good result for finding cluster with arbitrary shapes. This paper is planned to learn and relates various data mining clustering algorithms. Algorithms which are under exploration as follows: K-Means algorithm, K-Medoids, Distributed K-Means clustering algorithm, Hierarchical clustering algorithm, Grid-based Algorithm and Density based clustering algorithm. This paper compared all these clustering algorithms according to the many factors. After comparison of these clustering algorithms I describe that which clustering algorithms should be used in different conditions for getting the best result.},
  citeulike-article-id = {14335037},
  citeulike-linkout-0  = {http://dx.doi.org/10.1109/iccsp.2016.7754534},
  groups               = {Clustering and network analysis},
  posted-at            = {2017-04-10 12:14:30},
  timestamp            = {2020-02-27 04:04},
}

@Article{Pattarin-et-al-2004,
  author               = {Pattarin, Francesco and Paterlini, Sandra and Minerva, Tommaso},
  date                 = {2004-09},
  journaltitle         = {Computational Statistics \& Data Analysis},
  title                = {Clustering financial time series: an application to mutual funds style analysis},
  doi                  = {10.1016/j.csda.2003.11.009},
  issn                 = {0167-9473},
  number               = {2},
  pages                = {353--372},
  volume               = {47},
  abstract             = {Classification can be useful in giving a synthetic and informative description of contexts characterized by high degrees of complexity. Different approaches could be adopted to tackle the classification problem: statistical tools may contribute to increase the degree of confidence in the classification scheme. A classification algorithm for mutual funds style analysis is proposed, which combines different statistical techniques and exploits information readily available at low cost. Objective, representative, consistent and empirically testable classification schemes are strongly sought for in this field in order to give reliable information to investors and fund managers who are interested in evaluating and comparing different financial products. Institutional classification schemes, when available, do not always provide consistent and representative peer groups of funds. A "return-based"classification scheme is proposed, which aims at identifying mutual funds' styles by analysing time series of past returns. The proposed classification procedure consists of three basic steps: (a) a dimensionality reduction step based on principal component analysis, (b) a clustering step that exploits a robust evolutionary clustering methodology, and (c) a style identification step via a constrained regression model first proposed by William Sharpe. The algorithm is tested on a sample of Italian mutual funds and achieves satisfactory results with respect to (i) the agreement with the existing institutional classification and (ii) the explanatory power of out of sample variability in the cross-section of returns.},
  citeulike-article-id = {14367330},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.csda.2003.11.009},
  groups               = {Networks and investment management, Clustering and network analysis},
  posted-at            = {2017-06-02 22:29:31},
  timestamp            = {2020-02-27 04:04},
}

@InCollection{Peker-Aktan-2014,
  author               = {Peker, Sinem and Aktan, Bora},
  booktitle            = {The 8th International Scientific Conference "Business and Management 2014"},
  date                 = {2014},
  title                = {Clustering In European Stock Indices In Crisis And Non-Crisis Periods},
  doi                  = {10.3846/bm.2014.037},
  isbn                 = {9786094576508},
  location             = {Vilnius, Lithuania},
  publisher            = {Vilnius Gediminas Technical University Publishing House Technika},
  abstract             = {Grouping the major indices of stock markets based on their homogeneities may facilitate the selection period for investors especially today's information rich financial world. This paper attempts to detect and group the homogenous stock indices in Europe both throughout the crisis and non-crisis periods. The daily index returns of leading stock exchanges over the period 03.01.2007-09.04.2013 are considered; one of the hierarchical clustering techniques so-called Ward's Method is applied and similar cases are evaluated respectively. Then, Wilcoxon signed rank test is employed for the same periods on daily index returns and meaningful differences are found.},
  citeulike-article-id = {14149920},
  citeulike-linkout-0  = {http://dx.doi.org/10.3846/bm.2014.037},
  groups               = {Networks and investment management, Clustering and network analysis},
  owner                = {cristi},
  posted-at            = {2016-10-01 13:28:33},
  timestamp            = {2020-02-27 04:04},
}

@Article{Peng-et-al-2016,
  author               = {Peng, Chong and Kang, Zhao and Yang, Ming and Cheng, Qiang},
  date                 = {2016-07},
  journaltitle         = {IEEE Signal Processing Letters},
  title                = {Feature Selection Embedded Subspace Clustering},
  doi                  = {10.1109/lsp.2016.2573159},
  issn                 = {1070-9908},
  number               = {7},
  pages                = {1018--1022},
  volume               = {23},
  abstract             = {We propose a new subspace clustering method that integrates feature selection into subspace clustering. Rather than using all features to construct a low-rank representation of the data, we find such a representation using only relevant features, which helps in revealing more accurate data relationships. Two variants are proposed by using both convex and nonconvex rank approximations. Extensive experimental results confirm the effectiveness of the proposed method and models.},
  citeulike-article-id = {14351207},
  citeulike-linkout-0  = {http://dx.doi.org/10.1109/lsp.2016.2573159},
  posted-at            = {2017-05-05 01:22:20},
  timestamp            = {2020-02-27 04:04},
}

@Article{Ponta-Carbone-2019,
  author         = {Ponta, L. and Carbone, A.},
  date           = {2019-08-01},
  journaltitle   = {arXiv e-Print},
  title          = {Quantifying horizon dependence of asset prices: a cluster entropy approach},
  url            = {https://arxiv.org/abs/1908.00257},
  urldate        = {2019-08-11},
  abstract       = {Market dynamic is studied by quantifying the dependence of the entropy S(,n) of the clusters formed by the series of the prices pt and its moving average pt,n on temporal horizon M. We report results of the analysis performed on high-frequency data of the Nasdaq Composite, Dow Jones Industrial Avg and Standard \& Poor 500 indexes downloaded from the Bloomberg terminal this http URL. Both raw and sampled data series have been analysed for a broad range of horizons M, varying from one to twelve months over the year 2018. A systematic dependence of the cluster entropy function S(,n) on the horizon M has been evidenced in the analysed assets. Hence, the cluster entropy function is integrated over the cluster to yield a synthetic indicator of price evolution: the Market Dynamic Index I(M,n). Moreover, the Market Horizon Dependence defined as H(M,n)=I(M,n)-I(1,n) is calculated and compared with the values of the horizon dependence of the pricing kernel with different representative agent models obtained by a Kullback-Leibler entropy approach.},
  day            = {1},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 04:04},
}

@Article{Raffinot-2018,
  author               = {Raffinot, Thomas},
  date                 = {2017-12-22},
  journaltitle         = {The Journal of Portfolio Management},
  title                = {Hierarchical Clustering-Based Asset Allocation},
  doi                  = {10.3905/jpm.2018.44.2.089},
  issn                 = {0095-4918},
  number               = {2},
  pages                = {89--99},
  volume               = {44},
  abstract             = {This article proposes a hierarchical clustering-based asset allocation method, which uses graph theory and machine learning techniques. Hierarchical clustering refers to the formation of a recursive clustering, suggested by the data, not defined a priori. Several hierarchical clustering methods are presented and tested. Once the assets are hierarchically clustered, the authors compute a simple and efficient capital allocation within and across clusters of assets, so that many correlated assets receive the same total allocation as a single uncorrelated one. The out-of-sample performances of hierarchical clustering-based portfolios and more traditional risk-based portfolios are evaluated across three disparate datasets, which differ in term of the number of assets and the assets' composition. To avoid data snooping, the authors assess the comparison of profit measures using the bootstrap-based model confidence set procedure. Their empirical results indicate that hierarchical clustering-based portfolios are robust and truly diversified and achieve statistically better risk-adjusted performances than commonly used portfolio optimization techniques.},
  citeulike-article-id = {14510373},
  citeulike-linkout-0  = {http://dx.doi.org/10.3905/jpm.2018.44.2.089},
  day                  = {22},
  groups               = {Network_Invest, ML_Network_QWIM, PortfOptim_Network, Invest_Network, ML_InvestSelect},
  posted-at            = {2017-12-30 13:27:26},
  timestamp            = {2020-02-27 04:04},
}

@Article{Rastelli-Friel-2017,
  author         = {Rastelli, Riccardo and Friel, Nial},
  date           = {2017-10-31},
  journaltitle   = {Statistics and Computing},
  title          = {Optimal Bayesian estimators for latent variable cluster models},
  doi            = {10.1007/s11222-017-9786-y},
  issn           = {0960-3174},
  pages          = {1--18},
  abstract       = {In cluster analysis interest lies in probabilistically capturing partitions of individuals, items or observations into groups, such that those belonging to the same group share similar attributes or relational profiles. Bayesian posterior samples for the latent allocation variables can be effectively obtained in a wide range of clustering models, including finite mixtures, infinite mixtures, hidden Markov models and block models for networks. However, due to the categorical nature of the clustering variables and the lack of scalable algorithms, summary tools that can interpret such samples are not available. We adopt a Bayesian decision theoretical approach to define an optimality criterion for clusterings and propose a fast and context-independent greedy algorithm to find the best allocations. One important facet of our approach is that the optimal number of groups is automatically selected, thereby solving the clustering and the model-choice problems at the same time. We consider several loss functions to compare partitions and show that our approach can accommodate a wide range of cases. Finally, we illustrate our approach on both artificial and real datasets for three different clustering models: Gaussian mixtures, stochastic block models and latent block models for networks.},
  day            = {31},
  f1000-projects = {QuantInvest},
  groups         = {Proba_Bayes},
  timestamp      = {2020-02-27 04:04},
}

@Article{Ren-et-al-2017,
  author               = {Ren, Fei and Lu, Ya-Nan and Li, Sai-Ping and Jiang, Xiong-Fei and Zhong, Li-Xin and Qiu, Tian},
  date                 = {2017-01},
  journaltitle         = {PLOS ONE},
  title                = {Dynamic Portfolio Strategy Using Clustering Approach},
  doi                  = {10.1371/journal.pone.0169299},
  number               = {1},
  pages                = {e0169299+},
  volume               = {12},
  abstract             = {The problem of portfolio optimization is one of the most important issues in asset management. We here propose a new dynamic portfolio strategy based on the time-varying structures of MST networks in Chinese stock markets, where the market condition is further considered when using the optimal portfolios for investment. A portfolio strategy comprises two stages: First, select the portfolios by choosing central and peripheral stocks in the selection horizon using five topological parameters, namely degree, betweenness centrality, distance on degree criterion, distance on correlation criterion and distance on distance criterion. Second, use the portfolios for investment in the investment horizon. The optimal portfolio is chosen by comparing central and peripheral portfolios under different combinations of market conditions in the selection and investment horizons. Market conditions in our paper are identified by the ratios of the number of trading days with rising index to the total number of trading days, or the sum of the amplitudes of the trading days with rising index to the sum of the amplitudes of the total trading days. We find that central portfolios outperform peripheral portfolios when the market is under a drawup condition, or when the market is stable or drawup in the selection horizon and is under a stable condition in the investment horizon. We also find that peripheral portfolios gain more than central portfolios when the market is stable in the selection horizon and is drawdown in the investment horizon. Empirical tests are carried out based on the optimal portfolio strategy. Among all possible optimal portfolio strategies based on different parameters to select portfolios and different criteria to identify market conditions, 65 percent of our optimal portfolio strategies outperform the random strategy for the Shanghai A-Share market while the proportion is 70 percent for the Shenzhen A-Share market.},
  citeulike-article-id = {14291490},
  citeulike-linkout-0  = {http://dx.doi.org/10.1371/journal.pone.0169299},
  day                  = {27},
  groups               = {Networks and investment management, Network_Invest, PortfOptim_Dynamic, PortfOptim_Network, Invest_Network},
  posted-at            = {2017-03-03 18:42:12},
  publisher            = {Public Library of Science},
  timestamp            = {2020-02-27 04:04},
}

@Article{Roick-et-al-2019,
  author         = {Roick, Tyler and Karlis, Dimitris and McNicholas, Paul D.},
  date           = {2019-01-26},
  journaltitle   = {arXiv e-Print},
  title          = {Clustering Discrete Valued Time Series},
  url            = {https://arxiv.org/abs/1901.09249},
  urldate        = {2019-03-07},
  abstract       = {There is a need for the development of models that are able to account for discreteness in data, along with its time series properties and correlation. Our focus falls on INteger-valued AutoRegressive (INAR) type models. The INAR type models can be used in conjunction with existing model-based clustering techniques to cluster discrete valued time series data. With the use of a finite mixture model, several existing techniques such as the selection of the number of clusters, estimation using expectation-maximization and model selection are applicable. The proposed model is then demonstrated on real data to illustrate its clustering applications.},
  day            = {26},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 04:04},
}

@Article{Ronan-et-al-2018,
  author       = {Tom Ronan and Shawn Anastasio and Zhijie Qi and Pedro Henrique S. Vieira Tavares and Roman Sloutsky and Kristen M. Naegle},
  date         = {2018},
  journaltitle = {Journal of Machine Learning Research},
  title        = {OpenEnsembles: A Python Resource for Ensemble Clustering},
  number       = {26},
  pages        = {1--6},
  url          = {http://jmlr.org/papers/v19/18-100.html},
  volume       = {19},
  abstract     = {In this paper we introduce OpenEnsembles, a Python toolkit for performing and analyzing ensemble clustering. Ensemble clustering is the process of creating many clustering solutions for a given dataset and utilizing the relationships observed across the ensemble to identify final solutions, which are more robust, stable or better than the individual solutions within the ensemble. The OpenEnsembles library provides a unified interface for applying transformations to data, clustering data, visualizing individual clustering solutions, visualizing and finishing the ensemble, and calculating validation metrics for a clustering solution for any given partitioning of the data. We have documented examples of using OpenEnsembles to create, analyze, and visualize a number of different types of ensemble approaches on toy and example datasets. OpenEnsembles is released under the GNU General Public License version 3, can be installed via Conda or the Python Package Index (pip), and is available at https://github.com/NaegleLab/OpenEnsembles.},
  timestamp    = {2020-02-27 04:04},
}

@Article{Ross-2015a,
  author               = {Ross, Gordon J.},
  date                 = {2015-05},
  journaltitle         = {Physical Review E},
  title                = {Dynamic Multi-Factor Clustering of Financial Networks},
  doi                  = {10.1103/physreve.89.022809},
  eprint               = {1505.01550},
  eprinttype           = {arXiv},
  issn                 = {1539-3755},
  number               = {2},
  volume               = {89},
  abstract             = {We investigate the tendency for financial instruments to form clusters when there are multiple factors influencing the correlation structure. Specifically, we consider a stock portfolio which contains companies from different industrial sectors, located in several different countries. Both sector membership and geography combine to create a complex clustering structure where companies seem to first be divided based on sector, with geographical subclusters emerging within each industrial sector. We argue that standard techniques for detecting overlapping clusters and communities are not able to capture this type of structure, and show how robust regression techniques can instead be used to remove the influence of both sector and geography from the correlation matrix separately. Our analysis reveals that prior to the 2008 financial crisis, companies did not tend to form clusters based on geography. This changed immediately following the crisis, with geography becoming a more important determinant of clustering.},
  citeulike-article-id = {14150018},
  citeulike-linkout-0  = {http://arxiv.org/abs/1505.01550},
  citeulike-linkout-1  = {http://arxiv.org/pdf/1505.01550},
  citeulike-linkout-2  = {http://dx.doi.org/10.1103/physreve.89.022809},
  day                  = {7},
  groups               = {Networks and investment management, Clustering and network analysis},
  owner                = {cristi},
  posted-at            = {2016-10-01 17:00:42},
  timestamp            = {2020-02-27 04:04},
}

@Article{Roy-et-al-2017,
  author               = {Roy, Aurko and Pokutta, Sebastian},
  date                 = {2017},
  journaltitle         = {Journal of Machine Learning Research},
  title                = {Hierarchical Clustering via Spreading Metrics},
  number               = {88},
  pages                = {1--35},
  url                  = {http://jmlr.org/papers/v18/17-081.html},
  volume               = {18},
  abstract             = {We study the cost function for hierarchical clusterings introduced by (Dasgupta, 2016) where hierarchies are treated as first-class objects rather than deriving their cost from projections into flat clusters. It was also shown in (Dasgupta, 2016) that a top-down algorithm based on the uniform Sparsest Cut problem returns a hierarchical clustering of cost at most O(alpha n log n)O(alpha n log n) times the cost of the optimal hierarchical clustering, where alpha is the approximation ratio of the Sparsest Cut subroutine used. Thus using the best known approximation algorithm for Sparsest Cut due to Arora-Rao- Vazirani, the top-down algorithm returns a hierarchical clustering of cost at most O(log 3/2n)O(log 3/2n) times the cost of the optimal solution. We improve this by giving an O(log n)O(log n))-approximation algorithm for this problem. Our main technical ingredients are a combinatorial characterization of ultrametrics induced by this cost function, deriving an Integer Linear Programming (ILP) formulation for this family of ultrametrics, and showing how to iteratively round an LP relaxation of this formulation by using the idea of sphere growing which has been extensively used in the context of graph partitioning. We also prove that our algorithm returns an O(log n)O(log n)- approximate hierarchical clustering for a generalization of this cost function also studied in (Dasgupta, 2016). Experiments show that the hierarchies found by using the ILP formulation as well as our rounding algorithm often have better projections into flat clusters than the standard linkage based algorithms. We conclude with constant factor inapproximability results for this problem: 1) no polynomial size LP or SDP can achieve a constant factor approximation for this problem and 2) no polynomial time algorithm can achieve a constant factor approximation under the Small Set Expansion hypothesis.},
  citeulike-article-id = {14509660},
  citeulike-linkout-0  = {http://jmlr.org/papers/v18/17-081.html},
  keywords             = {*file-import-17-12-29},
  posted-at            = {2017-12-29 01:34:17},
  timestamp            = {2020-02-27 04:04},
}

@InCollection{Roy-Mandal-2015,
  author               = {Roy, Parthajit and Mandal, J. K.},
  booktitle            = {Computational Intelligence in Data Mining - Volume 3},
  date                 = {2015},
  title                = {Performance Evaluation of Some Clustering Indices},
  doi                  = {10.1007/978-81-322-2202-6\_46},
  editor               = {Jain, Lakhmi C. and Behera, Himansu S. and Mandal, Jyotsna K. and Mohapatra, Durga P.},
  pages                = {509--517},
  publisher            = {Springer India},
  series               = {Smart Innovation, Systems and Technologies},
  url                  = {http://dx.doi.org/10.1007/978-81-322-2202-6\_46},
  volume               = {33},
  abstract             = {This paper analyzes the performances of four internal and five external cluster validity indices. The internal indices are Banfeld-Raftery index, Davies-Bouldin index, Ray-Turi index and Scott-Symons index. Jaccard index, Folkes-Mallows index, Rand index, Rogers-Tanimoto index and Kulczynski index are the external indices considered. The standard K-Means algorithm and CLARA algorithm has been considered as testing models. Four standard data sets, namely Iris, Seeds, Wine and Flame data sets has been chosen for testing the performance of the indices. The performance of the indices with the increasing number of parameters of the data set is measured. The results are compared and analyzed.},
  citeulike-article-id = {14435141},
  citeulike-linkout-0  = {http://dx.doi.org/10.1007/978-81-322-2202-646},
  citeulike-linkout-1  = {http://link.springer.com/chapter/10.1007/978-81-322-2202-646},
  posted-at            = {2017-09-21 00:37:03},
  timestamp            = {2020-02-27 04:04},
}

@Article{Ryazanov-2016,
  author               = {Ryazanov, Vladimir},
  date                 = {2016-07},
  journaltitle         = {Intelligent Data Analysis},
  title                = {About estimation of quality of clustering results via its stability},
  doi                  = {10.3233/ida-160842},
  issn                 = {1088-467X},
  number               = {s1},
  pages                = {S5--S15},
  volume               = {20},
  abstract             = {We know that there are many clustering methods for the case of a known/unknown number of clusters. Clustering is a result of fulfillment of some stopping criterion. Usually, optimisation of some quality criterion is performed or iterative processes are accomplished. How to estimate the quality of clustering obtained by some method? Is the obtained clustering result corresponding to the objective reality or some stopping criterion of the algorithm is made and we have obtained only some partition? Here, a practical approach and the common general criteria based on an estimation of the stability of clustering are submitted. The criterion does not use any probabilistic assumptions or distances in feature space. For some well-known clustering algorithms, efficient methods for computing the introduced stability criteria according to the training set are obtained. Some illustrative real and artificial examples for various situations are shown.},
  citeulike-article-id = {14357928},
  citeulike-linkout-0  = {http://dx.doi.org/10.3233/ida-160842},
  day                  = {13},
  groups               = {Clustering and network analysis},
  posted-at            = {2017-05-16 14:07:47},
  timestamp            = {2020-02-27 04:04},
}

@Article{Sakakibara-et-al-2015,
  author               = {Sakakibara, Takumasa and Matsui, Tohgoroh and Mutoh, Atsuko and Inuzuka, Nobuhiro},
  date                 = {2015},
  journaltitle         = {Procedia Computer Science},
  title                = {Clustering Mutual Funds Based on Investment Similarity},
  doi                  = {10.1016/j.procs.2015.08.251},
  issn                 = {1877-0509},
  pages                = {881--890},
  volume               = {60},
  abstract             = {It is risky to invest to single or similar mutual funds because the variance of the return becomes large. Mutual funds are categorized based on the investment strategy by a company that rated funds based on performance, but the fund categories are different from its actual operations. While some previous studies have proposed methods to cluster mutual funds based on the historical performances, we cannot apply these methods to new mutual funds. In this paper, we clusters mutual funds based on the investment similarity instead of the historical performances. The contributions of this paper are: 1. To propose two new methods for classifying mutual funds based on the investment similarity, 2. To evaluate the proposed methods based on actual 551 Japanese mutual funds.},
  citeulike-article-id = {14212405},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.procs.2015.08.251},
  groups               = {Invest_Network},
  owner                = {cristi},
  posted-at            = {2016-11-21 22:10:38},
  timestamp            = {2020-02-27 04:04},
}

@Article{SardaEspinosa-2017,
  author       = {Alexis Sarda-Espinosa},
  date         = {2019},
  journaltitle = {The R Journal},
  title        = {Comparing Time-Series Clustering Algorithms in R Using the dtwclust Package},
  url          = {https://journal.r-project.org/archive/2019/RJ-2019-023/index.html},
  abstract     = {Most clustering strategies have not changed considerably since their initial definition. Most of the improvements are either related to the distance measure used to assess dissimilarity, or the function used to calculate prototypes or centroids. Time-series clustering is no exception, with the Dynamic Time Warping distance being particularly popular in that context. This distance is computationally expensive, so many related optimizations have been developed over the years. Since no single clustering algorithm can be said to perform best on all datasets, different strategies must be tested and compared, so a common infrastructure can be advantageous. In this manuscript, a general overview of time-series clustering is given, including many specifics related to Dynamic Time Warping and other recently proposed techniques. At the same time, a description of the dtwclust package for the R statistical software is provided, showcasing how it can be used to evaluate many different time-series clustering procedures.},
  howpublished = {Available at https://cran.r-project.org/web/packages/dtwclust/vignettes/dtwclust.pdf},
  timestamp    = {2020-02-27 04:04},
}

@Article{Schmitt-Westerhoff-2017,
  author               = {Schmitt, Noemi and Westerhoff, Frank},
  date                 = {2017-02},
  journaltitle         = {Quantitative Finance},
  title                = {Herding behaviour and volatility clustering in financial markets},
  doi                  = {10.1080/14697688.2016.1267391},
  pages                = {1--17},
  abstract             = {We propose a financial market model in which speculators follow a linear mix of technical and fundamental trading rules to determine their orders. Volatility clustering arises in our model due to speculators? herding behaviour. In case of heightened uncertainty, speculators observe other speculators? actions more closely. Since speculators? trading behaviour then becomes less heterogeneous, the market maker faces a less balanced excess demand and consequently adjusts prices more strongly. Estimating our model using the method of simulated moments reveals that it is able to explain a number of stylized facts of financial markets quite well. Various robustness checks with respect to the model setup reveal that our results are quite stable.},
  citeulike-article-id = {14316708},
  citeulike-linkout-0  = {http://dx.doi.org/10.1080/14697688.2016.1267391},
  citeulike-linkout-1  = {http://www.tandfonline.com/doi/abs/10.1080/14697688.2016.1267391},
  day                  = {1},
  groups               = {Networks and investment management, Clustering and network analysis, Vol_Cluster},
  posted-at            = {2017-03-23 08:51:27},
  publisher            = {Routledge},
  timestamp            = {2020-02-27 04:04},
}

@Article{Sekula-et-al-2017,
  author               = {Sekula, Michael and Datta, Somnath and Datta, Susmita},
  date                 = {2017-03},
  journaltitle         = {Bioinformation},
  title                = {optCluster: An R Package for Determining the Optimal Clustering Algorithm},
  doi                  = {10.6026/97320630013101},
  issn                 = {0973-8894},
  number               = {03},
  pages                = {101--103},
  volume               = {13},
  abstract             = {There exist numerous programs and packages that perform validation for a given clustering solution; however, clustering algorithms fare differently as judged by different validation measures. If more than one performance measure is used to evaluate multiple clustering partitions, an optimal result is often difficult to determine by visual inspection alone. This paper introduces optCluster, an R package that uses a single function to simultaneously compare numerous clustering partitions (created by different algorithms and/or numbers of clusters) and obtain a "best" option for a given dataset. The method of weighted rank aggregation is utilized by this package to objectively aggregate various performance measure scores, thereby taking away the guesswork that often follows a visual inspection of cluster results. The optCluster package contains biological validation measures as well as clustering algorithms developed specifically for RNA sequencing data, making it a useful tool for clustering genomic data.},
  citeulike-article-id = {14435134},
  citeulike-linkout-0  = {http://dx.doi.org/10.6026/97320630013101},
  day                  = {31},
  posted-at            = {2017-09-20 23:40:37},
  timestamp            = {2020-02-27 04:04},
}

@Article{Serviss-et-al-2017,
  author         = {Serviss, Jason T. and Gaadin, Jesper R. and Eriksson, Per and Folkersen, Lasse and Grander, Dan},
  date           = {2017-10-01},
  journaltitle   = {Bioinformatics},
  title          = {ClusterSignificance: a bioconductor package facilitating statistical analysis of class cluster separations in dimensionality reduced data},
  doi            = {10.1093/bioinformatics/btx393},
  number         = {19},
  pages          = {3126--3128},
  volume         = {33},
  abstract       = {Summary: Multi-dimensional data generated via high-throughput experiments is increasingly used in conjunction with dimensionality reduction methods to ascertain if resulting separations of the data correspond with known classes. This is particularly useful to determine if a subset of the variables, e.g. genes in a specific pathway, alone can separate samples into these established classes. Despite this, the evaluation of class separations is often subjective and performed via visualization. Here we present the ClusterSignificance package; a set of tools designed to assess the statistical significance of class separations downstream of dimensionality reduction algorithms. In addition, we demonstrate the design and utility of the ClusterSignificance package and utilize it to determine the importance of long non-coding RNA expression in the identity of multiple hematological malignancies. Availability and implementation: ClusterSignificance is an R package available via Bioconductor (https://bioconductor.org/packages/ClusterSignificance) under GPL-3. Contact: dan.grander@ki.se. Supplementary information: Supplementary data are available at Bioinformatics online.},
  day            = {1},
  f1000-projects = {QuantInvest},
  pmid           = {28957498},
  timestamp      = {2020-02-27 04:04},
}

@InCollection{Sherkat-et-al-2018,
  author         = {Sherkat, Ehsan and Nourashrafeddin, Seyednaser and Milios, Evangelos E. and Minghim, Rosane},
  booktitle      = {Proceedings of the 2018 Conference on Human Information Interaction\&Retrieval - IUI '18},
  date           = {2018-03-07},
  title          = {Interactive document clustering revisited: A visual analytics approach},
  doi            = {10.1145/3172944.3172964},
  isbn           = {9781450349451},
  location       = {New York, New York, USA},
  pages          = {281--292},
  publisher      = {ACM Press},
  url            = {http://dl.acm.org/citation.cfm?doid=3172944.3172964},
  abstract       = {Document clustering is an efficient way to get insight into large text collections. Due to the personalized nature of document clustering, even the best fully automatic algorithms cannot create clusters that accurately reflect the user's perspectives. To incorporate the users perspective in the clustering process and, at the same time, effectively visualize document collections to enhance user's sense-making of data, we propose a novel visual analytics system for interactive document clustering. We built our system on top of clustering algorithms that can adapt to user's feedback. First, the initial clustering is created based on the user-defined number of clusters and the selected clustering algorithm. Second, the clustering result is visualized to the user. A collection of coordinated visualization modules and document projection is designed to guide the user towards a better insight into the document collection and clusters. The user changes clusters and key-terms iteratively as a feedback to the clustering algorithm until the result is satisfactory. In key-term based interaction, the user assigns a set of key-terms to each target cluster to guide the clustering algorithm. A set of quantitative experiments, a use case, and a user study have been conducted to show the advantages of the approach for document analytics based on clustering.},
  day            = {7},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 04:04},
}

@Article{Song-et-al-2012b,
  author               = {Song, Won-Min and Di Matteo, T. and Aste, Tomaso},
  date                 = {2012-03},
  journaltitle         = {PLoS ONE},
  title                = {Hierarchical Information Clustering by Means of Topologically Embedded Graphs},
  doi                  = {10.1371/journal.pone.0031929},
  number               = {3},
  pages                = {e31929+},
  volume               = {7},
  abstract             = {We introduce a graph-theoretic approach to extract clusters and hierarchies in complex data-sets in an unsupervised and deterministic manner, without the use of any prior information. This is achieved by building topologically embedded networks containing the subset of most significant links and analyzing the network structure. For a planar embedding, this method provides both the intra-cluster hierarchy, which describes the way clusters are composed, and the inter-cluster hierarchy which describes how clusters gather together. We discuss performance, robustness and reliability of this method by first investigating several artificial data-sets, finding that it can outperform significantly other established approaches. Then we show that our method can successfully differentiate meaningful clusters and hierarchies in a variety of real data-sets. In particular, we find that the application to gene expression patterns of lymphoma samples uncovers biologically significant groups of genes which play key-roles in diagnosis, prognosis and treatment of some of the most relevant human lymphoid malignancies.},
  citeulike-article-id = {10441261},
  citeulike-linkout-0  = {http://dx.doi.org/10.1371/journal.pone.0031929},
  day                  = {9},
  owner                = {cristi},
  posted-at            = {2016-09-27 15:16:15},
  publisher            = {Public Library of Science},
  timestamp            = {2020-02-27 04:04},
}

@Article{So-Yip-2012,
  author               = {So, Mike K. P. and Yip, Iris W. H.},
  date                 = {2012-08},
  journaltitle         = {Journal of Forecasting},
  title                = {Multivariate GARCH Models with Correlation Clustering},
  doi                  = {10.1002/for.1234},
  number               = {5},
  pages                = {443--468},
  volume               = {31},
  abstract             = {A new clustered correlation multivariate generalized autoregressive conditional heteroskedasticity (CC-MGARCH) model that allows conditional correlations to form clusters is proposed. This model generalizes the time-varying correlation structure of Tse and Tsui (2002, Journal of Business and Economic Statistics 20: 351-361) by classifying the correlations among the series into groups.

To estimate the proposed model, Markov chain Monte Carlo methods are adopted. Two efficient sampling schemes for drawing discrete indicators are also developed. Simulations show that these efficient sampling schemes can lead to substantial savings in computation time in Monte Carlo procedures involving discrete indicators.

Empirical examples using stock market and exchange rate data are presented in which two-cluster and three-cluster models are selected using posterior probabilities. This implies that the conditional correlation equation is likely to be governed by more than one set of decaying parameters.},
  citeulike-article-id = {13935025},
  citeulike-linkout-0  = {http://dx.doi.org/10.1002/for.1234},
  day                  = {1},
  owner                = {cristi},
  posted-at            = {2016-02-18 07:21:10},
  publisher            = {John Wiley and Sons, Ltd},
  timestamp            = {2020-02-27 04:04},
}

@InCollection{Strapasson-et-al-2016,
  author               = {Strapasson, Joao E. and Pinele, Julianna and Costa, Sueli I. R.},
  booktitle            = {IEEE Sensor Array and Multichannel Signal Processing Workshop (SAM)},
  date                 = {2016-07},
  title                = {Clustering using the Fisher-Rao distance},
  doi                  = {10.1109/sam.2016.7569717},
  isbn                 = {978-1-5090-2103-1},
  location             = {Rio de Janerio, Brazil},
  pages                = {1--5},
  publisher            = {IEEE},
  abstract             = {In this paper we consider the Fisher-Rao distance in the space of the multivariate diagonal Gaussian distributions for clustering methods. Centroids in this space are derived and used to introduce two clustering algorithms for diagonal Gaussian mixture models associated to this metric: the k-means and the hierarchical clustering. These algorithms allow to reduce the number of components of such mixture models in the context of image segmentation. The algorithms presented here are compared with the Bregman hard and hierarchical clustering algorithms regarding the advantages of each method in different situations.},
  citeulike-article-id = {14335039},
  citeulike-linkout-0  = {http://dx.doi.org/10.1109/sam.2016.7569717},
  groups               = {Clustering and network analysis},
  posted-at            = {2017-04-10 12:16:22},
  timestamp            = {2020-02-27 04:04},
}

@Article{Tang-et-al-2017,
  author         = {Tang, Yang and Browne, Ryan P. and McNicholas, Paul D.},
  date           = {2017-05-09},
  journaltitle   = {arXiv e-Print},
  title          = {Flexible Clustering for High-Dimensional Data via Mixtures of Joint Generalized Hyperbolic Models},
  url            = {https://arxiv.org/abs/1705.03130},
  abstract       = {A mixture of joint generalized hyperbolic distributions (MJGHD) is introduced for asymmetric clustering for high-dimensional data. The MJGHD approach takes into account the cluster-specific subspace, thereby limiting the number of parameters to estimate while also facilitating visualization of results. Identifiability is discussed, and a multi-cycle ECM algorithm is outlined for parameter estimation. The MJGHD approach is illustrated on two real data sets, where the Bayesian information criterion is used for model selection.},
  day            = {9},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 04:04},
}

@Article{Taylor-2018,
  author         = {Taylor, Stephen Michael},
  date           = {2018},
  journaltitle   = {SSRN e-Print},
  title          = {Clustering financial return distributions using the fisher information metric},
  doi            = {10.2139/ssrn.3182914},
  issn           = {1556-5068},
  url            = {https://ssrn.com/abstract=3182914},
  abstract       = {Information Geometry provides a correspondence between differential geometry and statistics through the Fisher Information matrix. In particular, given two models from the same parametric family of distributions, one can define the distance between these models as the length of the shortest geodesic connecting them in a Riemannian manifold whose metric is given by the model Fisher Information matrix. One limitation that had hinder the adoption of this similarity measure in practical applications is that this distance is typically difficult to compute in a robust manner. We review such complications and provide a general form for the distance function for one parameter models. We next focus on two higher dimensional extreme value models including the Generalized Pareto and Generalized Extreme Value distributions that will be used in financial risk applications. Specifically, we first develop a technique to identify the nearest neighbors of a target security in the sense that their best fit model distributions have minimal Fisher distance to that of target. Second, we develop a hierarchical clustering technique that compares Generalized Extreme Value distributions fit to block maxima of a set of equity loss distributions to group together securities whose worst single day yearly loss distributions exhibit commonalities.},
  f1000-projects = {QuantInvest},
  groups         = {Invest_Network},
  timestamp      = {2020-02-27 04:04},
}

@Article{Teklehaymanot-et-al-2018,
  author         = {Teklehaymanot, Freweyni K. and Muma, Michael and Zoubir, Abdelhak M.},
  date           = {2018-11-29},
  journaltitle   = {arXiv e-Print},
  title          = {Robust Bayesian Cluster Enumeration},
  url            = {https://arxiv.org/abs/1811.12337},
  urldate        = {2019-04-25},
  abstract       = {A major challenge in cluster analysis is that the number of data clusters is mostly unknown and it must be estimated prior to clustering the observed data. In real-world applications, the observed data is often subject to heavy tailed noise and outliers which obscure the true underlying structure of the data. Consequently, estimating the number of clusters becomes challenging. To this end, we derive a robust cluster enumeration criterion by formulating the problem of estimating the number of clusters as maximization of the posterior probability of multivariate t candidate models. We utilize Bayes' theorem and asymptotic approximations to come up with a robust criterion that possesses a closed-form expression. Further, we refine the derivation and provide a robust cluster enumeration criterion for the finite sample regime. The robust criteria require an estimate of cluster parameters for each candidate model as an input. Hence, we propose a two-step cluster enumeration algorithm that uses the expectation maximization algorithm to partition the data and estimate cluster parameters prior to the calculation of one of the robust criteria. The performance of the proposed algorithm is tested and compared to existing cluster enumeration methods using numerical and real data experiments.},
  day            = {29},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 04:04},
}

@Article{Tellaroli-et-al-2016,
  author               = {Tellaroli, Paola and Bazzi, Marco and Donato, Michele and Brazzale, Alessandra R. and Draghici, Sorin},
  date                 = {2016-03},
  journaltitle         = {PLOS ONE},
  title                = {Cross-Clustering: A Partial Clustering Algorithm with Automatic Estimation of the Number of Clusters},
  doi                  = {10.1371/journal.pone.0152333},
  number               = {3},
  pages                = {e0152333+},
  volume               = {11},
  abstract             = {Four of the most common limitations of the many available clustering methods are: i) the lack of a proper strategy to deal with outliers; ii) the need for a good a priori estimate of the number of clusters to obtain reasonable results; iii) the lack of a method able to detect when partitioning of a specific data set is not appropriate; and iv) the dependence of the result on the initialization. Here we propose Cross-clustering (CC), a partial clustering algorithm that overcomes these four limitations by combining the principles of two well established hierarchical clustering algorithms: Ward's minimum variance and Complete-linkage. We validated CC by comparing it with a number of existing clustering methods, including Ward's and Complete-linkage. We show on both simulated and real datasets, that CC performs better than the other methods in terms of: the identification of the correct number of clusters, the identification of outliers, and the determination of real cluster memberships. We used CC to cluster samples in order to identify disease subtypes, and on gene profiles, in order to determine groups of genes with the same behavior. Results obtained on a non-biological dataset show that the method is general enough to be successfully used in such diverse applications. The algorithm has been implemented in the statistical language R and is freely available from the CRAN contributed packages repository.},
  citeulike-article-id = {14005861},
  citeulike-linkout-0  = {http://dx.doi.org/10.1371/journal.pone.0152333},
  day                  = {25},
  posted-at            = {2017-09-21 00:25:43},
  publisher            = {Public Library of Science},
  timestamp            = {2020-02-27 04:04},
}

@Book{Thrun-2018,
  author         = {Thrun, Michael Christoph},
  date           = {2018},
  title          = {Projection-Based Clustering through Self-Organization and Swarm Intelligence},
  doi            = {10.1007/978-3-658-20540-9},
  isbn           = {978-3-658-20539-3},
  location       = {Wiesbaden},
  publisher      = {Springer Fachmedien Wiesbaden},
  abstract       = {It covers aspects of unsupervised machine learning used for knowledge discovery in data science and introduces a data-driven approach to cluster analysis, the Databionic swarm(DBS). DBS consists of the 3D landscape visualization and clustering of data. The 3D landscape enables 3D printing of high-dimensional data structures.The clustering and number of clusters or an absence of cluster structure are verified by the 3D landscape at a glance. DBS is the first swarm-based technique that shows emergent properties while exploiting concepts of swarm intelligence, self-organization and the Nash equilibrium concept from game theory. It results in the elimination of a global objective function and the setting of parameters. By downloading the R package DBS can be applied to data drawn from diverse research fields and used even by non-professionals in the field of data mining.},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 04:04},
}

@Article{Tola-et-al-2008,
  author               = {Tola, Vincenzo and Lillo, Fabrizio and Gallegati, Mauro and Mantegna, Rosario N.},
  date                 = {2008-01},
  journaltitle         = {Journal of Economic Dynamics and Control},
  title                = {Cluster analysis for portfolio optimization},
  doi                  = {10.1016/j.jedc.2007.01.034},
  issn                 = {0165-1889},
  number               = {1},
  pages                = {235--258},
  volume               = {32},
  abstract             = {We consider the problem of the statistical uncertainty of the correlation matrix in the optimization of a financial portfolio. By assuming idealized conditions of perfect forecast ability for the future return and volatility of stocks and short selling, we show that the use of clustering algorithms can improve the reliability of the portfolio in terms of the ratio between predicted and realized risk. Bootstrap analysis indicates that this improvement is obtained in a wide range of the parameters N (number of assets) and T (investment horizon). The predicted and realized risk level and the relative portfolio composition of the selected portfolio for a given value of the portfolio return are also investigated for each considered filtering method. We also show that several of the results obtained by assuming idealized conditions are still observed under the more realistic assumptions of no short selling and mean return and volatility forecasting based on historical data.},
  citeulike-article-id = {14148037},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.jedc.2007.01.034},
  groups               = {Networks and investment management, Clustering and network analysis, Network_Invest, PortfOptim_Network, FrcstQWIM_ShortTerm, Vol_Cluster},
  owner                = {cristi},
  posted-at            = {2016-09-28 18:09:06},
  timestamp            = {2020-02-27 04:04},
}

@Article{Torrente-Brazma-2017,
  author               = {Torrente, Aurora and Brazma, Alvis},
  date                 = {2017-08},
  journaltitle         = {Bioinformatics},
  title                = {clustComp, a bioconductor package for the comparison of clustering results},
  doi                  = {10.1093/bioinformatics/btx532},
  issn                 = {1367-4803},
  abstract             = {clustComp is an open source Bioconductor package that implements different techniques for the comparison of two gene expression clustering results. These include flat versus flat and hierarchical versus flat comparisons. The visualization of the similarities is provided by means of a bipartite graph, whose layout is heuristically optimized. Its flexibility allows a suitable visualization for both small and large datasets.},
  citeulike-article-id = {14468380},
  citeulike-linkout-0  = {http://dx.doi.org/10.1093/bioinformatics/btx532},
  day                  = {23},
  posted-at            = {2017-10-28 18:52:23},
  timestamp            = {2020-02-27 04:04},
}

@Article{Turkmen-2015,
  author               = {Turkmen, Ali C.},
  date                 = {2015-08},
  journaltitle         = {arXiv e-Print},
  title                = {A Review of Nonnegative Matrix Factorization Methods for Clustering},
  eprint               = {1507.03194},
  eprinttype           = {arXiv},
  url                  = {https://arxiv.org/abs/1507.03194},
  abstract             = {Nonnegative Matrix Factorization (NMF) was first introduced as a low-rank matrix approximation technique, and has enjoyed a wide area of applications. Although NMF does not seem related to the clustering problem at first, it was shown that they are closely linked. In this report, we provide a gentle introduction to clustering and NMF before reviewing the theoretical relationship between them. We then explore several NMF variants, namely Sparse NMF, Projective NMF, Nonnegative Spectral Clustering and Cluster-NMF, along with their clustering interpretations.},
  citeulike-article-id = {13671514},
  citeulike-linkout-0  = {http://arxiv.org/abs/1507.03194},
  citeulike-linkout-1  = {http://arxiv.org/pdf/1507.03194},
  day                  = {28},
  owner                = {cristi},
  posted-at            = {2016-03-24 14:49:31},
  timestamp            = {2020-02-27 04:04},
}

@InCollection{Vaquez-et-al-2019,
  author         = {Vaquez, Iago and Villar, Jose R. and Sedano, Javier and Simic, Svetlana},
  booktitle      = {14th international conference on soft computing models in industrial and environmental applications (SOCO 2019)},
  date           = {2019},
  title          = {A preliminary study on multivariate time series clustering},
  doi            = {10.1007/978-3-030-20055-8\_45},
  editor         = {Martinez Alvarez, Francisco and Troncoso Lora, Alicia and Saez Munoz, Jose Antonio and Quintian, Hector and Corchado, Emilio},
  isbn           = {978-3-030-20054-1},
  pages          = {473--480},
  publisher      = {Springer International Publishing},
  series         = {Advances in intelligent systems and computing},
  url            = {http://link.springer.com/10.1007/978-3-030-20055-8\_45},
  urldate        = {2019-10-05},
  volume         = {950},
  abstract       = {Time Series (TS) clustering is one of the most effervescent research fields due to the Big Data and the IoT explosion. The problem gets more challenging if we consider the multivariate TS. In the field of Business and Management, multivariate TS are becoming more and more interesting as they allow to match events the co-occur in time but that is hardly noticeable. In this study, Recurrent Neural Networks and transfer learning have been used to analyze each example, measuring similarities between variables. All the results are finally aggregated to create an adjacency matrix that allows extracting the groups. Proof-of-concept experimentation has been included, showing that the solution might be valid after several improvements.},
  f1000-projects = {QuantInvest},
  groups         = {ML_ClustTimeSrs},
  issn           = {2194-5357},
  timestamp      = {2020-02-27 04:04},
}

@InCollection{Veenstra-et-al-2016a,
  author               = {Veenstra, Patrick and Cooper, Colin and Phelps, Steve},
  booktitle            = {8th Computer Science and Electronic Engineering (CEEC)},
  date                 = {2016-09},
  title                = {Spectral clustering using the kNN-MST similarity graph},
  doi                  = {10.1109/ceec.2016.7835917},
  isbn                 = {978-1-5090-2050-8},
  location             = {Colchester, United Kingdom},
  pages                = {222--227},
  publisher            = {IEEE},
  abstract             = {Spectral clustering is a technique that uses the spectrum of a similarity graph to cluster data. Part of this procedure involves calculating the similarity between data points and creating a similarity graph from the resulting similarity matrix. This is ordinarily achieved by creating a k-nearest neighbour (kNN) graph. In this paper, we show the benefits of using a different similarity graph, namely the union of the kNN graph and the minimum spanning tree of the negated similarity matrix (kNN-MST). We show that this has some distinct advantages on both synthetic and real datasets. Specifically, the clustering accuracy of kNN-MST is less dependent on the choice of k than kNN is.},
  citeulike-article-id = {14444472},
  citeulike-linkout-0  = {http://dx.doi.org/10.1109/ceec.2016.7835917},
  posted-at            = {2017-10-03 08:33:03},
  timestamp            = {2020-02-27 04:04},
}

@InCollection{Veldt-et-al-2017,
  author               = {Veldt, Nate and Wirth, Anthony I. and Gleich, David F.},
  booktitle            = {Proceedings of the 26th International Conference on World Wide Web},
  date                 = {2017},
  title                = {Correlation Clustering with Low-Rank Matrices},
  doi                  = {10.1145/3038912.3052586},
  isbn                 = {978-1-4503-4913-0},
  location             = {Perth, Australia},
  pages                = {1025--1034},
  publisher            = {International World Wide Web Conferences Steering Committee},
  series               = {WWW '17},
  abstract             = {Correlation clustering is a technique for aggregating data based on qualitative information about which pairs of objects are labeled `similar' or `dissimilar.' Because the optimization problem is NP-hard, much of the previous literature focuses on finding approximation algorithms. In this paper we explore how to solve the correlation clustering objective exactly when the data to be clustered can be represented by a low-rank matrix. We prove in particular that correlation clustering can be solved in polynomial time when the underlying matrix is positive semidefinite with small constant rank, but that the task remains NP-hard in the presence of even one negative eigenvalue. Based on our theoretical results, we develop an algorithm for efficiently ``solving'' low-rank positive semidefinite correlation clustering by employing a procedure for zonotope vertex enumeration. We demonstrate the effectiveness and speed of our algorithm by using it to solve several clustering problems on both synthetic and real-world data.},
  address              = {Republic and Canton of Geneva, Switzerland},
  citeulike-article-id = {14398863},
  citeulike-linkout-0  = {http://portal.acm.org/citation.cfm?id=3038912.3052586},
  citeulike-linkout-1  = {http://dx.doi.org/10.1145/3038912.3052586},
  groups               = {Clustering and network analysis},
  posted-at            = {2017-07-24 17:07:45},
  timestamp            = {2020-02-27 04:04},
}

@Article{Verma-et-al-2018,
  author         = {Verma, A. and Buonocore, R. J. and Di Matteo, T.},
  date           = {2018-11-14},
  journaltitle   = {Quantitative Finance},
  title          = {A cluster driven log-volatility factor model: a deepening on the source of the volatility clustering},
  doi            = {10.1080/14697688.2018.1535183},
  issn           = {1469-7688},
  pages          = {1--16},
  abstract       = {We introduce a new factor model for log volatilities that considers contributions, and performs dimensionality reduction, at a global level through the market, and at a local level through clusters and their interactions. We do not assume a-priori the number of clusters in the data, instead using the Directed Bubble Hierarchical Tree algorithm to fix the number of factors. We use the factor model to study how the log volatility contributes to volatility clustering, quantifying the strength of the volatility clustering using a new nonparametric integrated proxy. Indeed finding a link between volatility and volatility clustering, we find that a global analysis reveals that only the market contributes to the volatility clustering. A local analysis reveals that for some clusters, the cluster itself contributes statistically to the volatility clustering effect. This is significantly advantageous over other factor models, since it offers a way of selecting factors in a statistical way, whilst also keeping economically relevant factors. Finally, we show that the log volatility factor model explains a similar amount of memory to a principal components analysis factor model and an exploratory factor model.},
  day            = {14},
  f1000-projects = {QuantInvest},
  groups         = {Vol_Cluster},
  timestamp      = {2020-02-27 04:04},
}

@Article{Vinod-Viole-2017,
  author               = {Vinod, Hrishikesh D. and Viole, Fred},
  date                 = {2017},
  journaltitle         = {Computational Economics},
  title                = {Nonparametric Regression Using Clusters},
  doi                  = {10.1007/s10614-017-9713-5},
  pages                = {1--18},
  abstract             = {We present a fundamentally unique method of nonparametric regression using clusters and test it against classically established methods. We compare two nonlinear regression estimation packages called 'NNS', Viole (NNS: nonlinear nonparametric statistics, 2016), and 'np', Hayfield and Racine (J Stat Softw 27(5):1-32, 2008), with the help of a simulation using deterministic (DT) and stochastic (ST) regressor models. We find the respective coefficients of determination (R2)(R2) are close for DT models, while finding an advantage to NNS in ST and large sample cases. Regression coefficients are sometimes regarded as approximations to partial derivatives, especially in social sciences. Then, NNS alone has the ability to compute a range of partials evaluated at points within the sample and also out-of-sample. Thus NNS can provide a viable alternative to kernel based nonparametric regressions without using bandwidths for smoothing.},
  citeulike-article-id = {14398901},
  citeulike-linkout-0  = {http://dx.doi.org/10.1007/s10614-017-9713-5},
  citeulike-linkout-1  = {http://link.springer.com/article/10.1007/s10614-017-9713-5},
  groups               = {Clustering and network analysis, Regression_Nonlinear},
  posted-at            = {2017-07-24 20:27:05},
  publisher            = {Springer US},
  timestamp            = {2020-02-27 04:04},
}

@Article{Viole-2017a,
  author               = {Viole, Fred},
  date                 = {2017},
  journaltitle         = {SSRN e-Print},
  title                = {Classification Using NNS Clustering Analysis},
  url                  = {https://ssrn.com/abstract=2864711},
  abstract             = {NNS stands for Nonlinear Nonparametric Statistics, henceforth "NNS". What is NNS clustering analysis? NNS clustering is a method of partitioning the joint distribution into partial moment quadrants (clustering), and assigning identifiers to observations (classification). NNS clustering is very similar to k-means clustering, and we direct the reader to Vinod and Viole [2016] for a proof and comparison between the methods. This article is intended to present working examples of several classification problems using NNS clustering analysis.

We demonstrate how NNS clustering is quite effective, as well as an alternative method NNS employs for classification tasks. We compare predictions of test sets with NNS, k-means using the "cl.predict" routine offered in R to "predict class ids or memberships from R objects representing partitions", K nearest neighbors classification using the "knn" routine in R-package "class", and a naive Bayes classification using the "e1071" package.

The methods and results presented immediately raise suspicions on the pervasive notion of dimension reduction given the consistent performance of the NNS Multivariate Regression.},
  citeulike-article-id = {14398900},
  citeulike-linkout-0  = {https://papers.ssrn.com/sol3/papers.cfm?abstractid=2864711},
  groups               = {Clustering and network analysis},
  posted-at            = {2017-07-24 20:26:00},
  timestamp            = {2020-02-27 04:04},
}

@Article{Wang-et-al-2013,
  author               = {Wang, Yongning and Tsay, Ruey S. and Ledolter, Johannes and Shrestha, Keshab M.},
  date                 = {2013-12},
  journaltitle         = {Journal of Forecasting},
  title                = {Forecasting Simultaneously High-Dimensional Time Series: A Robust Model-Based Clustering Approach},
  doi                  = {10.1002/for.2264},
  number               = {8},
  pages                = {673--684},
  volume               = {32},
  abstract             = {This paper considers the problem of forecasting high-dimensional time series. It employs a robust clustering approach to perform classification of the component series. Each series within a cluster is assumed to follow the same model and the data are then pooled for estimation. The classification is model-based and robust to outlier contamination. The robustness is achieved by using the intrinsic mode functions of the Hilbert-Huang transform at lower frequencies.

These functions are found to be robust to outlier contamination. The paper also compares out-of-sample forecast performance of the proposed method with several methods available in the literature. The other forecasting methods considered include vector autoregressive models with or without LASSO, group LASSO, principal component regression, and partial least squares.

The proposed method is found to perform well in out-of-sample forecasting of the monthly unemployment rates of 50 US states.},
  citeulike-article-id = {12519623},
  citeulike-linkout-0  = {http://dx.doi.org/10.1002/for.2264},
  day                  = {1},
  groups               = {FrcstQWIM_TimeSrs},
  owner                = {cristi},
  posted-at            = {2016-02-18 07:10:43},
  timestamp            = {2020-02-27 04:04},
}

@Article{Wang-et-al-2018a,
  author         = {Wang, Min and Abrams, Zachary B and Kornblau, Steven M and Coombes, Kevin R},
  date           = {2018-01-08},
  journaltitle   = {BMC Bioinformatics},
  title          = {Thresher: determining the number of clusters while removing outliers.},
  doi            = {10.1186/s12859-017-1998-9},
  number         = {1},
  pages          = {9},
  volume         = {19},
  abstract       = {BACKGROUND: Cluster analysis is the most common unsupervised method for finding hidden groups in data. Clustering presents two main challenges: (1) finding the optimal number of clusters, and (2) removing "outliers" among the objects being clustered. Few clustering algorithms currently deal directly with the outlier problem. Furthermore, existing methods for identifying the number of clusters still have some drawbacks. Thus, there is a need for a better algorithm to tackle both challenges. RESULTS: We present a new approach, implemented in an R package called Thresher, to cluster objects in general datasets. Thresher combines ideas from principal component analysis, outlier filtering, and von Mises-Fisher mixture models in order to select the optimal number of clusters. We performed a large Monte Carlo simulation study to compare Thresher with other methods for detecting outliers and determining the number of clusters. We found that Thresher had good sensitivity and specificity for detecting and removing outliers. We also found that Thresher is the best method for estimating the optimal number of clusters when the number of objects being clustered is smaller than the number of variables used for clustering. Finally, we applied Thresher and eleven other methods to 25 sets of breast cancer data downloaded from the Gene Expression Omnibus; only Thresher consistently estimated the number of clusters to lie in the range of 4-7 that is consistent with the literature. CONCLUSIONS: Thresher is effective at automatically detecting and removing outliers. By thus cleaning the data, it produces better estimates of the optimal number of clusters when there are more variables than objects. When we applied Thresher to a variety of breast cancer datasets, it produced estimates that were both self-consistent and consistent with the literature. We expect Thresher to be useful for studying a wide variety of biological datasets.},
  day            = {8},
  f1000-projects = {QuantInvest},
  pmcid          = {PMC5759208},
  pmid           = {29310570},
  timestamp      = {2020-02-27 04:04},
}

@Article{Wenskovitch-et-al-2018,
  author         = {Wenskovitch, John and Crandell, Ian and Ramakrishnan, Naren and House, Leanna and Leman, Scotland and North, Chris},
  date           = {2018-01},
  journaltitle   = {IEEE Transactions on Visualization and Computer Graphics},
  title          = {Towards a systematic combination of dimension reduction and clustering in visual analytics.},
  doi            = {10.1109/{TVCG}.2017.2745258},
  number         = {1},
  pages          = {131--141},
  url            = {http://dx.doi.org/10.1109/{TVCG}.2017.2745258},
  volume         = {24},
  abstract       = {Dimension reduction algorithms and clustering algorithms are both frequently used techniques in visual analytics. Both families of algorithms assist analysts in performing related tasks regarding the similarity of observations and finding groups in datasets. Though initially used independently, recent works have incorporated algorithms from each family into the same visualization systems. However, these algorithmic combinations are often ad hoc or disconnected, working independently and in parallel rather than integrating some degree of interdependence. A number of design decisions must be addressed when employing dimension reduction and clustering algorithms concurrently in a visualization system, including the selection of each algorithm, the order in which they are processed, and how to present and interact with the resulting projection. This paper contributes an overview of combining dimension reduction and clustering into a visualization system, discussing the challenges inherent in developing a visualization system that makes use of both families of algorithms.},
  f1000-projects = {QuantInvest},
  groups         = {Dimens_Reduc},
  pmid           = {28866581},
  timestamp      = {2020-02-27 04:04},
}

@Article{Weylandt-et-al-2019,
  author         = {Weylandt, Michael and Nagorski, John and Allen, Genevera I.},
  date           = {2019-01-06},
  journaltitle   = {arXiv e-Print},
  title          = {Dynamic Visualization and Fast Computation for Convex Clustering via Algorithmic Regularization},
  url            = {https://arxiv.org/abs/1901.01477},
  urldate        = {2019-05-04},
  abstract       = {Convex clustering is a promising new approach to the classical problem of clustering, combining strong performance in empirical studies with rigorous theoretical foundations. Despite these advantages, convex clustering has not been widely adopted, due to its computationally intensive nature and its lack of compelling visualizations. To address these impediments, we introduce Algorithmic Regularization, an innovative technique for obtaining high-quality estimates of regularization paths using an iterative one-step approximation scheme. We justify our approach with a novel theoretical result, guaranteeing global convergence of the approximate path to the exact solution under easily-checked non-data-dependent assumptions. The application of algorithmic regularization to convex clustering yields the Convex Clustering via Algorithmic Regularization Paths (CARP) algorithm for computing the clustering solution path. On example data sets from genomics and text analysis, CARP delivers over a 100-fold speed-up over existing methods, while attaining a finer approximation grid than standard methods. Furthermore, CARP enables improved visualization of clustering solutions: the fine solution grid returned by CARP can be used to construct a convex clustering-based dendrogram, as well as forming the basis of a dynamic path-wise visualization based on modern web technologies. Our methods are implemented in the open-source R package clustRviz, available at this https URL.},
  day            = {6},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 04:04},
}

@Article{Whiteley-2019,
  author         = {Whiteley, Nick},
  date           = {2019-06-25},
  journaltitle   = {arXiv e-Print},
  title          = {Dynamic time series clustering via volatility change-points},
  url            = {https://arxiv.org/abs/1906.10372},
  urldate        = {2019-08-20},
  abstract       = {This note outlines a method for clustering time series based on a statistical model in which volatility shifts at unobserved change-points. The model accommodates some classical stylized features of returns and its relation to GARCH is discussed. Clustering is performed using a probability metric evaluated between posterior distributions of the most recent change-point associated with each series. This implies series are grouped together at a given time if there is evidence the most recent shifts in their respective volatilities were coincident or closely timed. The clustering method is dynamic, in that groupings may be updated in an online manner as data arrive. Numerical results are given analyzing daily returns of constituents of the S\&P 500.},
  day            = {25},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 04:04},
}

@Article{Wong-2013,
  author               = {Wong, Man H.},
  date                 = {2013-06},
  journaltitle         = {European Journal of Operational Research},
  title                = {Investment models based on clustered scenario trees},
  doi                  = {10.1016/j.ejor.2012.11.051},
  issn                 = {0377-2217},
  number               = {2},
  pages                = {314--324},
  volume               = {227},
  abstract             = {Stochastic programming is widely applied in financial decision problems. In particular, when we need to carry out the actual calculations for portfolio selection problems, we have to assign a value for each expected return and the associated conditional probability in advance. These estimated random parameters often rely on a scenario tree representing the distribution of the underlying asset returns. One of the drawbacks is that the estimated parameters may be deviated from the actual ones. Therefore, robustness is considered so as to cope with the issue of parameter inaccuracy. In view of this, we propose a clustered scenario-tree approach, which accommodates the parameter inaccuracy problem in the context of a scenario tree. Proposed a new kind of scenario tree, called ? cluster tree ?. It accommodates the parameter inaccuracy in the context of a scenario tree. The idea is illustrated with portfolio selection problems. Three risk measures are considered: probability, downside risk and CVaR. OR techniques include fractional programming, interior point methods and SOCP.},
  citeulike-article-id = {13989083},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.ejor.2012.11.051},
  groups               = {Network_Invest, Scenario_Portfolio},
  owner                = {cristi},
  posted-at            = {2016-03-27 18:42:57},
  timestamp            = {2020-02-27 04:04},
}

@Article{Xia-et-al-2018,
  author         = {Xia, Jiazhi and Gao, Le and Kong, Kezhi and Zhao, Ying and Chen, Yi and Kui, Xiaoyan and Liang, Yixiong},
  date           = {2018-10},
  journaltitle   = {Journal of Visual Languages \& Computing},
  title          = {Exploring linear projections for revealing clusters, outliers, and trends in subsets of multi-dimensional datasets},
  doi            = {10.1016/j.jvlc.2018.08.003},
  issn           = {1045-926X},
  pages          = {52--60},
  url            = {https://linkinghub.elsevier.com/retrieve/pii/{S1045926X18301289}},
  volume         = {48},
  abstract       = {Identifying patterns in 2D linear projections is important in understanding multi-dimensional datasets. However, local patterns, which are composed of partial data points, are usually obscured by noises and missed in traditional quality measure approaches that measure the whole dataset. In this paper, we propose an interactive interface to explore 2D linear projections with visual patterns on subsets. First, we propose a voting-based algorithm to recommend optimal projection, in which the identified pattern looks the most salient. Specifically, we propose three kinds of point-wise quality metrics of 2D linear projections for outliers, clusterings, and trends, respectively. For each sampled projection, we measure its importance by accumulating the metrics of selected points. The projection with the highest importance is recommended. Second, we design an exploring interface with a scatterplot, a projection trail map, and a control panel. Our interface allows users to explore projections by specifying interested data subsets. At last, we employ three datasets and demonstrate the effectiveness of our approach through three case studies of exploring clusters, outliers, and trends.},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 04:04},
}

@Article{Xiong-et-al-2017,
  author               = {Xiong, Caiming and Johnson, David M. and Corso, Jason J.},
  date                 = {2017-01},
  journaltitle         = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  title                = {Active Clustering with Model-Based Uncertainty Reduction},
  doi                  = {10.1109/tpami.2016.2539965},
  issn                 = {0162-8828},
  number               = {1},
  pages                = {5--17},
  volume               = {39},
  abstract             = {Semi-supervised clustering seeks to augment traditional clustering methods by incorporating side information provided via human expertise in order to increase the semantic meaningfulness of the resulting clusters. However, most current methods are passive in the sense that the side information is provided beforehand and selected randomly. This may require a large number of constraints, some of which could be redundant, unnecessary, or even detrimental to the clustering results. Thus in order to scale such semi-supervised algorithms to larger problems it is desirable to pursue an active clustering method, i.e., an algorithm that maximizes the effectiveness of the available human labor by only requesting human input where it will have the greatest impact. Here, we propose a novel online framework for active semi-supervised spectral clustering that selects pairwise constraints as clustering proceeds, based on the principle of uncertainty reduction. Using a first-order Taylor expansion, we decompose the expected uncertainty reduction problem into a gradient and a step-scale, computed via an application of matrix perturbation theory and cluster-assignment entropy, respectively. The resulting model is used to estimate the uncertainty reduction potential of each sample in the dataset. We then present the human user with pairwise queries with respect to only the best candidate sample. We evaluate our method using three different image datasets (faces, leaves and dogs), a set of common UCI machine learning datasets and a gene dataset. The results validate our decomposition formulation and show that our method is consistently superior to existing state-of-the-art techniques, as well as being robust to noise and to unknown numbers of clusters.},
  citeulike-article-id = {14334833},
  citeulike-linkout-0  = {http://dx.doi.org/10.1109/tpami.2016.2539965},
  day                  = {1},
  posted-at            = {2017-04-10 01:56:02},
  timestamp            = {2020-02-27 04:04},
}

@InCollection{Xue-et-al-2018,
  author         = {Xue, Jingming and Zhu, En and Liu, Qiang and Wang, Chuanli and Yin, Jianping},
  booktitle      = {Cloud Computing and Security: 4th International Conference, ICCCS 2018},
  date           = {2018},
  title          = {A Joint Approach to Data Clustering and Robo-Advisor},
  doi            = {10.1007/978-3-030-00006-6\_9},
  editor         = {Sun, Xingming and Pan, Zhaoqing and Bertino, Elisa},
  isbn           = {978-3-030-00005-9},
  pages          = {97--109},
  publisher      = {Springer International Publishing},
  series         = {Lecture notes in computer science},
  url            = {http://link.springer.com/10.1007/978-3-030-00006-6\_9},
  urldate        = {2019-10-12},
  volume         = {11063},
  abstract       = {Robo-advisor is a type of financial recommendation that can provide investors with financial advice or investment management online. Data clustering and item recommendation are both important and challenging in Robo-advisor. These two tasks are often considered independently and most efforts have been made to tackle them separately. However, users in data clustering and group relationship in item recommendation are inherently related. For example, a large number of financial transactions include not only the user asset information, but also the user social information. The existence of relations between users and groups motivates us to jointly perform clustering and item recommendation for Robo-advisor in this paper. In particular, we provide a principle way to capture the relations between users and groups, and propose a novel framework CLURE, which fuses data CLUstering and item REcommendation into a coherent model. With experiments on benchmark and real-world datasets, we demonstrate that the proposed framework CLURE achieves superior performance on both tasks compared to the state-of-the-art methods.},
  f1000-projects = {QuantInvest},
  issn           = {0302-9743},
  timestamp      = {2020-02-27 04:04},
}

@Article{Xu-Lange-2019,
  author         = {Xu, Jason and Lange, Kenneth},
  date           = {2019-05-24},
  journaltitle   = {Proceedings of Machine Learning Research},
  title          = {Power k-Means Clustering},
  url            = {http://proceedings.mlr.press/v97/xu19a.html},
  urldate        = {2019-09-15},
  abstract       = {Clustering is a fundamental task in unsupervised machine learning. Lloyd 1957 algorithm for k-means clustering remains one of the most widely used due to its speed and simplicity, but the greedy approach is sensitive to initialization and often falls short at a poor solution. This paper explores an alternative to Lloyd algorithm that retains its simplicity and mitigates its tendency to get trapped by local minima. Called power k-means, our method embeds the k-means problem in a continuous class of similar, better behaved problems with fewer local minima. Power k-means anneals its way toward the solution of ordinary k-means by way of majorization-minimization (MM), sharing the appealing descent property and low complexity of Lloyd algorithm. Further, our method complements widely used seeding strategies, reaping marked improvements when used together as demonstrated on a suite of simulated and real data examples.},
  day            = {24},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 04:04},
}

@InCollection{Yang-et-al-2016a,
  author    = {Hoseong Yang and Hye Jin Lee and Eugene Cho and Sungzoon Cho},
  booktitle = {IEEE International Conference on Big Data},
  date      = {2016},
  title     = {Automatic classification of securities using hierarchical clustering of the 10-Ks},
  url       = {https://ieeexplore.ieee.org/abstract/document/7841069},
  abstract  = {Industry classification has been rigorously utilized in academic research and business analytics. The existing classification schemes, however, have been constructed and maintained manually by domain experts, which require exhaustive time and human effort while vulnerable to subjectivity. Hence, the existing classification systems do not properly reflect the fast-changing trends of the firms and the capital market. As a remedy to such shortcomings, this paper proposes a new classification scheme, Business Text Industry Classification (BTIC), namely, that automatically clusters securities based on the textual information from the corporate disclosures. BTIC exploits the business section of the Form 10-Ks, in which firms provide their self-identities in a rich context. We employ doc2vec for document embedding and apply Ward's hierarchical clustering method to categorize securities into BTIC groups. Evaluation results using 12 financial ratios commonly found in financial research show that BTIC performs just as good as SIC and GICS in terms of inter- and intra-industry homogeneity, especially for the higher level of clustering. Given that, we claim that BTIC outperforms SIC and GICS in four aspects: process automation, objectivity, clustering flexibility, and result interpretability.},
  timestamp = {2020-02-27 04:04},
}

@InCollection{You-et-al-2016a,
  author               = {You, Shi Y. and Dan Wang, Yu and Luo, Lin K. and Peng, Hong},
  booktitle            = {11th International Conference on Computer Science and Education (ICCSE)},
  date                 = {2016-08},
  title                = {Finding the clusters with potential value in financial time series based on agglomerative hierarchical clustering},
  doi                  = {10.1109/iccse.2016.7581558},
  isbn                 = {978-1-5090-2218-2},
  location             = {Nagoya, Japan},
  pages                = {77--81},
  publisher            = {IEEE},
  abstract             = {It is interesting to find the clustering with potential value in financial time series. In this paper, we focus on this topic. The owned features of the clusters with potential value are provided firstly. Then, the agglomerative hierarchical clustering (AHC) is used to find those clusters automatically. There are two innovations in this paper. The first one is that the features of the clusters with potential value are embedded into the process of AHC, which reduces the time cost of clustering process. The second one is that we propose two indicators, whole similarity and trend similarity, to measure the persistence of the cluster. The experiment on ten time segments shows the obtained clusters is effective, in which both the whole similarity and the trend similarity on training data are markedly higher than that of randomized clustering. In addition, the persistence of these clusters on test data is also better that the result of randomized guess. We think that the strategy provided in this paper is helpful to find for the clustering with potential value.},
  citeulike-article-id = {14320267},
  citeulike-linkout-0  = {http://dx.doi.org/10.1109/iccse.2016.7581558},
  posted-at            = {2017-03-26 18:26:06},
  timestamp            = {2020-02-27 04:04},
}

@Article{Yuan-2005,
  author               = {Yuan, Baosheng},
  date                 = {2006-12},
  journaltitle         = {SSRN e-Print},
  title                = {Scaling, Clustering and Dynamics of Volatility in Financial Time Series},
  url                  = {https://ssrn.com/abstract=950960},
  abstract             = {This thesis investigates volatility clustering, scaling and dynamics in financial series of asset returns and studies the underlying mechanism. We propose a direct measure of volatility clustering based on the conditional probability distribution (CPD) of the returns given the return in the previous time interval. We found that the CPDs of returns in real financial time series exhibits universal scaling, characterized by a collapse of the CPDs (of different time lags and of different returns in the previous interval) into to a universal curve exhibiting a power-law tail with an exponent of 4. We construct a simple phenomenological model to explain the emergence of VC and the associated volatility scaling. We also study agent-based models of financial markets, and explore the impact of dynamical risk aversion (DRA) of heterogeneous agents on the price fluctuations. We found that the DRA is the primary driving force responsible for excess price fluctuations and the associated volatility clustering. Both our models (phenomenological model and agent-based model) are able to generate time series that reproduces stylized facts of the market data on different time scales. We have also studied general herding behavior often exhibited in financial markets in the context of an evolutionary Minority Game. We discovered a general mechanism for the transition from segregation into opposing groups to clustering towards cautious behavior.},
  citeulike-article-id = {13988076},
  citeulike-linkout-0  = {http://ssrn.com/abstract=950960},
  citeulike-linkout-1  = {http://papers.ssrn.com/sol3/Delivery.cfm/SSRNID950960code539232.pdf?abstractid=950960 and mirid=1},
  day                  = {22},
  groups               = {Vol_Cluster},
  owner                = {cristi},
  posted-at            = {2016-03-25 16:56:07},
  timestamp            = {2020-02-27 04:04},
}

@Article{Yu-et-al-2018a,
  author         = {Yu, Han and Chapman, Brian and Di Florio, Arianna and Eischen, Ellen and Gotz, David and Jacob, Mathews and Blair, Rachael Hageman},
  date           = {2018-08-28},
  journaltitle   = {Computational statistics},
  title          = {Bootstrapping estimates of stability for clusters, observations and model selection},
  doi            = {10.1007/s00180-018-0830-y},
  issn           = {0943-4062},
  number         = {1},
  pages          = {349--372},
  urldate        = {2019-04-27},
  volume         = {34},
  abstract       = {Clustering is a challenging problem in unsupervised learning. In lieu of a gold standard, stability has become a valuable surrogate to performance and robustness. In this work, we propose a non-parametric bootstrapping approach to estimating the stability of a clustering method, which also captures stability of the individual clusters and observations. This flexible framework enables different types of comparisons between clusterings and can be used in connection with two possible bootstrap approaches for stability. The first approach, scheme 1, can be used to assess confidence (stability) around clustering from the original dataset based on bootstrap replications. A second approach, scheme 2, searches over the bootstrap clusterings for an optimally stable partitioning of the data. The two schemes accommodate different model assumptions that can be motivated by an investigator trust (or lack thereof) in the original data and additional computational considerations. We propose a hierarchical visualization extrapolated from the stability profiles that give insights into the separation of groups, and projected visualizations for the inspection of the stability of individual operations. Our approaches show good performance in simulation and on real data. These approaches can be implemented using the R package bootcluster that is available on the Comprehensive R Archive Network (CRAN).},
  day            = {28},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 04:04},
}

@Article{Yu-et-al-2105b,
  author               = {Yu, Meichen and Hillebrand, Arjan and Tewarie, Prejaas and Meier, Jil and van Dijk, Bob and Van Mieghem, Piet and Stam, Cornelis Jan J.},
  date                 = {2015-02},
  journaltitle         = {Chaos},
  title                = {Hierarchical clustering in minimum spanning trees.},
  issn                 = {1089-7682},
  number               = {2},
  url                  = {http://view.ncbi.nlm.nih.gov/pubmed/25725643},
  volume               = {25},
  abstract             = {The identification of clusters or communities in complex networks is a reappearing problem. The minimum spanning tree (MST), the tree connecting all nodes with minimum total weight, is regarded as an important transport backbone of the original weighted graph. We hypothesize that the clustering of the MST reveals insight in the hierarchical structure of weighted graphs. However, existing theories and algorithms have difficulties to define and identify clusters in trees. Here, we first define clustering in trees and then propose a tree agglomerative hierarchical clustering (TAHC) method for the detection of clusters in MSTs. We then demonstrate that the TAHC method can detect clusters in artificial trees, and also in MSTs of weighted social networks, for which the clusters are in agreement with the previously reported clusters of the original weighted networks. Our results therefore not only indicate that clusters can be found in MSTs, but also that the MSTs contain information about the underlying clusters of the original weighted network.},
  citeulike-article-id = {14150073},
  citeulike-linkout-0  = {http://view.ncbi.nlm.nih.gov/pubmed/25725643},
  citeulike-linkout-1  = {http://www.hubmed.org/display.cgi?uids=25725643},
  groups               = {Networks and investment management, Clustering and network analysis},
  owner                = {cristi},
  pmid                 = {25725643},
  posted-at            = {2016-10-01 20:43:49},
  timestamp            = {2020-02-27 04:04},
}

@Article{Zambom-et-al-2019,
  author         = {Zambom, Adriano Zanin and Collazos, Julian A. and Dias, Ronaldo},
  date           = {2019-05-02},
  journaltitle   = {arXiv e-Print},
  title          = {Selection of the Number of Clusters in Functional Data Analysis},
  url            = {https://arxiv.org/abs/1905.00977},
  urldate        = {2019-08-10},
  abstract       = {Identifying the number KK of clusters in a dataset is one of the most difficult problems in clustering analysis. A choice of KK that correctly characterizes the features of the data is essential for building meaningful clusters. In this paper we tackle the problem of estimating the number of clusters in functional data analysis by introducing a new measure that can be used with different procedures in selecting the optimal KK. The main idea is to use a combination of two test statistics, which measure the lack of parallelism and the mean distance between curves, to compute criteria such as the within and between cluster sum of squares. Simulations in challenging scenarios suggest that procedures using this measure can detect the correct number of clusters more frequently than existing methods in the literature. The application of the proposed method is illustrated on several real datasets.},
  day            = {2},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 04:04},
}

@TechReport{Zhang-Maringer-2010,
  author      = {Jin Zhang and Dietmar Maringer},
  date        = {2010},
  institution = {COMISEF Computational Optimization Methods in Statistics, Econometrics and Finance},
  title       = {Asset Allocation under Hierarchical Clustering},
  url         = {https://ideas.repec.org/p/com/wpaper/036.html},
  abstract    = {This paper proposes a clustering asset allocation scheme which provides better risk-adjusted portfolio performance than those obtained from tradi- tional asset allocation approaches such as the equal weight strategy and the Markowitz minimum variance allocation. The clustering criterion used, which involves maximization of the in-sample Sharpe ratio (SR), is different from traditional clustering criteria reported in the literature. Two evolu- tionary methods, namely Differential Evolution and Genetic Algorithm, are employed to search for such an optimal clustering structure given a clus- ter number. To explore the clustering impact on the SR, the in-sample and the out-of-sample SR distributions of the portfolios are studied using bootstrapped data as well as simulated paths from the single index market model. It was found that the SR distributions of the portfolios under the clustering asset allocation structure have higher mean values and skewness but approximately the same standard deviation and kurtosis than those in the non-clustered case. Genetic Algorithm is suggested as a more efficient approach than Differential Evolution for the purpose of solving the cluster-ing problem.},
  groups      = {Networks and investment management, Clustering and network analysis, Network_Invest, PortfOptim_Network},
  timestamp   = {2020-02-27 04:04},
}

@Article{Zhang-Maringer-2011,
  author               = {Zhang, Jin and Maringer, Dietmar},
  date                 = {2011-11},
  journaltitle         = {Expert Systems with Applications},
  title                = {Distributing weights under hierarchical clustering: A way in reducing performance breakdown},
  doi                  = {10.1016/j.eswa.2011.05.052},
  issn                 = {0957-4174},
  number               = {12},
  pages                = {14952--14959},
  volume               = {38},
  abstract             = {This paper proposes a clustering asset allocation scheme which provides better risk-adjusted portfolio performance than those obtained from traditional asset allocation approaches such as the equal weight strategy and the Markowitz minimum variance allocation. The clustering criterion used, which involves maximization of the in-sample Sharpe ratio (SR), is different from traditional clustering criteria reported in the literature. Two evolutionary methods, namely Differential Evolution and Genetic Algorithm, are employed to search for such an optimal clustering structure given a cluster number. To explore the clustering impact on the SR, the in-sample and the out-of-sample SR distributions of the portfolios are studied using bootstrapped data as well as simulated paths from the single index market model. It was found that the SR distributions of the portfolios under the clustering asset allocation structure have higher mean values and skewness but approximately the same standard deviation and kurtosis than those in the non-clustered case. Genetic Algorithm is suggested as a more efficient approach than Differential Evolution for the purpose of solving the clustering problem. We introduce a clustering scheme to improve portfolio Sharpe ratio. Mean and Skewness of Sharpe ratio can be improved by using the clustering scheme. Genetic Algorithm is apt at finding an optimal clustering structure. Clustering asset helps to improve portfolio risk-adjusted performance. Sharpe ratio maximization can be considered as a suitable clustering criterion.},
  citeulike-article-id = {9504815},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.eswa.2011.05.052},
  owner                = {cristi},
  posted-at            = {2016-10-01 16:47:37},
  timestamp            = {2020-02-27 04:04},
}

@InProceedings{Zhang-Zhu-2011,
  author         = {Zhang, Dabin and Zhu, Hou},
  booktitle      = {Fourth International Joint Conference on Computational Sciences and Optimization},
  date           = {2011-04-15},
  title          = {A clustering methodology for industry categorization using business cycle},
  doi            = {10.1109/{CSO}.2011.22},
  isbn           = {978-1-4244-9712-6},
  pages          = {318--321},
  publisher      = {IEEE},
  url            = {http://ieeexplore.ieee.org/document/5957670/},
  urldate        = {2019-12-04},
  day            = {15},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 04:04},
}

@Article{Zhong-Enke-2017a,
  author               = {Zhong, Xiao and Enke, David},
  date                 = {2017-12},
  journaltitle         = {Neurocomputing},
  title                = {A comprehensive cluster and classification mining procedure for daily stock market return forecasting},
  doi                  = {10.1016/j.neucom.2017.06.010},
  issn                 = {0925-2312},
  pages                = {152--168},
  volume               = {267},
  abstract             = {Data mining and big data analytic techniques are playing an important role in many application fields, including the financial markets. However, only few studies have focused on predicting daily stock market returns, and among these studies, the data mining procedures utilized are either incomplete or inefficient. This paper presents a comprehensive data mining process to forecast the daily direction of the S\&P 500 Index ETF (SPY) return based on 60 financial and economical features. The fuzzy c-means method (FCM) is initially used to cluster the preprocessed data. A principal component analysis (PCA) is applied next to the entire data set and each of seven clusters. The dimension of the entire cleaned data set is then reduced according to the combining results from the entire data set and each cluster. Corresponding to different levels of the dimensionality reduction, twelve new data sets are generated from the entire cleaned data. Artificial neural networks (ANNs) and logistic regression models are then used with the twelve transformed data sets for classification in order to forecast the daily direction of future market returns and indicate the efficiency of dimensionality reduction with PCA. A group of hypothesis tests are performed over the classification and simulation results to show that the ANNs give significantly higher classification accuracy than logistic regression, and that the trading strategies guided by the comprehensive cluster and classification mining procedure based on PCA and ANNs gain higher risk-adjusted profits than the comparison benchmarks, as well as those strategies guided by the forecasts based on PCA and logistic regression models.},
  citeulike-article-id = {14500341},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.neucom.2017.06.010},
  groups               = {Data_Cleaning},
  posted-at            = {2017-12-11 05:03:50},
  timestamp            = {2020-02-27 04:04},
}

@Article{Zhou-et-al-2016,
  author               = {Zhou, Shibing and Xu, Zhenyuan and Liu, Fei},
  date                 = {2016},
  journaltitle         = {IEEE Transactions on Neural Networks and Learning Systems},
  title                = {Method for Determining the Optimal Number of Clusters Based on Agglomerative Hierarchical Clustering},
  doi                  = {10.1109/tnnls.2016.2608001},
  issn                 = {2162-237X},
  pages                = {1--11},
  abstract             = {It is crucial to determine the optimal number of clusters for the clustering quality in cluster analysis. From the standpoint of sample geometry, two concepts, i.e., the sample clustering dispersion degree and the sample clustering synthesis degree, are defined, and a new clustering validity index is designed. Moreover, a method for determining the optimal number of clusters based on an agglomerative hierarchical clustering (AHC) algorithm is proposed. The new index and the method can evaluate the clustering results produced by the AHC and determine the optimal number of clusters for multiple types of datasets, such as linear, manifold, annular, and convex structures. Theoretical research and experimental results indicate the validity and good performance of the proposed index and the method.},
  citeulike-article-id = {14435140},
  citeulike-linkout-0  = {http://dx.doi.org/10.1109/tnnls.2016.2608001},
  posted-at            = {2017-09-21 00:36:08},
  timestamp            = {2020-02-27 04:04},
}

@Article{Zhu-et-al-2019c,
  author         = {Zhu, Dandan and Han, Tian and Zhou, Linqi and Yang, Xiaokang and Wu, Ying Nian},
  date           = {2019-11-19},
  journaltitle   = {arXiv e-Print},
  title          = {Deep Unsupervised Clustering with Clustered Generator Model},
  url            = {https://arxiv.org/abs/1911.08459v1},
  urldate        = {2019-12-15},
  abstract       = {This paper addresses the problem of unsupervised clustering which remains one of the most fundamental challenges in machine learning and artificial intelligence. We propose the clustered generator model for clustering which contains both continuous and discrete latent variables. Discrete latent variables model the cluster label while the continuous ones model variations within each cluster. The learning of the model proceeds in a unified probabilistic framework and incorporates the unsupervised clustering as an inner step without the need for an extra inference model as in existing variational-based models. The latent variables learned serve as both observed data embedding or latent representation for data distribution. Our experiments show that the proposed model can achieve competitive unsupervised clustering accuracy and can learn disentangled latent representations to generate realistic samples. In addition, the model can be naturally extended to per-pixel unsupervised clustering which remains largely unexplored.},
  day            = {19},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 04:04},
}

@Article{Ahern-2013,
  author               = {Ahern, Kenneth R.},
  date                 = {2013-01},
  journaltitle         = {SSRN e-Print},
  title                = {Network Centrality and the Cross Section of Stock Returns},
  url                  = {https://ssrn.com/abstract=2197370},
  abstract             = {Industries that are more central in the network of intersectoral trade earn higher stock returns than industries that are less central. To explain this finding, I argue that stocks in more central industries have greater market risk because they have greater exposure to sectoral shocks that transmit from one industry to another through intersectoral trade. Consistent with this argument, stock returns of central industries covary more closely with market returns and future consumption growth. In addition, the empirical evidence suggests that sectoral shocks that contribute to aggregate risk are more likely to pass through central industries than peripheral industries.},
  citeulike-article-id = {14148587},
  citeulike-linkout-0  = {http://ssrn.com/abstract=2197370},
  citeulike-linkout-1  = {http://papers.ssrn.com/sol3/Delivery.cfm/SSRNID2368442code434608.pdf?abstractid=2197370 and mirid=1},
  day                  = {7},
  groups               = {Networks and investment management},
  owner                = {cristi},
  posted-at            = {2016-09-28 19:56:55},
  timestamp            = {2020-02-27 04:28},
}

@Article{Ai-2017,
  author               = {Ai, Xinbo},
  date                 = {2017-06-26},
  journaltitle         = {Entropy},
  title                = {Node Importance Ranking of Complex Networks with Entropy Variation},
  doi                  = {10.3390/e19070303},
  number               = {7},
  pages                = {303+},
  volume               = {19},
  abstract             = {The heterogeneous nature of a complex network determines the roles of each node in the network that are quite different. Mechanisms of complex networks such as spreading dynamics, cascading reactions, and network synchronization are highly affected by a tiny fraction of so-called important nodes. Node importance ranking is thus of great theoretical and practical significance. Network entropy is usually utilized to characterize the amount of information encoded in the network structure and to measure the structural complexity at the graph level. We find that entropy can also serve as a local level metric to quantify node importance. We propose an entropic metric, Entropy Variation, defining the node importance as the variation of network entropy before and after its removal, according to the assumption that the removal of a more important node is likely to cause more structural variation. Like other state-of-the-art methods for ranking node importance, the proposed entropic metric is also used to utilize structural information, but at the systematical level, not the local level. Empirical investigations on real life networks, the Snake Idioms Network, and several other well-known networks, demonstrate the superiority of the proposed entropic metric, notably outperforming other centrality metrics in identifying the top-k most important nodes.},
  citeulike-article-id = {14461230},
  citeulike-linkout-0  = {http://dx.doi.org/10.3390/e19070303},
  citeulike-linkout-1  = {http://www.mdpi.com/1099-4300/19/7/303},
  citeulike-linkout-2  = {http://www.mdpi.com/1099-4300/19/7/303/pdf},
  day                  = {26},
  posted-at            = {2017-10-19 16:21:38},
  timestamp            = {2020-02-27 04:28},
}

@Article{Araujo-Gobel-2019,
  author         = {Araujo, Tanya and Gobel, Maximilian},
  date           = {2019-07},
  journaltitle   = {Physica A: Statistical Mechanics and its Applications},
  title          = {Reframing the S\&P 500 network of stocks along the 21st century},
  doi            = {10.1016/j.physa.2019.121062},
  issn           = {0378-4371},
  pages          = {121062},
  url            = {https://linkinghub.elsevier.com/retrieve/pii/S0378437119306478},
  urldate        = {2019-10-12},
  volume         = {526},
  abstract       = {Abstract Based on a sample of 296 stocks from the S\&P 500, the time-varying network structure within three distinct two-year periods since the beginning of the 21st century was analyzed. Logged first-differences of daily stock prices serve as input for a correlation-based distance measure between any two of the 296 stocks. The computation of a Minimal Spanning Tree then abstracts from a complete network and allows for a topological analysis of the resulting community structure. Both the Great Recession (2007-2008) and the Global Commodity Crisis (2010-2011) reveal tendencies of enhanced community formation compared to a formerly rather randomized network structure. Nevertheless, the drivers of the resulting clustering are found not to be related to industry sector affiliation.},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 04:28},
}

@Article{Arsov-Mirceva-2019,
  author         = {Arsov, Nino and Mirceva, Georgina},
  date           = {2019-11-26},
  journaltitle   = {arXiv e-Print},
  title          = {Network Embedding: An Overview},
  url            = {https://arxiv.org/abs/1911.11726v1},
  urldate        = {2019-12-15},
  abstract       = {Networks are one of the most powerful structures for modeling problems in the real world. Downstream machine learning tasks defined on networks have the potential to solve a variety of problems. With link prediction, for instance, one can predict whether two persons will become friends on a social network. Many machine learning algorithms, however, require that each input example is a real vector. Network embedding encompasses various methods for unsupervised, and sometimes supervised, learning of feature representations of nodes and links in a network. Typically, embedding methods are based on the assumption that the similarity between nodes in the network should be reflected in the learned feature representations. In this paper, we review significant contributions to network embedding in the last decade. In particular, we look at four methods: Spectral Clustering, DeepWalk, Large-scale Information Network Embedding (LINE), and node2vec. We describe each method and list its advantages and shortcomings. In addition, we give examples of real-world machine learning problems on networks in which the embedding is critical in order to maximize the predictive performance of the machine learning task. Finally, we take a look at research trends and state-of-the art methods in the research on network embedding.},
  day            = {26},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 04:28},
}

@Article{Balasubramanian-et-al-2019a,
  author         = {Koushik Balasubramanian and Harish Sundaresh and Diqing Wu and Kearns, Kevin},
  date           = {2019-05-01},
  journaltitle   = {The Journal of Financial Data Science},
  title          = {Do stocks stalk other stocks in their complex network? A complex networks approach to stock market dynamics},
  doi            = {10.3905/jfds.2019.1.2.035},
  issn           = {2640-3951},
  url            = {https://jfds.pm-research.com/content/1/2/35},
  urldate        = {2019-09-14},
  day            = {1},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 04:28},
}

@Article{Banerjee-et-al-2019,
  author         = {Banerjee, Sayantan and Guhathakurta, Kousik},
  date           = {2019-11-14},
  journaltitle   = {arXiv e-Print},
  title          = {Change-point Analysis in Financial Networks},
  url            = {https://arxiv.org/abs/1911.05952v1},
  urldate        = {2019-12-17},
  abstract       = {A major impact of globalization has been the information flow across the financial markets rendering them vulnerable to financial contagion. Research has focused on network analysis techniques to understand the extent and nature of such information flow. It is now an established fact that a stock market crash in one country can have a serious impact on other markets across the globe. It follows that such crashes or critical regimes will affect the network dynamics of the global financial markets. In this paper, we use sequential change point detection in dynamic networks to detect changes in the network characteristics of thirteen stock markets across the globe. Our method helps us to detect changes in network behavior across all known stock market crashes during the period of study. In most of the cases, we can detect a change in the network characteristics prior to crash. Our work thus opens the possibility of using this technique to create a warning bell for critical regimes in financial markets.},
  day            = {14},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 04:28},
}

@Article{Barfuss-et-al-2016,
  author               = {Barfuss, Wolfram and Massara, Guido P. and Di Matteo, T. and Aste, Tomaso},
  date                 = {2016-12-13},
  journaltitle         = {Physical Review E},
  title                = {Parsimonious modeling with information filtering networks},
  doi                  = {10.1103/physreve.94.062306},
  issn                 = {2470-0045},
  number               = {6},
  volume               = {94},
  abstract             = {We introduce a methodology to construct parsimonious probabilistic models. This method makes use of information filtering networks to produce a robust estimate of the global sparse inverse covariance from a simple sum of local inverse covariances computed on small subparts of the network. Being based on local and low-dimensional inversions, this method is computationally very efficient and statistically robust, even for the estimation of inverse covariance of high-dimensional, noisy, and short time series. Applied to financial data our method results are computationally more efficient than state-of-the-art methodologies such as Glasso producing, in a fraction of the computation time, models that can have equivalent or better performances but with a sparser inference structure. We also discuss performances with sparse factor models where we notice that relative performances decrease with the number of factors. The local nature of this approach allows us to perform computations in parallel and provides a tool for dynamical adaptation by partial updating when the properties of some variables change without the need of recomputing the whole model. This makes this approach particularly suitable to handle big data sets with large numbers of variables. Examples of practical application for forecasting, stress testing, and risk allocation in financial systems are also provided.},
  citeulike-article-id = {14444475},
  citeulike-linkout-0  = {http://dx.doi.org/10.1103/physreve.94.062306},
  day                  = {13},
  posted-at            = {2017-10-03 08:40:15},
  timestamp            = {2020-02-27 04:28},
}

@Article{Barnett-Onnela-2016,
  author               = {Barnett, Ian and Onnela, Jukka-Pekka},
  date                 = {2016-01-07},
  journaltitle         = {Scientific Reports},
  title                = {Change Point Detection in Correlation Networks},
  doi                  = {10.1038/srep18893},
  issn                 = {2045-2322},
  number               = {1},
  volume               = {6},
  abstract             = {Many systems of interacting elements can be conceptualized as networks, where network nodes represent the elements and network ties represent interactions between the elements. In systems where the underlying network evolves, it is useful to determine the points in time where the network structure changes significantly as these may correspond to functional change points. We propose a method for detecting change points in correlation networks that, unlike previous change point detection methods designed for time series data, requires minimal distributional assumptions. We investigate the difficulty of change point detection near the boundaries of the time series in correlation networks and study the power of our method and competing methods through simulation. We also show the generalizable nature of the method by applying it to stock price data as well as fMRI data.},
  citeulike-article-id = {14444487},
  citeulike-linkout-0  = {http://dx.doi.org/10.1038/srep18893},
  day                  = {7},
  groups               = {ChngPoints_TimeSrs},
  posted-at            = {2017-10-03 09:10:53},
  timestamp            = {2020-02-27 04:28},
}

@Article{Behrisch-et-al-2016,
  author               = {Behrisch, Michael and Bach, Benjamin and Henry Riche, Nathalie and Schreck, Tobias and Fekete, Jean-Daniel},
  date                 = {2016-06},
  journaltitle         = {Computer Graphics Forum},
  title                = {Matrix Reordering Methods for Table and Network Visualization},
  doi                  = {10.1111/cgf.12935},
  issn                 = {0167-7055},
  number               = {3},
  pages                = {693--716},
  volume               = {35},
  abstract             = {This survey provides a description of algorithms to reorder visual matrices of tabular data and adjacency matrix of Networks. The goal of this survey is to provide a comprehensive list of reordering algorithms published in different fields such as statistics, bioinformatics, or graph theory. While several of these algorithms are described in publications and others are available in software libraries and programs, there is little awareness of what is done across all fields. Our survey aims at describing these reordering algorithms in a unified manner to enable a wide audience to understand their differences and subtleties. We organize this corpus in a consistent manner, independently of the application or research field. We also provide practical guidance on how to select appropriate algorithms depending on the structure and size of the matrix to reorder, and point to implementations when available.},
  citeulike-article-id = {14474677},
  citeulike-linkout-0  = {http://dx.doi.org/10.1111/cgf.12935},
  posted-at            = {2017-11-11 22:09:57},
  timestamp            = {2020-02-27 04:28},
}

@Article{Bhattacharya-et-al-2019,
  author         = {Bhattacharya, Rohit and Malinsky, Daniel and Shpitser, Ilya},
  date           = {2019-06-29},
  journaltitle   = {arXiv e-Print},
  title          = {Causal Inference Under Interference And Network Uncertainty},
  url            = {https://arxiv.org/abs/1907.00221},
  urldate        = {2019-10-03},
  abstract       = {Classical causal and statistical inference methods typically assume the observed data consists of independent realizations. However, in many applications this assumption is inappropriate due to a network of dependences between units in the data. Methods for estimating causal effects have been developed in the setting where the structure of dependence between units is known exactly, but in practice there is often substantial uncertainty about the precise network structure. This is true, for example, in trial data drawn from vulnerable communities where social ties are difficult to query directly. In this paper we combine techniques from the structure learning and interference literatures in causal inference, proposing a general method for estimating causal effects under data dependence when the structure of this dependence is not known a priori. We demonstrate the utility of our method on synthetic datasets which exhibit network dependence.},
  day            = {29},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 04:28},
}

@Article{Billio-et-al-2016b,
  author               = {Billio, Monica and Caporin, Massimiliano and Panzica, Roberto C. and Pelizzon, Loriana},
  date                 = {2016},
  journaltitle         = {SSRN e-Print},
  title                = {The Impact of Network Connectivity on Factor Exposures, Asset Pricing and Portfolio Diversification},
  url                  = {https://ssrn.com/abstract=2914218},
  abstract             = {This paper extends the classic factor-based asset pricing model by including network linkages in linear factor models. We assume that the network linkages are exogenously provided. This extension of the model allows a better understanding of the causes of systematic risk and shows that (i) network exposures act as an inflating factor for systematic exposure to common factors and (ii) the power of diversification is reduced by the presence of network connections. Moreover, we show that in the presence of network links a misspecified traditional linear factor model presents residuals that are correlated and heteroskedastic. We support our claims with an extensive simulation experiment.},
  citeulike-article-id = {14460391},
  citeulike-linkout-0  = {https://papers.ssrn.com/sol3/papers.cfm?abstractid=2914218},
  posted-at            = {2017-10-18 14:56:56},
  timestamp            = {2020-02-27 04:28},
}

@InCollection{Binner-et-al-2004,
  author               = {Binner, Jane M. and Elger, Thomas and Nilsson, Birger and Tepper, Jonathan A.},
  booktitle            = {Volume 19, Applications of Artificial Intelligence in Finance and Economics},
  date                 = {2004},
  title                = {Tools for non-linear time series forecasting in economics - an empirical comparison of regime switching vector autoregressive models and recurrent neural networks},
  doi                  = {10.1016/s0731-9053(04)19003-8},
  isbn                 = {0-7623-1150-9},
  location             = {Bingley},
  pages                = {71--91},
  publisher            = {Emerald (MCB UP )},
  volume               = {19},
  abstract             = {The purpose of this study is to contrast the forecasting performance of two non-linear models, a regime-switching vector autoregressive model (RS-VAR) and a recurrent neural network (RNN), to that of a linear benchmark VAR model. Our specific forecasting experiment is U.K. inflation and we utilize monthly data from 1969 to 2003. The RS-VAR and the RNN perform approximately on par over both monthly and annual forecast horizons. Both non-linear models perform significantly better than the VAR model.},
  citeulike-article-id = {13995905},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/s0731-9053(04)19003-8},
  groups               = {FrcstQWIM_TimeSrs},
  owner                = {cristi},
  posted-at            = {2016-04-04 02:20:24},
  timestamp            = {2020-02-27 04:28},
}

@Article{Blocher-2016,
  author               = {Blocher, Jesse},
  date                 = {2016-09},
  journaltitle         = {Journal of Financial Markets},
  title                = {Network externalities in mutual funds},
  doi                  = {10.1016/j.finmar.2016.04.001},
  issn                 = {1386-4181},
  pages                = {1--26},
  volume               = {30},
  abstract             = {Amplifying network externalities among mutual funds can explain substantial flow-based effects documented in the literature. These network externalities are generated by mutual funds with common holdings and return-chasing investors. This externality is 32-92 percent as large as typical direct effects (i.e. the fund's own lagged flows). This effect is independent of style investing and robust to multiple specifications of holdings similarity. Effects between fund styles provide evidence of crowded trades or 'style drift'. The literature on mutual fund flows documents surprisingly large return effects given that mutual fund flows are uninformed (i.e., not related to fundamentals). I provide evidence that network externalities generate the necessary amplification mechanism to support these results. Network externalities are generated by mutual funds with common holdings and return-chasing investors. Economically, I show that the fund flow network externality is 32-92 percent as large as the typical explanatory effects (e.g., lagged flows). Network externalities generate a 1.5 percent quarterly excess return that reverses in the subsequent year, and are independent of style investing and robust to multiple specifications of holdings similarity.},
  citeulike-article-id = {14219707},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.finmar.2016.04.001},
  groups               = {Networks and investment management},
  owner                = {cristi},
  posted-at            = {2016-12-04 20:50:20},
  timestamp            = {2020-02-27 04:28},
}

@Article{Boginski-et-al-2014,
  author               = {Boginski, Vladimir and Butenko, Sergiy and Shirokikh, Oleg and Trukhanov, Svyatoslav and Gil Lafuente, Jaime},
  date                 = {2014},
  journaltitle         = {Annals of Operations Research},
  title                = {A network-based data mining approach to portfolio selection via weighted clique relaxations},
  doi                  = {10.1007/s10479-013-1395-3},
  number               = {1},
  pages                = {23--34},
  volume               = {216},
  abstract             = {We introduce a new network-based data mining approach to selecting diversified portfolios by modeling the stock market as a network and utilizing combinatorial optimization techniques to find maximum-weight s-plexes in the obtained networks. The considered approach is based on the weighted market graph model, which is used for identifying clusters of stocks according to a correlation-based criterion. The proposed techniques provide a new framework for selecting profitable diversified portfolios, which is verified by computational experiments on historical data over the past decade. In addition, the proposed approach can be used as a complementary tool for narrowing down a set of - candidate- stocks for a diversified portfolio, which can potentially be analyzed using other known portfolio selection techniques.},
  citeulike-article-id = {14150016},
  citeulike-linkout-0  = {http://dx.doi.org/10.1007/s10479-013-1395-3},
  citeulike-linkout-1  = {http://link.springer.com/article/10.1007/s10479-013-1395-3},
  groups               = {Networks and investment management, PortfOptim_Network},
  owner                = {cristi},
  posted-at            = {2016-10-01 16:58:14},
  publisher            = {Springer US},
  timestamp            = {2020-02-27 04:28},
}

@Article{Bramson-Vandermarliere-2017,
  author               = {Bramson, Aaron and Vandermarliere, Benjamin},
  date                 = {2016-09-27},
  journaltitle         = {Scientific Reports},
  title                = {Benchmarking Measures of Network Influence},
  doi                  = {10.1038/srep34052},
  issn                 = {2045-2322},
  number               = {1},
  volume               = {6},
  abstract             = {Identifying key agents for the transmission of diseases (ideas, technology, etc.) across social networks has predominantly relied on measures of centrality on a static base network or a temporally flattened graph of agent interactions. Various measures have been proposed as the best trackers of influence, such as degree centrality, betweenness, and k-shell, depending on the structure of the connectivity. We consider SIR and SIS propagation dynamics on a temporally-extruded network of observed interactions and measure the conditional marginal spread as the change in the magnitude of the infection given the removal of each agent at each time: its temporal knockout (TKO) score. We argue that this TKO score is an effective benchmark measure for evaluating the accuracy of other, often more practical, measures of influence. We find that none of the network measures applied to the induced flat graphs are accurate predictors of network propagation influence on the systems studied; however, temporal networks and the TKO measure provide the requisite targets for the search for effective predictive measures.},
  citeulike-article-id = {14461233},
  citeulike-linkout-0  = {http://dx.doi.org/10.1038/srep34052},
  day                  = {27},
  posted-at            = {2017-10-19 16:24:14},
  timestamp            = {2020-02-27 04:28},
}

@Article{Brida-et-al-2016,
  author               = {Brida, Juan G. and Matesanz, David and Seijas, Maria N.},
  date                 = {2016-02},
  journaltitle         = {Physica A: Statistical Mechanics and its Applications},
  title                = {Network analysis of returns and volume trading in stock markets: The Euro Stoxx case},
  doi                  = {10.1016/j.physa.2015.10.078},
  issn                 = {0378-4371},
  pages                = {751--764},
  volume               = {444},
  abstract             = {This study applies network analysis to analyze the structure of the Euro Stoxx market during the long period from 2002 up to 2014. The paper generalizes previous research on stock market networks by including asset returns and volume trading as the main variables to study the financial market. A multidimensional generalization of the minimal spanning tree (MST) concept is introduced, by adding the role of trading volume to the traditional approach which only includes price returns. Additionally, we use symbolization methods to the raw data to study the behavior of the market structure in different, normal and critical, situations. The hierarchical organization of the network is derived, and the MST for different sub-periods of 2002-2014 is created to illustrate how the structure of the market evolves over time. From the structural topologies of these trees, different clusters of companies are identified and analyzed according to their geographical and economic links. Two important results are achieved. Firstly, as other studies have highlighted, at the time of the financial crisis after 2008 the network becomes a more centralized one. Secondly and most important, during our second period of analysis, 2008-2014, we observe that hierarchy becomes more country-specific where different sub-clusters of stocks belonging to France, Germany, Spain or Italy are found apart from their business sector group. This result may suggest that during this period of time financial investors seem to be worried most about country specific economic circumstances.},
  citeulike-article-id = {14444500},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.physa.2015.10.078},
  posted-at            = {2017-10-03 09:49:12},
  timestamp            = {2020-02-27 04:28},
}

@Article{Caccioli-et-al-2017,
  author               = {Caccioli, Fabio and Barucca, Paolo and Kobayashi, Teruyoshi},
  date                 = {2017-10-31},
  journaltitle         = {Journal of Computational Social Science},
  title                = {Network models of financial systemic risk: A review},
  doi                  = {10.1007/s42001-017-0008-3},
  issn                 = {2432-2717},
  abstract             = {The global financial system can be represented as a large complex network in which banks, hedge funds and other financial institutions are interconnected to each other through visible and invisible financial linkages. Recently, a lot of attention has been paid to the understanding of the mechanisms that can lead to a breakdown of this network. This can happen when the existing financial links turn from being a means of risk diversification to channels for the propagation of risk across financial institutions. In this review article, we summarize recent developments in the modeling of financial systemic risk. We focus in particular on network approaches, such as models of default cascades due to bilateral exposures or to overlapping portfolios, and we also report on recent findings on the empirical structure of interbank networks. The current review provides a landscape of the newly arising interdisciplinary field lying at the intersection of several disciplines, such as network science, physics, engineering, economics, and ecology.},
  citeulike-article-id = {14482736},
  citeulike-linkout-0  = {http://arxiv.org/abs/1710.11512},
  citeulike-linkout-1  = {http://arxiv.org/pdf/1710.11512},
  citeulike-linkout-2  = {http://dx.doi.org/10.1007/s42001-017-0008-3},
  day                  = {31},
  posted-at            = {2017-12-03 20:30:05},
  timestamp            = {2020-02-27 04:28},
}

@Article{Caraiani-2017,
  author               = {Caraiani, Petre},
  date                 = {2017-01},
  journaltitle         = {Physica A: Statistical Mechanics and its Applications},
  title                = {The predictive power of local properties of financial networks},
  doi                  = {10.1016/j.physa.2016.08.032},
  issn                 = {0378-4371},
  pages                = {79--90},
  volume               = {466},
  abstract             = {We construct correlation based financial networks in time. We derive local properties in such networks. Some local properties Granger-cause the overall dynamics of the stock market. The literature on analyzing the dynamics of financial networks has focused so far on the predictive power of global measures of networks like entropy or index cohesive force. In this paper, I show that the local network properties have similar predictive power. I focus on key network measures like average path length, average degree or cluster coefficient, and also consider the diameter and the s-metric. Using Granger causality tests, I show that some of these measures have statistically significant prediction power with respect to the dynamics of aggregate stock market. Average path length is most robust relative to the frequency of data used or specification (index or growth rate). Most measures are found to have predictive power only for monthly frequency. Further evidences that support this view are provided through a simple regression model.},
  citeulike-article-id = {14357941},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.physa.2016.08.032},
  posted-at            = {2017-05-16 14:20:00},
  timestamp            = {2020-02-27 04:28},
}

@InCollection{Chakrabarti-2017,
  author               = {Chakrabarti, AnindyaS},
  booktitle            = {Econophysics and Sociophysics: Recent Progress and Future Directions},
  date                 = {2017},
  title                = {Network Theory in Macroeconomics and Finance},
  doi                  = {10.1007/978-3-319-47705-3\_5},
  editor               = {Abergel, Frederic and Aoyama, Hideaki and Chakrabarti, Bikas K. and Chakraborti, Anirban and Deo, Nivedita and Raina, Dhruv and Vodenska, Irena},
  pages                = {71--83},
  publisher            = {Springer International Publishing},
  series               = {New Economic Windows},
  abstract             = {In the last couple of years, a large body of theoretical and empirical work has been done emphasizing the network structures of economies. Availability of large data and suitable tools have made such a transition possible. The implications of such work is also apparent in settling age-old debates of micro versus macro foundations and to model and quantify shock propagation mechanisms through the networks. Here I summarize a number of major topics where significant work has been done in the recent times using both physics-based and economics-based models of networks and show that they are complementary in approach.},
  citeulike-article-id = {14433280},
  citeulike-linkout-0  = {http://dx.doi.org/10.1007/978-3-319-47705-35},
  citeulike-linkout-1  = {http://link.springer.com/chapter/10.1007/978-3-319-47705-35},
  posted-at            = {2017-09-17 20:42:12},
  timestamp            = {2020-02-27 04:28},
}

@Article{Chang-et-al-2018b,
  author         = {Chang, Yen-Yu and Sun, Fan-Yun and Wu, Yueh-Hua and Lin, Shou-De},
  date           = {2018-09-06},
  journaltitle   = {arXiv e-Print},
  title          = {A Memory-Network Based Solution for Multivariate Time-Series Forecasting},
  url            = {https://arxiv.org/abs/1809.02105},
  urldate        = {2018-09-10},
  abstract       = {Multivariate time series forecasting is extensively studied throughout the years with ubiquitous applications in areas such as finance, traffic, environment, etc. Still, concerns have been raised on traditional methods for incapable of modeling complex patterns or dependencies lying in real word data. To address such concerns, various deep learning models, mainly Recurrent Neural Network (RNN) based methods, are proposed. Nevertheless, capturing extremely long-term patterns while effectively incorporating information from other variables remains a challenge for time-series forecasting. Furthermore, lack-of-explainability remains one serious drawback for deep neural network models. Inspired by Memory Network proposed for solving the question-answering task, we propose a deep learning based model named Memory Time-series network (MTNet) for time series forecasting. MTNet consists of a large memory component, three separate encoders, and an autoregressive component to train jointly. Additionally, the attention mechanism designed enable MTNet to be highly interpretable. We can easily tell which part of the historic data is referenced the most.},
  day            = {6},
  f1000-projects = {QuantInvest},
  groups         = {FrcstQWIM_TimeSrs, ML_ForcstTimeSrs},
  timestamp      = {2020-02-27 04:28},
}

@Article{Chen-et-al-2015g,
  author               = {Chen, Kun and Luo, Peng and Sun, Bianxia and Wang, Huaiqing},
  date                 = {2015-10},
  journaltitle         = {Physica A: Statistical Mechanics and its Applications},
  title                = {Which stocks are profitable? A network method to investigate the effects of network structure on stock returns},
  doi                  = {10.1016/j.physa.2015.05.047},
  issn                 = {0378-4371},
  pages                = {224--235},
  volume               = {436},
  abstract             = {According to asset pricing theory, a stock's expected returns are determined by its exposure to systematic risk. In this paper, we propose a new method for analyzing the interaction effects among industries and stocks on stock returns. We construct a complex network based on correlations of abnormal stock returns and use centrality and modularity, two popular measures in social science, to determine the effect of interconnections on industry and stock returns. Supported by previous studies, our findings indicate that a relationship exists between inter-industry closeness and industry returns and between stock centrality and stock returns. The theoretical and practical contributions of these findings are discussed.},
  citeulike-article-id = {14461238},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.physa.2015.05.047},
  posted-at            = {2017-10-19 16:42:25},
  timestamp            = {2020-02-27 04:28},
}

@Article{Chen-et-al-2016d,
  author               = {Chen, Bin and Wang, Zhixue and Luo, Chen},
  date                 = {2016-12-20},
  journaltitle         = {Journal of Systems Engineering and Electronics},
  title                = {Integrated evaluation approach for node importance of complex networks based on relative entropy},
  doi                  = {10.21629/jsee.2016.06.10},
  issn                 = {1004-4132},
  number               = {6},
  pages                = {1219--1226},
  volume               = {27},
  abstract             = {Evaluating the node importance correctly is a crucial issue for complex networks research. If a network has multiple or ambiguous morphological features, the evaluation result of an individual index may be unilateral. Meanwhile, the results may be inconsistent if different indexes are adopted simultaneously. To solve the problem, an integrated approach is proposed based on relative entropy. In this approach, a system of multiple indexes is constructed firstly. Then each evaluation result of an individual index is handled into a discrete distribution. Finally an optimal integrated evaluation solution is obtained by linear programming. This approach has a well-formed theoretic basis and an easily calculated procedure, which can be used in a variety of complex networks. Experimental results show that the proposed approach is more effective than other different methods proposed in some literatures.},
  citeulike-article-id = {14461260},
  citeulike-linkout-0  = {http://dx.doi.org/10.21629/jsee.2016.06.10},
  day                  = {20},
  posted-at            = {2017-10-19 17:32:50},
  timestamp            = {2020-02-27 04:28},
}

@Article{Chu-Nadarajah-2017,
  author               = {Chu, J. and Nadarajah, S.},
  date                 = {2017-04},
  journaltitle         = {Physica A: Statistical Mechanics and its Applications},
  title                = {A statistical analysis of UK financial networks},
  doi                  = {10.1016/j.physa.2016.12.073},
  issn                 = {0378-4371},
  pages                = {445--459},
  volume               = {471},
  abstract             = {In recent years, with a growing interest in big or large datasets, there has been a rise in the application of large graphs and networks to financial big data. Much of this research has focused on the construction and analysis of the network structure of stock markets, based on the relationships between stock prices. Motivated by Boginski et al. (2005), who studied the characteristics of a network structure of the US stock market, we construct network graphs of the UK stock market using same method. We fit four distributions to the degree density of the vertices from these graphs, the Pareto I, Frechet, lognormal, and generalised Pareto distributions, and assess the goodness of fit. Our results show that the degree density of the complements of the market graphs, constructed using a negative threshold value close to zero, can be fitted well with the Frechet and lognormal distributions.},
  citeulike-article-id = {14461236},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.physa.2016.12.073},
  posted-at            = {2017-10-19 16:41:06},
  timestamp            = {2020-02-27 04:28},
}

@Article{Clemente-et-al-2019,
  author         = {Clemente, Gian Paolo and Grassi, Rosanna and Hitaj, Asmerilda},
  date           = {2019-07-02},
  journaltitle   = {arXiv e-Print},
  title          = {Smart network based portfolios},
  url            = {https://arxiv.org/abs/1907.01274},
  urldate        = {2019-08-20},
  abstract       = {In this article we deal with the problem of portfolio allocation by enhancing network theory tools. We use the dependence structure of the correlations network in constructing some well-known risk-based models in which the estimation of correlation matrix is a building block in the portfolio optimization. We formulate and solve all these portfolio allocation problems using both the standard approach and the network-based approach. Moreover, in constructing the network-based portfolios we propose the use of two different estimators for the covariance matrix: the sample estimator and the shrinkage toward constant correlation one. All the strategies under analysis are implemented on two high-dimensional portfolios having different characteristics, covering the period from January 2001 to December 2017. We find that the network-based portfolio consistently better performs and has lower risk compared to the corresponding standard portfolio in an out-of-sample perspective.},
  day            = {2},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 04:28},
}

@Article{Clemente-et-al-2019a,
  author         = {Clemente, Gian Paolo and Grassi, Rosanna and Hitaj, Asmerilda},
  date           = {2019},
  journaltitle   = {Annals of Operations Research},
  title          = {Asset allocation: new evidence through network approaches},
  url            = {https://link.springer.com/article/10.1007/s10479-019-03136-y},
  abstract       = {The main contribution of the paper is to unveil the role of the network structure in the financial markets to improve the portfolio selection process, where nodes indicate securities and edges capture the dependence structure of the system. Three different methods are proposed in order to extract the dependence structure between assets in a network context. Starting from this modified structure, we formulate and then we solve the asset allocation problem. We find that the optimal portfolios obtained through a network-based approach are composed mainly of peripheral assets, which are poorly connected with the others. These portfolios, in the majority of cases, are characterized by an higher trade-off between performance and risk with respect to the traditional global minimum variance portfolio. Additionally, this methodology benefits of a graphical visualization of the selected portfolio directly over the graphic layout of the network, which helps in improving our understanding of the optimal strategy.},
  day            = {20},
  f1000-projects = {QuantInvest},
  groups         = {Invest_Network},
  timestamp      = {2020-02-27 04:28},
}

@Article{Clemente-Grassi-2018,
  author               = {Clemente, Gian P. and Grassi, Rosanna},
  date                 = {2018-06-19},
  journaltitle         = {Chaos, Solitons \& Fractals},
  title                = {Directed clustering in weighted networks: a new perspective},
  doi                  = {10.1016/j.chaos.2017.12.007},
  eprint               = {1706.07322},
  eprinttype           = {arXiv},
  issn                 = {0960-0779},
  pages                = {26--38},
  volume               = {107},
  abstract             = {In this paper, we consider the problem of assessing local clustering in complex networks. Various definitions for this measure have been proposed for the cases of networks having weighted edges, but less attention has been paid to both weighted and directed networks. We provide a new local clustering coefficient for this kind of networks, starting from those existing in the literature for the weighted and undirected case. Furthermore, we extract from our coefficient four specific components, in order to separately consider different link patterns of triangles. Empirical applications on several real networks from different frameworks and with different order are provided. The performance of our coefficient is also compared with that of existing coefficients in the literature.},
  citeulike-article-id = {14515534},
  citeulike-linkout-0  = {http://arxiv.org/abs/1706.07322},
  citeulike-linkout-1  = {http://arxiv.org/pdf/1706.07322},
  citeulike-linkout-2  = {http://dx.doi.org/10.1016/j.chaos.2017.12.007},
  day                  = {19},
  posted-at            = {2018-01-11 22:01:47},
  timestamp            = {2020-02-27 04:28},
}

@Article{Clevert-et-al-2015,
  author               = {Clevert, Djork-Arne and Mayr, Andreas and Unterthiner, Thomas and Hochreiter, Sepp},
  date                 = {2015-06},
  journaltitle         = {arXiv e-Print},
  title                = {Rectified Factor Networks},
  eprint               = {1502.06464},
  eprinttype           = {arXiv},
  url                  = {https://arxiv.org/abs/1502.06464},
  abstract             = {We propose rectified factor networks (RFNs) to efficiently construct very sparse, non-linear, high-dimensional representations of the input. RFN models identify rare and small events in the input, have a low interference between code units, have a small reconstruction error, and explain the data covariance structure. RFN learning is a generalized alternating minimization algorithm derived from the posterior regularization method which enforces non-negative and normalized posterior means. We proof convergence and correctness of the RFN learning algorithm. On benchmarks, RFNs are compared to other unsupervised methods like autoencoders, RBMs, factor analysis, ICA, and PCA. In contrast to previous sparse coding methods, RFNs yield sparser codes, capture the data's covariance structure more precisely, and have a significantly smaller reconstruction error. We test RFNs as pretraining technique for deep networks on different vision datasets, where RFNs were superior to RBMs and autoencoders. On gene expression data from two pharmaceutical drug discovery studies, RFNs detected small and rare gene modules that revealed highly relevant new biological insights which were so far missed by other unsupervised methods.},
  citeulike-article-id = {14147497},
  citeulike-linkout-0  = {http://arxiv.org/abs/1502.06464},
  citeulike-linkout-1  = {http://arxiv.org/pdf/1502.06464},
  day                  = {11},
  owner                = {cristi},
  posted-at            = {2016-09-27 21:33:08},
  timestamp            = {2020-02-27 04:28},
}

@Article{Darst-et-al-2014,
  author               = {Darst, Richard K. and Nussinov, Zohar and Fortunato, Santo},
  date                 = {2014-03-20},
  journaltitle         = {Physical Review E},
  title                = {Improving the performance of algorithms to find communities in networks},
  doi                  = {10.1103/physreve.89.032809},
  issn                 = {1539-3755},
  number               = {3},
  volume               = {89},
  abstract             = {Most algorithms to detect communities in networks typically work without any information on the cluster structure to be found, as one has no a priori knowledge of it, in general. Not surprisingly, knowing some features of the unknown partition could help its identification, yielding an improvement of the performance of the method. Here we show that, if the number of clusters was known beforehand, standard methods, like modularity optimization, would considerably gain in accuracy, mitigating the severe resolution bias that undermines the reliability of the results of the original unconstrained version. The number of clusters can be inferred from the spectra of the recently introduced nonbacktracking and flow matrices, even in benchmark graphs with realistic community structure. The limit of such a two-step procedure is the overhead of the computation of the spectra.},
  citeulike-article-id = {14449651},
  citeulike-linkout-0  = {http://dx.doi.org/10.1103/physreve.89.032809},
  day                  = {20},
  posted-at            = {2017-10-12 23:15:45},
  timestamp            = {2020-02-27 04:31},
}

@Article{Das-et-al-2019,
  author         = {Das, Sanjiv R. and Kim, Seoyoung and Ostrov, Daniel N.},
  date           = {2019-01-31},
  journaltitle   = {The Journal of Financial Data Science},
  title          = {Dynamic Systemic Risk: Networks in Data Science},
  url            = {https://jfds.pm-research.com/content/1/1/141},
  abstract       = {In this article, the authors propose a theory-driven framework for monitoring system-wide risk by extending data science methods widely deployed in social networks. Their approach extends the one-firm Merton credit risk model to a generalized stochastic network-based framework across all financial institutions, comprising a novel approach to measuring systemic risk over time. The authors identify four desired properties for any systemic risk measure. They also develop measures for the risks created by each individual institution and a measure for risk created by each pairwise connection between institutions. Four specific implementation models are then explored, and brief empirical examples illustrate the ease of implementation of these four models and show general consistency among their results.},
  day            = {31},
  f1000-projects = {QuantInvest},
  publisher      = {Institutional Investor Journals Umbrella},
  timestamp      = {2020-02-27 04:31},
}

@Article{deCarvalho-et-al-2018,
  author         = {{de Carvalho}, Pablo Jose Campos and Gupta, Aparna},
  date           = {2018-06},
  journaltitle   = {Journal of banking \& finance},
  title          = {A network approach to unravel asset price comovement using minimal dependence structure},
  doi            = {10.1016/j.jbankfin.2018.04.012},
  issn           = {0378-4266},
  pages          = {119--132},
  volume         = {91},
  abstract       = {Abstract We develop a network representation-based methodology to aid an exploratory analysis of temporally evolving comovement in asset prices. This parsimonious order- n representation of the most significant comovement in asset prices, filtered by common factors, allows tackling a large number of assets and unraveling their complex comovement structure. Flexibility in choosing explanatory factors to suit the specific objectives of a study makes this methodology useful for portfolio analysis, risk parity approaches, and risk management decisions. We illustrate the features of the methodology for a set of major industry equity indices and to blue chip stocks, where we analyze the dynamic relevance of Fama-French factors. Investigating the network for more than 20 years, including the dot-com bust, global financial crisis, and European debt crisis, helps draw many insights. For instance, unexpected industries are seen to connect idiosyncratically through the dot-com bust. We demonstrate that a network factor model based portfolio allocation performs better than a regular factor model based allocation.},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 04:31},
}

@Article{Delpini-et-al-2018,
  author         = {Delpini, Danilo and Battiston, Stefano and Caldarelli, Guido and Riccaboni, Massimo},
  date           = {2018-01-07},
  journaltitle   = {arXiv e-Print},
  title          = {The Network of U.S. Mutual Fund Investments: Diversification, Similarity and Fragility throughout the Global Financial Crisis},
  url            = {https://arxiv.org/abs/1801.02205},
  abstract       = {Network theory proved recently to be useful in the quantification of many properties of financial systems. The analysis of the structure of investment portfolios is a major application since their eventual correlation and overlap impact the actual risk diversification by individual investors. We investigate the bipartite network of US mutual fund portfolios and their assets. We follow its evolution during the Global Financial Crisis and analyse the interplay between diversification, as understood in classical portfolio theory, and similarity of the investments of different funds. We show that, on average, portfolios have become more diversified and less similar during the crisis. However, we also find that large overlap is far more likely than expected from models of random allocation of investments. This indicates the existence of strong correlations between fund portfolio strategies. We introduce a simplified model of propagation of financial shocks, that we exploit to show that a systemic risk component origins from the similarity of portfolios. The network is still vulnerable after crisis because of this effect, despite the increase in the diversification of portfolios. Our results indicate that diversification may even increase systemic risk when funds diversify in the same way. Diversification and similarity can play antagonistic roles and the trade-off between the two should be taken into account to properly assess systemic risk.},
  day            = {7},
  f1000-projects = {QuantInvest},
  groups         = {Invest_Network, Invest_Diversif},
  timestamp      = {2020-02-27 04:31},
}

@Article{Demange-2016,
  author         = {Demange, Gabrielle},
  date           = {2016-12-08},
  journaltitle   = {Management Science},
  title          = {Contagion in financial networks: A threat index},
  doi            = {10.1287/mnsc.2016.2592},
  issn           = {0025-1909},
  urldate        = {2019-09-01},
  abstract       = {This paper proposes to measure the spillover effects that cross liabilities generate on the magnitude of default in a system of financially linked institutions. Based on a simple model and an explicit criterion aggregate debt repayments measure is defined for each institution, affected by its characteristics and links to others. These measures for each institution relevant information on the interaction between the liabilities structure and the shocks to resources, and they can be useful to determine optimal intervention policies. The approach is illustrated to evaluate the consolidated foreign claims of 10 European Union countries. This paper was accepted by Amit Seru, finance.},
  day            = {8},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 04:31},
}

@Article{DiMatteo-et-al-2010,
  author               = {Di Matteo, T. and Pozzi, F. and Aste, T.},
  date                 = {2010-08},
  journaltitle         = {The European Physical Journal B - Condensed Matter and Complex Systems},
  title                = {The use of dynamical networks to detect the hierarchical organization of financial market sectors},
  doi                  = {10.1140/epjb/e2009-00286-0},
  issn                 = {1434-6028},
  number               = {1},
  pages                = {3--11},
  volume               = {73},
  abstract             = {Two kinds of filtered networks: minimum spanning trees (MSTs) and planar maximally filtered graphs (PMFGs) are constructed from dynamical correlations computed over a moving window. We study the evolution over time of both hierarchical and topological properties of these graphs in relation to market fluctuations. We verify that the dynamical PMFG preserves the same hierarchical structure as the dynamical MST, providing in addition a more significant and richer structure, a stronger robustness and dynamical stability. Central and peripheral stocks are differentiated by using a combination of different topological measures. We find stocks well connected and central; stocks well connected but peripheral; stocks poorly connected but central; stocks poorly connected and peripheral. It results that the Financial sector plays a central role in the entire system. The robustness, stability and persistence of these findings are verified by changing the time window and by performing the computations on different time periods. We discuss these results and the economic meaning of this hierarchical positioning.},
  citeulike-article-id = {5519260},
  citeulike-linkout-0  = {http://dx.doi.org/10.1140/epjb/e2009-00286-0},
  citeulike-linkout-1  = {http://www.springerlink.com/content/f4p405558326881j},
  citeulike-linkout-2  = {http://link.springer.com/article/10.1140/epjb/e2009-00286-0},
  day                  = {18},
  groups               = {Networks and investment management},
  owner                = {cristi},
  posted-at            = {2016-09-28 18:42:02},
  publisher            = {Springer-Verlag},
  timestamp            = {2020-02-27 04:31},
}

@Article{Djauhari-Gan-2015,
  author               = {Djauhari, Maman A. and Gan, Siew L.},
  date                 = {2015-02},
  journaltitle         = {Physica A: Statistical Mechanics and its Applications},
  title                = {Optimality problem of network topology in stocks market analysis},
  doi                  = {10.1016/j.physa.2014.09.060},
  issn                 = {0378-4371},
  pages                = {108--114},
  volume               = {419},
  abstract             = {Since its introduction fifteen years ago, minimal spanning tree has become an indispensible tool in econophysics. It is to filter the important economic information contained in a complex system of financial markets' commodities. Here we show that, in general, that tool is not optimal in terms of topological properties. Consequently, the economic interpretation of the filtered information might be misleading. To overcome that non-optimality problem, a set of criteria and a selection procedure of an optimal minimal spanning tree will be developed. By using New York Stock Exchange data, the advantages of the proposed method will be illustrated in terms of the power-law of degree distribution.},
  citeulike-article-id = {14429820},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.physa.2014.09.060},
  posted-at            = {2017-09-13 09:57:44},
  timestamp            = {2020-02-27 04:31},
}

@Article{Djauhari-Gan-2016,
  author               = {Djauhari, Maman A. and Gan, Siew L.},
  date                 = {2016-09},
  journaltitle         = {Journal of Statistical Mechanics: Theory and Experiment},
  title                = {Network topology of economic sectors},
  doi                  = {10.1088/1742-5468/2016/09/093401},
  issn                 = {1742-5468},
  number               = {9},
  pages                = {093401+},
  volume               = {2016},
  abstract             = {A lot of studies dealing with stock network analysis, where each individual stock is represented by a univariate time series of its closing price, have been published. In these studies, the similarity of two different stocks is quantified using a Pearson correlation coefficient on the logarithmic price returns. In this paper, we generalize the notion of similarity between univariate time series into multivariate time series which might be of different dimensions. This allows us to deal with economic sector network analysis, where the similarity between economic sectors is defined using Escoufier's vector correlation RV. To the best of our knowledge, there is no study dealing with this notion of economic sector similarity. Two examples of data from the New York stock exchange will be presented and discussed, and some important results will be highlighted.},
  citeulike-article-id = {14150068},
  citeulike-linkout-0  = {http://dx.doi.org/10.1088/1742-5468/2016/09/093401},
  day                  = {09},
  groups               = {Networks and investment management},
  owner                = {cristi},
  posted-at            = {2016-10-01 20:27:45},
  timestamp            = {2020-02-27 04:31},
}

@Article{Emmons-et-al-2016,
  author               = {Emmons, Scott and Kobourov, Stephen and Gallant, Mike and Borner, Katy},
  date                 = {2016-07-08},
  journaltitle         = {PLOS ONE},
  title                = {Analysis of Network Clustering Algorithms and Cluster Quality Metrics at Scale},
  doi                  = {10.1371/journal.pone.0159161},
  number               = {7},
  pages                = {e0159161+},
  volume               = {11},
  abstract             = {Notions of community quality underlie the clustering of networks. While studies surrounding network clustering are increasingly common, a precise understanding of the realtionship between different cluster quality metrics is unknown. In this paper, we examine the relationship between stand-alone cluster quality metrics and information recovery metrics through a rigorous analysis of four widely-used network clustering algorithms-Louvain, Infomap, label propagation, and smart local moving. We consider the stand-alone quality metrics of modularity, conductance, and coverage, and we consider the information recovery metrics of adjusted Rand score, normalized mutual information, and a variant of normalized mutual information used in previous work. Our study includes both synthetic graphs and empirical data sets of sizes varying from 1,000 to 1,000,000 nodes. We find significant differences among the results of the different cluster quality metrics. For example, clustering algorithms can return a value of 0.4 out of 1 on modularity but score 0 out of 1 on information recovery. We find conductance, though imperfect, to be the stand-alone quality metric that best indicates performance on the information recovery metrics. Additionally, our study shows that the variant of normalized mutual information used in previous work cannot be assumed to differ only slightly from traditional normalized mutual information. Smart local moving is the overall best performing algorithm in our study, but discrepancies between cluster evaluation metrics prevent us from declaring it an absolutely superior algorithm. Interestingly, Louvain performed better than Infomap in nearly all the tests in our study, contradicting the results of previous work in which Infomap was superior to Louvain. We find that although label propagation performs poorly when clusters are less clearly defined, it scales efficiently and accurately to large graphs with well-defined clusters.},
  citeulike-article-id = {14447476},
  citeulike-linkout-0  = {http://dx.doi.org/10.1371/journal.pone.0159161},
  day                  = {8},
  posted-at            = {2017-10-08 14:17:40},
  publisher            = {Public Library of Science},
  timestamp            = {2020-02-27 04:31},
}

@Article{EngUthaiwat-2018,
  author               = {Eng-Uthaiwat, Harnchai},
  date                 = {2018},
  journaltitle         = {Review of Quantitative Finance and Accounting},
  title                = {Stock market return predictability: Does network topology matter?},
  doi                  = {10.1007/s11156-017-0676-3},
  pages                = {1--28},
  abstract             = {This paper provides new evidence for the predictability of excess market portfolio returns using a network approach. In particular, this article introduces a measure of interconnectedness to capture the interrelationship of returns of 100 largest stocks in SandP 500 during 1990-2014. In the financial network literature, the interconnection of a stock network is often regarded as a channel through which an idiosyncratic shock propagates. The idiosyncratic risk propagation is crucial to the debate over the relationship between idiosyncratic risk and market returns because the idiosyncratic risk is not always diversified away. Rather, the network can sometimes amplify the effect of the idiosyncratic risk to cause aggregate fluctuation. In accordance with this theoretical argument, I empirically show that the network topology, measured by diameter, works together with the idiosyncratic risk, measured by average stock variance, to affect the market portfolio returns. This relationship persists after controlling for well-known variables known to forecast the stock market returns.},
  citeulike-article-id = {14501361},
  citeulike-linkout-0  = {http://dx.doi.org/10.1007/s11156-017-0676-3},
  citeulike-linkout-1  = {http://link.springer.com/article/10.1007/s11156-017-0676-3},
  groups               = {Predictability_Return_Other, Predictability_FinInfo, FcstQWIM_Equity},
  posted-at            = {2017-12-12 00:45:29},
  publisher            = {Springer US},
  timestamp            = {2020-02-27 04:31},
}

@Article{Eryigit-Eryigit-2009,
  author               = {Eryigit, Mehmet and Eryigit, Resul},
  date                 = {2009-09},
  journaltitle         = {Physica A: Statistical Mechanics and its Applications},
  title                = {Network structure of cross-correlations among the world market indices},
  doi                  = {10.1016/j.physa.2009.04.028},
  issn                 = {0378-4371},
  number               = {17},
  pages                = {3551--3562},
  volume               = {388},
  abstract             = {We report the results of an investigation of the properties of the networks formed by the cross-correlations of the daily and weekly index changes of 143 stock market indices from 59 different countries. Analysis of the asset graphs, minimum spanning trees (MST) and planar maximally filtered graphs (PMFG) of the afermentioned networks confirms that globalization has been increasing in recent years. North American and European markets are observed to be much more strongly connected among themselves compared to the integration with the other geographical regions. Surprisingly, the integration of East Asian markets among themselves as well as to the Western markets is found to be rather weak. MST and PMFG of both daily and weekly return correlations indicates that the clustering of the indices is mostly geographical. The French fsbf250 index is found to be most important node of the MST and PMFG based on several graph centrality measures.},
  citeulike-article-id = {14150082},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.physa.2009.04.028},
  groups               = {Networks and investment management},
  owner                = {cristi},
  posted-at            = {2016-10-01 20:51:00},
  timestamp            = {2020-02-27 04:31},
}

@Article{Forss-Sarlin-2018,
  author         = {Forss, Thomas and Sarlin, Peter},
  date           = {2018},
  journaltitle   = {The Journal of Network Theory in Finance},
  title          = {News-sentiment networks as a company risk indicator},
  doi            = {10.21314/{JNTF}.2018.039},
  issn           = {2055-7795},
  number         = {1},
  pages          = {65--86},
  url            = {https://www.risk.net/journal-of-network-theory-in-finance/5515601/news-sentiment-networks-as-a-company-risk-indicator},
  urldate        = {2019-05-30},
  volume         = {4},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 04:31},
}

@Article{Fortunato-Hric-2016,
  author               = {Fortunato, Santo and Hric, Darko},
  date                 = {2016-11},
  journaltitle         = {Physics Reports},
  title                = {Community detection in networks: A user guide},
  doi                  = {10.1016/j.physrep.2016.09.002},
  issn                 = {0370-1573},
  pages                = {1--44},
  volume               = {659},
  abstract             = {Community detection in networks is one of the most popular topics of modern network science. Communities, or clusters, are usually groups of vertices having higher probability of being connected to each other than to members of other groups, though other patterns are possible. Identifying communities is an ill-defined problem. There are no universal protocols on the fundamental ingredients, like the definition of community itself, nor on other crucial issues, like the validation of algorithms and the comparison of their performances. This has generated a number of confusions and misconceptions, which undermine the progress in the field. We offer a guided tour through the main aspects of the problem. We also point out strengths and weaknesses of popular methods, and give directions to their use.},
  citeulike-article-id = {14445701},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.physrep.2016.09.002},
  posted-at            = {2017-10-05 12:14:54},
  timestamp            = {2020-02-27 04:31},
}

@Article{Frey-Hledik-2018,
  author         = {Frey, Rudiger and Hledik, Juraj},
  date           = {2018-05-15},
  journaltitle   = {Risks},
  title          = {Diversification and systemic risk: A financial network perspective},
  doi            = {10.3390/risks6020054},
  issn           = {2227-9091},
  number         = {2},
  pages          = {54},
  url            = {http://www.mdpi.com/2227-9091/6/2/54},
  urldate        = {2019-12-03},
  volume         = {6},
  abstract       = {In this paper, we study the implications of diversification in the asset portfolios of banks for financial stability and systemic risk. Adding to the existing literature, we analyse this issue in a network model of the interbank market. We carry out a simulation study that determines the probability of a systemic crisis in the banking network as a function of both the level of diversification, and the connectivity and structure of the financial network. In contrast to earlier studies we find that diversification at the level of individual banks may be beneficial for financial stability even if it does lead to a higher asset return correlation across banks.},
  day            = {15},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 04:31},
}

@Article{Gao-et-al-2017,
  author               = {Gao, Gelin and Mishra, Bud and Ramazzotti, Daniele},
  date                 = {2017-03},
  journaltitle         = {arXiv e-Print},
  title                = {Efficient Simulation of Financial Stress Testing Scenarios with Suppes-Bayes Causal Networks},
  eprint               = {1703.03076},
  eprinttype           = {arXiv},
  url                  = {https://arxiv.org/abs/1703.03076},
  abstract             = {The most recent financial upheavals have cast doubt on the adequacy of some of the conventional quantitative risk management strategies, such as VaR (Value at Risk), in many common situations. Consequently, there has been an increasing need for verisimilar financial stress testings, namely simulating and analyzing financial portfolios in extreme, albeit rare scenarios. Unlike conventional risk management which exploits statistical correlations among financial instruments, here we focus our analysis on the notion of probabilistic causation, which is embodied by Suppes-Bayes Causal Networks (SBCNs), SBCNs are probabilistic graphical models that have many attractive features in terms of more accurate causal analysis for generating financial stress scenarios. In this paper, we present a novel approach for conducting stress testing of financial portfolios based on SBCNs in combination with classical machine learning classification tools. The resulting method is shown to be capable of correctly discovering the causal relationships among financial factors that affect the portfolios and thus, simulating stress testing scenarios with a higher accuracy and lower computational complexity than conventional Monte Carlo Simulations.},
  citeulike-article-id = {14381688},
  citeulike-linkout-0  = {http://arxiv.org/abs/1703.03076},
  citeulike-linkout-1  = {http://arxiv.org/pdf/1703.03076},
  day                  = {8},
  groups               = {Test_Scenario, Proba_Bayes, Scenario_Risk},
  posted-at            = {2017-06-23 20:26:51},
  timestamp            = {2020-02-27 04:31},
}

@Article{Grabocka-SchmidtThieme-2018,
  author         = {Grabocka, Josif and {Schmidt-Thieme}, Lars},
  date           = {2018-12-20},
  journaltitle   = {arXiv e-Print},
  title          = {NeuralWarp: Time-Series Similarity with Warping Networks},
  url            = {https://arxiv.org/abs/1812.08306},
  urldate        = {2019-09-11},
  abstract       = {Research on time-series similarity measures has emphasized the need for elastic methods which align the indices of pairs of time series and a plethora of non-parametric have been proposed for the task. On the other hand, deep learning approaches are dominant in closely related domains, such as learning image and text sentence similarity. In this paper, we propose {NeuralWarp}, a novel measure that models the alignment of time-series indices in a deep representation space, by modeling a warping function as an upper level neural network between deeply-encoded time series values. Experimental results demonstrate that {NeuralWarp} outperforms both non-parametric and un-warped deep models on a range of diverse real-life datasets.},
  day            = {20},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 04:31},
}

@PhdThesis{Greppi-2017,
  author               = {Greppi, Alessandro},
  date                 = {2017},
  institution          = {University of Pavia},
  title                = {Bayesian Networks Models for Equity Market},
  url                  = {https://iris.unipv.it/retrieve/handle/11571/1203358/184948/Bayesian%20Networks%20Models%20for%20Equity%20Markets.pdf},
  abstract             = {we propose in this work an innovative approach that exploits graphical models in order to provide buy or sell indications on the most capitalized equity market in the world: the SandP500. Adopting a model allows us to deal with a complex framework by generating a reliable approximation of the real world. We decided to analyze the SandP500 because its dynamics are influenced by a large amount of variables whose interpretation represents a challenging task for practitioners. Generally, an investor observes the market and then he makes a decision but procedure is generally time consuming. This is why computers and algorithms are spreading in the last years through financial industry with the objective of supporting fund managers and strategists. The aim of this work is to build a model that is able to perform in a mouse-click simulations on alternative market scenarios by exploiting algorithms potential. In order to do that, we use graphical models: Bayesian Networks (BNs) and their extension called Object Oriented Bayesian Networks (OOBNs). The attribute "graphical" means that they can both be represented by a graph, a feature that makes complex frameworks easier to interpret. Furthermore, thanks to Hugin, a software that has been designed to deal exclusively with these models, we can exploit the potentiality of some algorithms that allow us to learn directly from the data the network structure and to define prior probabilities. Thanks to these features, it is possible to observe known or unexpected dependence/independence relations among the variables and to simulate the impact of new information across the network. In summary, we have chosen BNs and OOBNs because graphical models allow showing clearly and intuitively dependence and independence relations. Moreover, they deal efficiently with uncertain situations by exploiting some of the most established probability theories, such as the Bayes' Rule. Furthermore, by using the Hugin software we can exploit the algorithms implemented in it and learn directly from the data the network structure or simulate in real time different scenarios},
  citeulike-article-id = {14514586},
  groups               = {Proba_Bayes},
  posted-at            = {2018-01-10 17:43:15},
  timestamp            = {2020-02-27 04:31},
}

@Article{Hartman-Hlinka-2018,
  author         = {Hartman, David and Hlinka, Jaroslav},
  date           = {2018-08},
  journaltitle   = {Chaos},
  title          = {Nonlinearity in stock networks.},
  doi            = {10.1063/1.5023309},
  number         = {8},
  pages          = {083127},
  volume         = {28},
  abstract       = {Stock networks, constructed from stock price time series, are a well-established tool for the characterization of complex behavior in stock markets. Following Mantegna's seminal paper, the linear Pearson's correlation coefficient between pairs of stocks has been the usual way to determine network edges. Recently, possible effects of nonlinearity on the graph-theoretical properties of such networks have been demonstrated when using nonlinear measures such as mutual information instead of linear correlation. In this paper, we quantitatively characterize the nonlinearity in stock time series and the effect it has on stock network properties. This is achieved by a systematic multi-step approach that allows us to quantify the nonlinearity of coupling; correct its effects wherever it is caused by simple univariate non-Gaussianity; potentially localize in space and time any remaining strong sources of this nonlinearity; and, finally, study the effect nonlinearity has on global network properties. By applying this multi-step approach to stocks included in three prominent indices (New York Stock Exchange 100, Financial Times Stock Exchange 100, and Standard and Poor 500), we establish that the apparent nonlinearity that has been observed is largely due to univariate non-Gaussianity. Furthermore, strong nonstationarity in a few specific stocks may play a role. In particular, the sharp decrease in some stocks during the global financial crisis of 2008 gives rise to apparent nonlinear dependencies among stocks.},
  f1000-projects = {QuantInvest},
  pmid           = {30180637},
  timestamp      = {2020-02-27 04:31},
}

@Article{Hautsch-et-al-2014,
  author               = {Hautsch, Nikolaus and Schaumburg, Julia and Schienle, Melanie},
  date                 = {2014-07},
  journaltitle         = {International Journal of Forecasting},
  title                = {Forecasting systemic impact in financial networks},
  doi                  = {10.1016/j.ijforecast.2013.09.004},
  issn                 = {0169-2070},
  number               = {3},
  pages                = {781--794},
  volume               = {30},
  abstract             = {We propose a methodology for forecasting the systemic impact of financial institutions in interconnected systems. Utilizing a five-year sample including the 2008/9 financial crisis, we demonstrate how the approach can be used for the timely systemic risk monitoring of large European banks and insurance companies. We predict firms' systemic relevance as the marginal impact of individual downside risks on systemic distress. So-called systemic risk betas account for a company's position within the network of financial interdependencies, in addition to its balance sheet characteristics and its exposure to general market conditions. Relying only on publicly available daily market data, we determine time-varying systemic risk networks, and forecast the systemic relevance on a quarterly basis. Our empirical findings reveal time-varying risk channels and firms' specific roles as risk transmitters and/or risk recipients.},
  citeulike-article-id = {14148605},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.ijforecast.2013.09.004},
  owner                = {cristi},
  posted-at            = {2016-09-28 20:06:39},
  timestamp            = {2020-02-27 04:31},
}

@Article{Heiberger-2014,
  author               = {Heiberger, Raphael H.},
  date                 = {2014-01},
  journaltitle         = {Physica A: Statistical Mechanics and its Applications},
  title                = {Stock network stability in times of crisis},
  doi                  = {10.1016/j.physa.2013.08.053},
  issn                 = {0378-4371},
  pages                = {376--381},
  volume               = {393},
  abstract             = {Despite many efforts crises on financial markets are in large part still scientific black-boxes. In this paper, we use a winner-take-all approach to construct a longitudinal network of SandP 500 companies and their correlations between 2000 and 2012. A comparison to complex ecosystems is drawn, especially whether the May-Wigner theorem can describe real-world economic phenomena. The results confirm the utility of the May-Wigner theorem as a stability indicator for the US stock market, since its development matches with the two major crises of this period, the dot-com bubble and, particularly, the financial crisis. In those times of financial turmoil, the stock network changes its composition, but unlike ecological systems it tightens and the disassortative structure of prosperous markets transforms into a more centralized topology.},
  citeulike-article-id = {14461314},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.physa.2013.08.053},
  posted-at            = {2017-10-19 21:11:45},
  timestamp            = {2020-02-27 04:31},
}

@Article{Heiberger-2018,
  author               = {Heiberger, Raphael H.},
  date                 = {2018-01},
  journaltitle         = {Physica A: Statistical Mechanics and its Applications},
  title                = {Predicting economic growth with stock networks},
  doi                  = {10.1016/j.physa.2017.07.022},
  issn                 = {0378-4371},
  pages                = {102--111},
  volume               = {489},
  abstract             = {Networks derived from stock prices are often used to model developments on financial markets and are tightly intertwined with crises. Yet, the influence of changing market topologies on the broader economy (i.e. GDP) is unclear. In this paper, we propose a Bayesian approach that utilizes individual-level network measures of companies as lagged probabilistic features to predict national economic growth. We use a comprehensive data set consisting of Standard and Poor's 500 corporations from January 1988 until October 2016. The final model forecasts correctly all major recession and prosperity phases of the U.S. economy up to one year ahead. By employing different network measures on the level of corporations, we can also identify which companies' stocks possess a key role in a changing economic environment and may be used as indication of critical (and prosperous) developments. More generally, the proposed approach allows to predict probabilities for different overall states of social entities by using local network positions and could be applied on various phenomena.},
  citeulike-article-id = {14461318},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.physa.2017.07.022},
  posted-at            = {2017-10-19 21:14:58},
  timestamp            = {2020-02-27 04:31},
}

@Article{Herskovic-2018,
  author         = {Herskovic, Bernard},
  date           = {2018-08},
  journaltitle   = {The Journal of Finance},
  title          = {Networks in production: asset pricing implications},
  doi            = {10.1111/jofi.12684},
  issn           = {0022-1082},
  number         = {4},
  pages          = {1785--1818},
  urldate        = {2019-09-01},
  volume         = {73},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 04:31},
}

@Article{Huang-Wang-2019,
  author         = {Huang, Wei-Qiang and Wang, Dan},
  date           = {2019-06},
  journaltitle   = {Finance Research Letters},
  title          = {Financial network linkages to predict economic output},
  doi            = {10.1016/j.frl.2019.06.004},
  issn           = {1544-6123},
  url            = {https://linkinghub.elsevier.com/retrieve/pii/S1544612319301746},
  urldate        = {2019-09-01},
  abstract       = {We investigate the impact of financial system on China's economic output from a financial institution tail-event driven networks (TENETs) perspective and forecast future economic growth by ARDL models. We assess five network topological measurements to reflect the change in the financial system, which detect tail risk spillover effects and reflect cross-sectional dimension of systemic risk. Through a study of relationship at different time lags, we find that except for total connectedness, all estimated network topological measurements have long-run positive impacts on economic growth and that test results present a highly accurate forecast for China's economic output using the network linkages.},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 04:31},
}

@Article{Interdonato-et-al-2017,
  author               = {Interdonato, Roberto and Tagarelli, Andrea and Ienco, Dino and Sallaberry, Arnaud and Poncelet, Pascal},
  date                 = {2017},
  journaltitle         = {Data Mining and Knowledge Discovery},
  title                = {Local community detection in multilayer networks},
  doi                  = {10.1007/s10618-017-0525-y},
  number               = {5},
  pages                = {1444--1479},
  volume               = {31},
  abstract             = {The problem of local community detection in graphs refers to the identification of a community that is specific to a query node and relies on limited information about the network structure. Existing approaches for this problem are defined to work in dynamic network scenarios, however they are not designed to deal with complex real-world networks, in which multiple types of connectivity might be considered. In this work, we fill this gap in the literature by introducing the first framework for local community detection in multilayer networks (ML-LCD). We formalize the ML-LCD optimization problem and provide three definitions of the associated objective function, which correspond to different ways to incorporate within-layer and across-layer topological features. We also exploit our framework to generate multilayer global community structures. We conduct an extensive experimentation using seven real-world multilayer networks, which also includes comparison with state-of-the-art methods for single-layer local community detection and for multilayer global community detection. Results show the significance of our proposed methods in discovering local communities over multiple layers, and also highlight their ability in producing global community structures that are better in modularity than those produced by native global community detection approaches.},
  citeulike-article-id = {14449647},
  citeulike-linkout-0  = {http://dx.doi.org/10.1007/s10618-017-0525-y},
  citeulike-linkout-1  = {http://link.springer.com/article/10.1007/s10618-017-0525-y},
  posted-at            = {2017-10-12 23:07:06},
  publisher            = {Springer US},
  timestamp            = {2020-02-27 04:31},
}

@InCollection{Iori-Mantegna-2018,
  author         = {Iori, Giulia and Mantegna, Rosario N.},
  booktitle      = {Handbook of Computational Economics},
  date           = {2018},
  title          = {Empirical analyses of networks in finance},
  doi            = {10.1016/bs.hescom.2018.02.005},
  isbn           = {9780444641311},
  pages          = {637--685},
  publisher      = {Elsevier},
  series         = {Handbook of computational economics},
  url            = {https://www.sciencedirect.com/science/article/pii/S1574002118300054},
  volume         = {4},
  abstract       = {The recent global financial crisis has triggered a huge interest in the use of network concepts and network tools to better understand how instabilities can propagate through the financial system. The literature is today quite vast, covering both theoretical and empirical aspects. This review concentrates on empirical work, and associated methodologies, concerned with the evaluation of the fragility and resilience of financial and credit markets. The first part of the review examines the literature on systemic risk that arise from banks mutual exposures. These exposures stem primarily from interbank lending and derivative positions, but also, indirectly, from common holdings of other asset classes, that can lead to common shocks in instances of fire sales, and from widespread non-performing loans to the real sector during period of economic downturns. We survey (a) studies that characterize the structure of national interbank networks, in some cases using a multiplex representations, (b) studies that introduce novel methods to quantify systemic risk and identify systemically important institutions, such as via stress test scenarios, (c) studies that assess which regulatory measures can help mitigate the propagation of contagion and distress in the financial system, and (d) studies that explore which location advantages may arise from holding privileged positions in the interbank network, such as via preferential lending relationships, or because of occupying a more central node, and if such advantages can provide an early indication of the build up of systemic risk. The second part of the review is dedicated to the analysis of indirect networks, specifically (e) proximity based network, i.e. networks obtained starting from a proximity measure sometime filtered with a network filtering methodology, (f) association network, i.e. networks where a link between two financial actors is set if a statistical test again a null hypothesis is rejected, and (g) statistically validated networks, i.e. event or relationship networks where a subset of links is selected according to a statistical validation associated with the rejection of a random null hypothesis. The need for a joint consideration of direct and indirect channels of contagion is briefly discussed.},
  f1000-projects = {QuantInvest},
  groups         = {Test_MultiHypotheses, Invest_Network},
  issn           = {1574-0021},
  timestamp      = {2020-02-27 04:31},
}

@Article{Jager-et-al-2017,
  author               = {Jager, Georg and Hofer, Christian and Kapeller, Marie and Fullsack, Manfred},
  date                 = {2017-12-18},
  journaltitle         = {PLOS ONE},
  title                = {Hidden early-warning signals in scale-free networks},
  doi                  = {10.1371/journal.pone.0189853},
  number               = {12},
  pages                = {e0189853+},
  volume               = {12},
  abstract             = {Critical transitions of complex systems can often be predicted by so-called early-warning signals (EWS). In some cases, however, such signals cannot be detected although a critical transition is imminent. Observing a relation of EWS-detectability and the network topology in which the system is implemented, we simulate and investigate scale-free networks and identify which networks show, and which do not show EWS in the framework of a two state system that exhibits critical transitions. Additionally, we adapt our approach by examining the effective state of the system, rather than its natural state, and conclude that this transformation can reveal hidden EWS in networks where those signals are otherwise obscured by a complex topology.},
  citeulike-article-id = {14527024},
  citeulike-linkout-0  = {http://dx.doi.org/10.1371/journal.pone.0189853},
  day                  = {18},
  posted-at            = {2018-01-30 16:50:21},
  publisher            = {Public Library of Science},
  timestamp            = {2020-02-27 04:36},
}

@Article{Jalili-Perc-2017,
  author               = {Jalili, Mahdi and Perc, Matjaz},
  date                 = {2017-07},
  journaltitle         = {Journal of Complex Networks},
  title                = {Information cascades in complex networks},
  doi                  = {10.1093/comnet/cnx019},
  issn                 = {2051-1310},
  abstract             = {Information cascades are important dynamical processes in complex networks. An information cascade can describe the spreading dynamics of rumour, disease, memes, or marketing campaigns, which initially start from a node or a set of nodes in the network. If conditions are right, information cascades rapidly encompass large parts of the network, thus leading to epidemics or epidemic spreading. Certain network topologies are particularly conducive to epidemics, while others decelerate and even prohibit rapid information spreading. Here we review models that describe information cascades in complex networks, with an emphasis on the role and consequences of node centrality. In particular, we present simulation results on sample networks that reveal just how relevant the centrality of initiator nodes is on the latter development of an information cascade, and we define the spreading influence of a node as the fraction of nodes that is activated as a result of the initial activation of that node. A systemic review of existing results shows that some centrality measures, such as the degree and betweenness, are positively correlated with the spreading influence, while other centrality measures, such as eccentricity and the information index, have negative correlation. A positive correlation implies that choosing a node with the highest centrality value will activate the largest number of nodes, while a negative correlation implies that the node with the lowest centrality value will have the same effect. We discuss possible applications of these results, and we emphasize how information cascades can help us identify nodes with the highest spreading capability in complex networks.},
  citeulike-article-id = {14461257},
  citeulike-linkout-0  = {http://dx.doi.org/10.1093/comnet/cnx019},
  day                  = {06},
  posted-at            = {2017-10-19 17:28:36},
  timestamp            = {2020-02-27 04:36},
}

@InCollection{Kalygin-et-al-2014a,
  author               = {Kalygin, ValeryA and Koldanov, AlexanderP and Pardalos, PanosM},
  booktitle            = {Learning and Intelligent Optimization},
  date                 = {2014},
  title                = {A General Approach to Network Analysis of Statistical Data Sets},
  doi                  = {10.1007/978-3-319-09584-4\_10},
  editor               = {Pardalos, Panos M. and Resende, Mauricio G. C. and Vogiatzis, Chrysafis and Walteros, Jose L.},
  pages                = {88--97},
  publisher            = {Springer International Publishing},
  series               = {Lecture Notes in Computer Science},
  volume               = {8426},
  abstract             = {The main goal of the present paper is the development of general approach to network analysis of statistical data sets. First a general method of market network construction is proposed on the base of idea of measures of association. It is noted that many existing network models can be obtained as a particular case of this method. Next it is shown that statistical multiple decision theory is an appropriate theoretical basis for market network analysis of statistical data sets. Finally conditional risk for multiple decision statistical procedures is introduced as a natural measure of quality in market network analysis. Some illustrative examples are given.},
  citeulike-article-id = {14292072},
  citeulike-linkout-0  = {http://dx.doi.org/10.1007/978-3-319-09584-410},
  citeulike-linkout-1  = {http://link.springer.com/chapter/10.1007/978-3-319-09584-410},
  groups               = {Networks and investment management},
  posted-at            = {2017-03-05 18:02:25},
  timestamp            = {2020-02-27 04:36},
}

@Article{Kenett-Havlin-2014,
  author               = {Kenett, DrorY and Havlin, Shlomo},
  date                 = {2015},
  journaltitle         = {Mind and Society},
  title                = {Network science: a useful tool in economics and finance},
  doi                  = {10.1007/s11299-015-0167-y},
  number               = {2},
  pages                = {155--167},
  volume               = {14},
  abstract             = {The increasing frequency and scope of financial crises has made global financial stability one of the major concerns of economic policy and decision makers. Under this highly complex environment, supervision of the financial system has to be thought of as a systemic task, focusing not only on the strength of the institutions but also on the interdependent relations among them, unraveling the structure and dynamic of the system as a whole. In recent years, network science has emerged as a leading tool for the investigation of complex systems. Here we review several applications of network science in finance and economics, and discuss existing challenges and future directions which will substantiate network science as a key tool for financial academics, practitioners, and policy and decision makers.},
  citeulike-article-id = {14292069},
  citeulike-linkout-0  = {http://dx.doi.org/10.1007/s11299-015-0167-y},
  citeulike-linkout-1  = {http://link.springer.com/article/10.1007/s11299-015-0167-y},
  groups               = {Networks and investment management},
  posted-at            = {2017-03-05 17:58:13},
  publisher            = {Springer Berlin Heidelberg},
  timestamp            = {2020-02-27 04:36},
}

@Article{Khan-Niazi-2017,
  author               = {Khan, Bisma S. and Niazi, Muaz A.},
  date                 = {2017-08},
  journaltitle         = {SSRN e-Print},
  title                = {Network Community Detection: A Review and Visual Survey},
  eprint               = {1708.00977},
  eprinttype           = {arXiv},
  url                  = {https://arxiv.org/abs/1708.00977},
  abstract             = {Community structure is an important area of research. It has received a considerable attention from the scientific community. Despite its importance, one of the key problems in locating information about community detection is the diverse spread of related articles across various disciplines. To the best of our knowledge, there is no current comprehensive review of recent literature which uses a scientometric analysis using complex networks analysis covering all relevant articles from the Web of Science (WoS). Here we present a visual survey of key literature using CiteSpace. The idea is to identify emerging trends besides using network techniques to examine the evolution of the domain. Towards that end, we identify the most influential, central, as well as active nodes using scientometric analyses. We examine authors, key articles, cited references, core subject categories, key journals, institutions, as well as countries. The exploration of the scientometric literature of the domain reveals that Yong Wang is a pivot node with the highest centrality. Additionally, we have observed that Mark Newman is the most highly cited author in the network. We have also identified that the journal, "Reviews of Modern Physics" has the strongest citation burst. In terms of cited documents, an article by Andrea Lancichinetti has the highest centrality score. We have also discovered that the origin of the key publications in this domain is from the United States. Whereas Scotland has the strongest and longest citation burst. Additionally, we have found that the categories of "Computer Science" and "Engineering" lead other categories based on frequency and centrality respectively.},
  citeulike-article-id = {14445689},
  citeulike-linkout-0  = {http://arxiv.org/abs/1708.00977},
  citeulike-linkout-1  = {http://arxiv.org/pdf/1708.00977},
  day                  = {3},
  posted-at            = {2017-10-05 11:54:05},
  timestamp            = {2020-02-27 04:36},
}

@Article{Kim-Sayama-2017,
  author               = {Kim, Minjun and Sayama, Hiroki},
  date                 = {2017-10},
  journaltitle         = {Applied Network Science},
  title                = {Predicting stock market movements using network science: an information theoretic approach},
  doi                  = {10.1007/s41109-017-0055-y},
  issn                 = {2364-8228},
  number               = {1},
  volume               = {2},
  abstract             = {A stock market is considered as one of the highly complex systems, which consists of many components whose prices move up and down without having a clear pattern. The complex nature of a stock market challenges us on making a reliable prediction of its future movements. In this paper, we aim at building a new method to forecast the future movements of Standard and Poor's 500 Index (SandP 500) by constructing time-series complex networks of SandP 500 underlying companies by connecting them with links whose weights are given by the mutual information of 60-minute price movements of the pairs of the companies with the consecutive 5,340 minutes price records. We showed that the changes in the strength distributions of the networks provide an important information on the network's future movements. We built several metrics using the strength distributions and network measurements such as centrality, and we combined the best two predictors by performing a linear combination. We found that the combined predictor and the changes in SandP 500 show a quadratic relationship, and it allows us to predict the amplitude of the one step future change in SandP 500. The result showed significant fluctuations in SandP 500 Index when the combined predictor was high. In terms of making the actual index predictions, we built ARIMA models. We found that adding the network measurements into the ARIMA models improves the model accuracy. These findings are useful for financial market policy makers as an indicator based on which they can interfere with the markets before the markets make a drastic change, and for quantitative investors to improve their forecasting models.},
  citeulike-article-id = {14461313},
  citeulike-linkout-0  = {http://dx.doi.org/10.1007/s41109-017-0055-y},
  day                  = {10},
  posted-at            = {2017-10-19 21:07:41},
  timestamp            = {2020-02-27 04:36},
}

@Article{Kocheturov-et-al-2014,
  author               = {Kocheturov, Anton and Batsyn, Mikhail and Pardalos, Panos M.},
  date                 = {2014-11},
  journaltitle         = {Physica A: Statistical Mechanics and its Applications},
  title                = {Dynamics of cluster structures in a financial market network},
  doi                  = {10.1016/j.physa.2014.06.077},
  issn                 = {0378-4371},
  pages                = {523--533},
  volume               = {413},
  abstract             = {In the course of recent fifteen years the network analysis has become a powerful tool for studying financial markets. In this work we analyze stock markets of the USA and Sweden. We study cluster structures of a market network constructed from a correlation matrix of returns of the stocks traded in each of these markets. Such cluster structures are obtained by means of the P-Median Problem (PMP) whose objective is to maximize the total correlation between a set of stocks called medians of size pp and other stocks. Every cluster structure is an undirected disconnected weighted graph in which every connected component (cluster) is a star, or a tree with one central node (called a median) and several leaf nodes connected with the median by weighted edges. Our main observation is that in non-crisis periods of time cluster structures change more chaotically, while during crises they show more stable behavior and fewer changes. Thus an increasing stability of a market graph cluster structure obtained via the PMP could be used as an indicator of a coming crisis.},
  citeulike-article-id = {14357951},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.physa.2014.06.077},
  groups               = {Clustering and network analysis},
  keywords             = {pdf},
  posted-at            = {2017-05-16 14:37:14},
  timestamp            = {2020-02-27 04:36},
}

@Article{Konstantinov-Rebmann-2019,
  author         = {Gueorgui Konstantinov and Jonas Rebmann},
  date           = {2019-08-01},
  journaltitle   = {The Journal of Financial Data Science},
  title          = {From risk factors to networks: A case study on interconnectedness using currency funds},
  doi            = {10.3905/jfds.2019.1.3.108},
  issn           = {2640-3951},
  url            = {https://jfds.pm-research.com/content/1/3/108},
  urldate        = {2019-09-06},
  abstract       = {In this article, the authors introduce a combined approach for investigating currency funds by using methods from network science and risk factor analysis. They document a positive relationship between currency funds style exposure, fund age, size, and connectedness, providing both economically and statistically significant results. The most important funds in the network can influence the currency market with significant exposure to the risk factors carry, value, and trend. In general, the authors approach helps investors to identify market interconnectedness; shows how risk can be transmitted; and highlights the factors that could represent significant idiosyncratic, systematic, and systemic economic risk. The authors argue that the interconnectedness is asymmetrical and the network is reciprocal. There are funds with significant importance scores. They provide a framework for practical implementation.},
  day            = {1},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 04:36},
}

@Article{Kuismin-Sillanpaa-2017,
  author               = {Kuismin, Markku O. and Sillanpaa, Mikko J.},
  date                 = {2017-11},
  journaltitle         = {Wiley Interdisciplinary Reviews: Computational Statistics},
  title                = {Estimation of covariance and precision matrix, network structure, and a view toward systems biology},
  doi                  = {10.1002/wics.1415},
  issn                 = {1939-5108},
  number               = {6},
  pages                = {e1415+},
  volume               = {9},
  abstract             = {Covariance matrix and its inverse, known as the precision matrix, have many applications in multivariate analysis because their elements can exhibit the variance, correlation, covariance, and conditional independence between variables. The practice of estimating the precision matrix directly without involving any matrix inversion has obtained significant attention in the literature. We review the methods that have been implemented in R and their R packages, particularly when there are more variables than data samples and discuss ideas behind them. We describe how sparse precision matrix estimation methods can be used to infer network structure. Finally, we discuss methods that are suitable for gene coexpression network construction.},
  citeulike-article-id = {14461340},
  citeulike-linkout-0  = {http://dx.doi.org/10.1002/wics.1415},
  posted-at            = {2017-10-20 00:25:10},
  timestamp            = {2020-02-27 04:36},
}

@Article{Kumar-Deo-2012,
  author               = {Kumar, Sunil and Deo, Nivedita},
  date                 = {2012-08},
  journaltitle         = {Physical review. E, Statistical, nonlinear, and soft matter physics},
  title                = {Correlation and network analysis of global financial indices.},
  issn                 = {1550-2376},
  number               = {2 Pt 2},
  volume               = {86},
  abstract             = {Random matrix theory (RMT) and network methods are applied to investigate the correlation and network properties of 20 financial indices. The results are compared before and during the financial crisis of 2008. In the RMT method, the components of eigenvectors corresponding to the second largest eigenvalue form two clusters of indices in the positive and negative directions. The components of these two clusters switch in opposite directions during the crisis. The network analysis uses the Fruchterman-Reingold layout to find clusters in the network of indices at different thresholds. At a threshold of 0.6, before the crisis, financial indices corresponding to the Americas, Europe, and Asia-Pacific form separate clusters. On the other hand, during the crisis at the same threshold, the American and European indices combine together to form a strongly linked cluster while the Asia-Pacific indices form a separate weakly linked cluster. If the value of the threshold is further increased to 0.9 then the European indices (France, Germany, and the United Kingdom) are found to be the most tightly linked indices. The structure of the minimum spanning tree of financial indices is more starlike before the crisis and it changes to become more chainlike during the crisis. The average linkage hierarchical clustering algorithm is used to find a clearer cluster structure in the network of financial indices. The cophenetic correlation coefficients are calculated and found to increase significantly, which indicates that the hierarchy increases during the financial crisis. These results show that there is substantial change in the structure of the organization of financial indices during a financial crisis.},
  citeulike-article-id = {14150006},
  citeulike-linkout-0  = {http://view.ncbi.nlm.nih.gov/pubmed/23005819},
  citeulike-linkout-1  = {http://www.hubmed.org/display.cgi?uids=23005819},
  groups               = {Networks and investment management, Clustering and network analysis, Invest_Network},
  owner                = {cristi},
  pmid                 = {23005819},
  posted-at            = {2016-10-01 16:43:16},
  timestamp            = {2020-02-27 04:36},
}

@Article{Kumar-et-al-2020,
  author         = {Kumar, Sudarshan and Di Matteo, Tiziana and Chakrabarti, Anindya S.},
  date           = {2020-01-06},
  journaltitle   = {arXiv e-Print},
  title          = {Disentangling shock diffusion on complex networks: Identification through graph planarity},
  url            = {https://arxiv.org/abs/2001.01518},
  urldate        = {2020-01-23},
  abstract       = {Large scale networks delineating collective dynamics often exhibit cascading failures across nodes leading to a system-wide collapse. Prominent examples of such phenomena would include collapse on financial and economic networks. Intertwined nature of the dynamics of nodes in such network makes it difficult to disentangle the source and destination of a shock that percolates through the network, a property known as reflexivity. In this article, a novel methodology is proposed which combines vector autoregression model with an unique identification restrictions obtained from the topological structure of the network to uniquely characterize cascades. In particular, we show that planarity of the network allows us to statistically estimate a dynamical process consistent with the observed network and thereby uniquely identify a path for shock propagation from any chosen epicenter to all other nodes in the network. We analyze the distress propagation mechanism in closed loops giving rise to a detailed picture of the effect of feedback loops in transmitting shocks. We show usefulness and applications of the algorithm in two networks with dynamics at different time-scales: worldwide GDP growth network and stock network. In both cases, we observe that the model predicts the impact of the shocks emanating from the US would be concentrated within the cluster of developed countries and the developing countries show very muted response, which is consistent with empirical observations over the past decade.},
  day            = {6},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 04:36},
}

@Article{Lacasa-et-al-2015,
  author               = {Lacasa, Lucas and Nicosia, Vincenzo and Latora, Vito},
  date                 = {2015-10},
  journaltitle         = {Scientific Reports},
  title                = {Network structure of multivariate time series},
  doi                  = {10.1038/srep15508},
  issn                 = {2045-2322},
  pages                = {15508+},
  volume               = {5},
  abstract             = {Our understanding of a variety of phenomena in physics, biology and economics crucially depends on the analysis of multivariate time series. While a wide range tools and techniques for time series analysis already exist, the increasing availability of massive data structures calls for new approaches for multidimensional signal processing. We present here a non-parametric method to analyse multivariate time series, based on the mapping of a multidimensional time series into a multilayer network, which allows to extract information on a high dimensional dynamical system through the analysis of the structure of the associated multiplex network. The method is simple to implement, general, scalable, does not require ad hoc phase space partitioning, and is thus suitable for the analysis of large, heterogeneous and non-stationary time series. We show that simple structural descriptors of the associated multiplex networks allow to extract and quantify nontrivial properties of coupled chaotic maps, including the transition between different dynamical phases and the onset of various types of synchronization. As a concrete example we then study financial time series, showing that a multiplex network analysis can efficiently discriminate crises from periods of financial stability, where standard methods based on time-series symbolization often fail.},
  citeulike-article-id = {14149946},
  citeulike-linkout-0  = {http://dx.doi.org/10.1038/srep15508},
  day                  = {21},
  groups               = {Networks and investment management, NonStatry_FinTimeSrs},
  owner                = {cristi},
  posted-at            = {2016-10-01 13:48:37},
  timestamp            = {2020-02-27 04:36},
}

@Article{Lawyer-2015,
  author               = {Lawyer, Glenn},
  date                 = {2015-03},
  journaltitle         = {Scientific Reports},
  title                = {Understanding the influence of all nodes in a network},
  doi                  = {10.1038/srep08665},
  issn                 = {2045-2322},
  number               = {1},
  pages                = {8665+},
  volume               = {5},
  abstract             = {Centrality measures such as the degree, k-shell, or eigenvalue centrality can identify a network's most influential nodes, but are rarely usefully accurate in quantifying the spreading power of the vast majority of nodes which are not highly influential. The spreading power of all network nodes is better explained by considering, from a continuous-time epidemiological perspective, the distribution of the force of infection each node generates. The resulting metric, the expected force, accurately quantifies node spreading power under all primary epidemiological models across a wide range of archetypical human contact networks. When node power is low, influence is a function of neighbor degree. As power increases, a node's own degree becomes more important. The strength of this relationship is modulated by network structure, being more pronounced in narrow, dense networks typical of social networking and weakening in broader, looser association networks such as the Internet. The expected force can be computed independently for individual nodes, making it applicable for networks whose adjacency matrix is dynamic, not well specified, or overwhelmingly large.},
  citeulike-article-id = {13546139},
  citeulike-linkout-0  = {http://dx.doi.org/10.1038/srep08665},
  day                  = {2},
  posted-at            = {2017-10-19 16:09:21},
  timestamp            = {2020-02-27 04:36},
}

@InCollection{Lee-Djauhari-2013,
  author               = {Lee, Gan S. and Djauhari, Maman A.},
  booktitle            = {AIP Conference Proceedings},
  date                 = {2013},
  title                = {Multidimensional stock network analysis: An Escoufier's RV coefficient approach},
  doi                  = {10.1063/1.4823975},
  location             = {Kuala Lumpur, Malaysia},
  pages                = {550--555},
  abstract             = {The current practice of stocks network analysis is based on the assumption that the time series of closed stock price could represent the behaviour of the each stock. This assumption leads to consider minimal spanning tree (MST) and sub-dominant ultrametric (SDU) as an indispensible tool to filter the economic information contained in the network. Recently, there is an attempt where researchers represent stock not only as a univariate time series of closed price but as a bivariate time series of closed price and volume. In this case, they developed the so-called multidimensional MST to filter the important economic information. However, in this paper, we show that their approach is only applicable for that bivariate time series only. This leads us to introduce a new methodology to construct MST where each stock is represented by a multivariate time series. An example of Malaysian stock exchange will be presented and discussed to illustrate the advantages of the method},
  citeulike-article-id = {14461295},
  citeulike-linkout-0  = {http://dx.doi.org/10.1063/1.4823975},
  posted-at            = {2017-10-19 19:51:21},
  timestamp            = {2020-02-27 04:36},
}

@Article{Lee-et-al-2019b,
  author         = {Lee, Tae Kyun and Cho, Joon Hyung and Kwon, Deuk Sin and Sohn, So Young},
  date           = {2019-03},
  journaltitle   = {Expert systems with applications},
  title          = {Global stock market investment strategies based on financial network indicators using machine learning techniques},
  doi            = {10.1016/j.eswa.2018.09.005},
  issn           = {0957-4174},
  pages          = {228--242},
  volume         = {117},
  abstract       = {Abstract This study presents financial network indicators that can be applied to global stock market investment strategies. We propose to design both undirected and directed volatility networks of global stock market based on simple pair-wise correlation and system-wide connectedness of national stock indices using a vector auto-regressive model. We examine the effect and usefulness of network indicators by applying them as inputs for determining strategies via several machine learning approaches (logistic regression, support vector machine, and random forest). Two strategies are constructed considering stock price indices: (1) global stock market prediction strategy and (2) regional allocation strategy for developed market/emerging market. According to the results of the performance analysis, network indicators were proven to be important supplementary indicators in predicting global stock market and regional relative directions (up/down). In particular, these indicators were more effective during market crisis periods. This study is the first attempt to construct strategies for global portfolio management using financial network indicators and to suggest how network indicators can be used in practical fields.},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 04:36},
}

@Article{Lee-Woo-2013,
  author               = {Lee, Yun-Jung and Woo, Gyun},
  date                 = {2013-11},
  journaltitle         = {The Journal of the Korea Contents Association},
  title                = {Analysis of the Stock Market Network for Portfolio Recommendation},
  doi                  = {10.5392/jkca.2013.13.11.048},
  issn                 = {1598-4877},
  number               = {11},
  pages                = {48--58},
  volume               = {13},
  abstract             = {The stock market is constantly changing and sometimes a slump or a sudden rising in stocks happens without any special reason. So the stock market is recognized as a complex system and it is hard to predict the change on stock prices. In this paper we consider the stock market to a network consisting of stocks. We analyzed the dynamics of the Korean stock market network and evaluated the changing of the correlation between shares consisting of the time series data of 137 companies belong to KOSPI200. Our analysis shows that the stock prices tend to plummet when the correlation between stocks is very high. We propose a method for recommending the stock portfolio based on the analysis of the stock market network. To show the effectiveness of the recommended portfolio, we conducted the simulated stock investment and compared the recommended portfolio with the efficient portfolio proposed Markowitz. According to the experiment results, the rate of return of the portfolio is about 10.6\% which is about 3.7\% and 5.6\% higher than the average rate of return of the efficient portfolio and KOSPI200 respectively.},
  citeulike-article-id = {14461307},
  citeulike-linkout-0  = {http://dx.doi.org/10.5392/jkca.2013.13.11.048},
  day                  = {28},
  groups               = {Invest_Network},
  posted-at            = {2017-10-19 20:57:30},
  timestamp            = {2020-02-27 04:36},
}

@Article{Leibon-et-al-2008,
  author         = {Leibon, Gregory and Pauls, Scott and Rockmore, Daniel and Savell, Robert},
  date           = {2008-12-30},
  journaltitle   = {Proceedings of the National Academy of Sciences},
  title          = {Topological structures in the equities market network},
  url            = {https://www.pnas.org/content/105/52/20589},
  urldate        = {2020-01-16},
  day            = {30},
  f1000-projects = {QuantInvest},
  publisher      = {National Academy of Sciences},
  timestamp      = {2020-02-27 04:36},
}

@InCollection{Leone-et-al-2018,
  author         = {Leone, Andre and Tomasini, Marcello and Al Rozz, Younis and Menezes, Ronaldo},
  booktitle      = {Complex networks \& their applications VI},
  date           = {2018},
  title          = {On the Performance of Network Science Metrics as Long-Term Investment Strategies in Stock Markets},
  doi            = {10.1007/978-3-319-72150-7\_85},
  editor         = {Cherifi, Chantal and Cherifi, Hocine and Karsai, Marton and Musolesi, Mirco},
  isbn           = {978-3-319-72149-1},
  pages          = {1053--1064},
  publisher      = {Springer International Publishing},
  series         = {Studies in computational intelligence},
  volume         = {689},
  abstract       = {Firms and individuals have always searched for investment strategies that perform well and are robust to market variations. Over the years, many strategies have claimed to be effective but few resist the effect of time, that is, most of them become outdated. It turns out that markets have a -correcting ability; the secretive/novel nature of strategies firms employ cannot win forever; other firms eventually implement competing strategies causing the market to adjust. Nowadays, most investment firms to their clients two approaches: high reward and low reward. Unfortunately the possibility of high reward is generally coupled with low robustness (volatility) and if one wants high robustness the yields are low (low reward). In this paper, we use an approach based on network characteristics extracted from historical market data. Network Science has argued that all complex systems have an underlying network structure that explains the behavior of the system. With this in mind, we propose a long-term investment strategy that builds a network from historical investment data, and considers the current state of this network to decide how to create portfolios. We argue that our approach performs better than standard long-term approaches.},
  f1000-projects = {QuantInvest},
  issn           = {1860-{949X}},
  timestamp      = {2020-02-27 04:36},
}

@InCollection{Li-et-al-2016g,
  author               = {Li, Shutian and Yang, Yi and Li, Caihong and Li, Lian and Gui, Xiangquan},
  booktitle            = {6th International Conference on Electronics Information and Emergency Communication (ICEIEC)},
  date                 = {2016-06},
  title                = {Stock correlation analysis based on complex network},
  doi                  = {10.1109/iceiec.2016.7589713},
  isbn                 = {978-1-5090-1997-7},
  location             = {Beijing, China},
  pages                = {174--177},
  publisher            = {IEEE},
  abstract             = {Aim at the study of the relationship of correlation between the stocks of SSE(Shanghai Stock Exchange) companies and the movement of SSE complex index, by using the mathematical statistics method and correlation coefficient analysis, select each stock's daily closing price as object, establish the complex network of stocks. Through preprocessing the data sets, we used R Studio get the mean correlation coefficient matrix, which is the foundation of the financial network. The results confirm the utility of the correlation coefficient analysis as a stability indicator for SSE stock market, since it proves that the correlation of stocks is different according to the crisis and prosperous of stock market in times. Its development is opposite to the market index, the interaction between the stocks attenuates when the complex index rises and enhances when the index falls. We also used K-means clustering algorithm to classify these nodes into different communities and found that the structures of networks vary widely according to their correlations. The method of this paper and the model it proposed is not only for the selection of our data sets, but also can be generalized to other fields of research.},
  citeulike-article-id = {14438477},
  citeulike-linkout-0  = {http://dx.doi.org/10.1109/iceiec.2016.7589713},
  posted-at            = {2017-09-26 20:51:04},
  timestamp            = {2020-02-27 04:36},
}

@Article{Li-et-al-2016l,
  author         = {Li, Tianxi and Levina, Elizaveta and Zhu, Ji},
  date           = {2016-02-03},
  journaltitle   = {arXiv e-Print},
  title          = {Prediction models for network-linked data},
  url            = {https://arxiv.org/abs/1602.01192},
  abstract       = {Prediction algorithms typically assume the training data are independent samples, but in many modern applications samples come from individuals connected by a network. For example, in adolescent health studies of risk-taking behaviors, information on the subjects' social network is often available and plays an important role through network cohesion, the empirically observed phenomenon of friends behaving similarly. Taking cohesion into account in prediction models should allow us to improve their performance. Here we propose a network-based penalty on individual node effects to encourage similarity between predictions for linked nodes, and show that incorporating it into prediction leads to improvement over traditional models both theoretically and empirically when network cohesion is present. The penalty can be used with many loss-based prediction methods, such as regression, generalized linear models, and Cox's proportional hazard model. Applications to predicting levels of recreational activity and marijuana usage among teenagers from the AddHealth study based on both demographic covariates and friendship networks are discussed in detail and show that our approach to taking friendships into account can significantly improve predictions of behavior while providing interpretable estimates of covariate effects.},
  day            = {3},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 04:36},
}

@Article{Li-et-al-2016m,
  author         = {Li, Tianxi and Levina, Elizaveta and Zhu, Ji},
  date           = {2016-12-14},
  journaltitle   = {arXiv e-Print},
  title          = {Network cross-validation by edge sampling},
  url            = {https://arxiv.org/abs/1612.04717},
  abstract       = {While many statistical models and methods are now available for network analysis, resampling network data remains a challenging problem. Cross-validation is a useful general tool for model selection and parameter tuning, but is not directly applicable to networks since splitting network nodes into groups requires deleting edges and destroys some of the network structure. Here we propose a new network resampling strategy based on splitting edges rather than nodes, applicable to both cross-validation and bootstrap for a wide range of network model selection tasks. We provide a theoretical justification for our method in a general setting and examples of how our method can be used in specific network model selection and parameter tuning tasks. Numerical results on simulated networks and on a citation network of statisticians show that this cross-validation approach works well for model selection.},
  day            = {14},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 04:36},
}

@Article{Li-et-al-2017b,
  author               = {Li, Wenzhe and Guo, Dong and Steeg, Greg V. and Galstyan, Aram},
  date                 = {2017-10},
  journaltitle         = {SSRN e-Print},
  title                = {Unifying Local and Global Change Detection in Dynamic Networks},
  eprint               = {1710.03035},
  eprinttype           = {arXiv},
  url                  = {https://arxiv.org/abs/1710.03035},
  abstract             = {Many real-world networks are complex dynamical systems, where both local (e.g., changing node attributes) and global (e.g., changing network topology) processes unfold over time. Local dynamics may provoke global changes in the network, and the ability to detect such effects could have profound implications for a number of real-world problems. Most existing techniques focus individually on either local or global aspects of the problem or treat the two in isolation from each other. In this paper we propose a novel network model that simultaneously accounts for both local and global dynamics. To the best of our knowledge, this is the first attempt at modeling and detecting local and global change points on dynamic networks via a unified generative framework. Our model is built upon the popular mixed membership stochastic blockmodels (MMSB) with sparse co-evolving patterns. We derive an efficient stochastic gradient Langevin dynamics (SGLD) sampler for our proposed model, which allows it to scale to potentially very large networks. Finally, we validate our model on both synthetic and real-world data and demonstrate its superiority over several baselines.},
  citeulike-article-id = {14460117},
  citeulike-linkout-0  = {http://arxiv.org/abs/1710.03035},
  citeulike-linkout-1  = {http://arxiv.org/pdf/1710.03035},
  day                  = {9},
  groups               = {ChngPoints_TimeSrs},
  posted-at            = {2017-10-18 00:36:07},
  timestamp            = {2020-02-27 04:36},
}

@Article{Li-et-al-2019d,
  author         = {Li, Yan and Jiang, Xiong-Fei and Tian, Yue and Li, Sai-Ping and Zheng, Bo},
  date           = {2019-02},
  journaltitle   = {Physica A: Statistical Mechanics and its Applications},
  title          = {Portfolio optimization based on network topology},
  doi            = {10.1016/j.physa.2018.10.014},
  issn           = {0378-4371},
  pages          = {671--681},
  url            = {https://linkinghub.elsevier.com/retrieve/pii/S0378437118313529},
  urldate        = {2019-04-19},
  volume         = {515},
  abstract       = {The structure and dynamics of complex network systems are of current research interest. We illustrate the dependency between the network topology and its function, considering the complex financial network as a typical example. The networks are built from the full cross-correlation matrix and the global-motion one respectively, aiming at filtering the noise interference of the dynamic networks and understanding the driving mechanism of different interactions. Dynamic structural features of the core and periphery nodes are investigated, and it is demonstrated that the peripherality in a network can be used as an indicator for identifying the optimal assets. With the network filtering approach and peripherality measure, portfolios with different performances are constructed. Compared to the full cross-correlation matrix, the global-motion one shows significant advantages in the portfolio optimization, and the underlying mechanism is carefully analyzed. These methods are also with potential significance to the understanding of other social, biological and transport systems.},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 04:36},
}

@Article{Lin-Guo-2019,
  author         = {Lin, Li and Guo, Xin-Yu},
  date           = {2019},
  journaltitle   = {SSRN e-Print},
  title          = {Identifying Fragility for the Stock Market: Perspective from the Portfolio Overlaps Network},
  doi            = {10.2139/ssrn.3329291},
  issn           = {1556-5068},
  url            = {https://www.ssrn.com/abstract=3329291},
  urldate        = {2019-09-22},
  abstract       = {In a financial system, the interconnectedness among entities from investing in common assets (portfolio overlaps) is considered an important channel for the propagation of systemic risk because this interconnectedness can facilitate the contagion of fire sales and lead to widespread sales as well as spiral devaluation of assets. This mechanism also applies to the stock market. Particularly, it is responsible for the presence of fragility, which is defined as the potential global instability of stock prices when confronted by a local negative shock. Thus, we propose to quantitatively explore the temporal pattern of the topological features of the portfolio overlaps network (PON) to identify fragility, with the aim of either capturing a precursor for imminent market crashes or confirming further downward falls during market turmoil. Here, PON is the projected monopartite network used to characterise the mutual overlaps between funds pairs, which is induced from the bipartite network of stocks-funds ownerships. Further, we adopt a solid statistical procedure to construct the validated PON by filtering out mildly risky connections that are not statistically robust. Based on an empirical investigation of the Chinese stock market from 2005--2018 that witnessed collective unquenchable price slumps, we found that the topology of the validated PON can indeed provide some sensible indicators for identifying market fragility. The results suggest that (i) the average closeness centrality and the critical value of effective infection rate could serve as leading indicators of impending market turmoil, and (ii) the simple degree-based measure for nodes and relative frequency measures for links are actual synchronous indicators that could also explain the source of fragility.},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 04:36},
}

@Article{Liu-Deng-2019,
  author         = {Liu, Fan and Deng, Yong},
  date           = {2019},
  journaltitle   = {IEEE access : practical innovations, open solutions},
  title          = {A fast algorithm for network forecasting time series},
  doi            = {10.1109/{ACCE\SS}.2019.2926986},
  issn           = {2169-3536},
  pages          = {102554--102560},
  url            = {https://ieeexplore.ieee.org/document/8755971/},
  urldate        = {2019-09-11},
  volume         = {7},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 04:36},
}

@Article{Liu-et-al-2011a,
  author               = {Liu, Yang-Yu Y. and Slotine, Jean-Jacques J. and Barabasi, Albert-Laszlo L.},
  date                 = {2011-05},
  journaltitle         = {Nature},
  title                = {Controllability of complex networks.},
  doi                  = {10.1038/nature10011},
  issn                 = {1476-4687},
  number               = {7346},
  pages                = {167--173},
  volume               = {473},
  abstract             = {The ultimate proof of our understanding of natural or technological systems is reflected in our ability to control them. Although control theory offers mathematical tools for steering engineered and natural systems towards a desired state, a framework to control complex self-organized systems is lacking. Here we develop analytical tools to study the controllability of an arbitrary complex directed network, identifying the set of driver nodes with time-dependent control that can guide the system's entire dynamics. We apply these tools to several real networks, finding that the number of driver nodes is determined mainly by the network's degree distribution. We show that sparse inhomogeneous networks, which emerge in many real complex systems, are the most difficult to control, but that dense and homogeneous networks can be controlled using a few driver nodes. Counterintuitively, we find that in both model and real systems the driver nodes tend to avoid the high-degree nodes.},
  citeulike-article-id = {9277950},
  citeulike-linkout-0  = {http://dx.doi.org/10.1038/nature10011},
  citeulike-linkout-1  = {http://dx.doi.org/10.1038/nature10011},
  citeulike-linkout-2  = {http://view.ncbi.nlm.nih.gov/pubmed/21562557},
  citeulike-linkout-3  = {http://www.hubmed.org/display.cgi?uids=21562557},
  day                  = {12},
  pmid                 = {21562557},
  posted-at            = {2017-12-08 01:54:11},
  publisher            = {Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  timestamp            = {2020-02-27 04:36},
}

@Article{Lohre-et-al-2014,
  author               = {Lohre, Harald and Papenbrock, Jochen and Poonia, Muddit},
  date                 = {2014-10},
  journaltitle         = {SSRN e-Print},
  title                = {The Use of Correlation Networks in Parametric Portfolio Policies},
  url                  = {https://ssrn.com/abstract=2505732},
  abstract             = {Correlation networks reveal a rich picture of market risk structure dynamics. A rather compact and well-organized sector correlation network is indicative of a healthy market, whereas a widely spread sector correlation network characterizes a more fragile market environment. Intuitively, some characteristics of the correlation network can serve as natural measures of systemic risk. Pursuing an equity market timing strategy we document the predictive content of these measures to translate into a meaningful portfolio utility. Moreover, this result continues to hold when controlling for common predictors of the equity risk premium. Not only can correlation networks be useful as an aggregate market timing signal but also in navigating the cross-section of equity sectors. We especially document a significant outperformance of peripheral versus central equity sectors that cannot be explained by momentum or low volatility effects. Finally, we implement a parametric portfolio policy that comprises the complete information content of the sector network topology conditional on a given level of risk aversion.},
  citeulike-article-id = {13997388},
  citeulike-linkout-0  = {http://ssrn.com/abstract=2505732},
  citeulike-linkout-1  = {http://papers.ssrn.com/sol3/Delivery.cfm/SSRNID2505732code694027.pdf?abstractid=2505732 and mirid=1},
  day                  = {6},
  owner                = {cristi},
  posted-at            = {2016-04-05 04:34:36},
  timestamp            = {2020-02-27 04:36},
}

@Article{Lo-Stein-2016,
  author               = {Lo, Andrew W. and Stein, Roger M.},
  date                 = {2016-03},
  journaltitle         = {The Journal of Alternative Investments},
  title                = {TRC Networks and Systemic Risk},
  doi                  = {10.3905/jai.2016.18.4.052},
  issn                 = {1520-3255},
  number               = {4},
  pages                = {52--67},
  volume               = {18},
  abstract             = {The authors introduce a new approach to identifying and monitoring systemic risk that combines network analysis and tail risk contribution (TRC). Network analysis provides great flexibility in representing and exploring linkages between institutions, but it can be overly general in describing the risk exposures of one entity to another. TRC provides a more focused view of key systemic risks and richer financial intuition, but it may miss important linkages between financial institutions. Integrating these two methods can provide information on key relationships between institutions that may become relevant during periods of systemic stress. The authors demonstrate this approach using the exposures of money market funds to major financial institutions during July 2011. The results for their example suggest that TRC networks can highlight both institutions and funds that may become distressed during a financial crisis.},
  citeulike-article-id = {14433281},
  citeulike-linkout-0  = {http://dx.doi.org/10.3905/jai.2016.18.4.052},
  posted-at            = {2017-09-17 20:43:04},
  timestamp            = {2020-02-27 04:36},
}

@Article{Lu-et-al-2016,
  author               = {Lu, Linyuan and Chen, Duanbing and Ren, Xiao-Long and Zhang, Qian-Ming and Zhang, Yi-Cheng and Zhou, Tao},
  date                 = {2016-09},
  journaltitle         = {Physics Reports},
  title                = {Vital nodes identification in complex networks},
  doi                  = {10.1016/j.physrep.2016.06.007},
  issn                 = {0370-1573},
  pages                = {1--63},
  volume               = {650},
  abstract             = {Real networks exhibit heterogeneous nature with nodes playing far different roles in structure and function. To identify vital nodes is thus very significant, allowing us to control the outbreak of epidemics, to conduct advertisements for e-commercial products, to predict popular scientific publications, and so on. The vital nodes identification attracts increasing attentions from both computer science and physical societies, with algorithms ranging from simply counting the immediate neighbors to complicated machine learning and message passing approaches. In this review, we clarify the concepts and metrics, classify the problems and methods, as well as review the important progresses and describe the state of the art. Furthermore, we provide extensive empirical analyses to compare well-known methods on disparate real networks, and highlight the future directions. In spite of the emphasis on physics-rooted approaches, the unification of the language and comparison with cross-domain methods would trigger interdisciplinary solutions in the near future.},
  citeulike-article-id = {14461246},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.physrep.2016.06.007},
  keywords             = {pdf},
  posted-at            = {2017-10-19 17:16:17},
  timestamp            = {2020-02-27 04:36},
}

@Article{Mao-Xiao-2019,
  author         = {Mao, Shengzhong and Xiao, Fuyuan},
  date           = {2019},
  journaltitle   = {IEEE access : practical innovations, open solutions},
  title          = {Time series forecasting based on complex network analysis},
  doi            = {10.1109/{ACCE\SS}.2019.2906268},
  issn           = {2169-3536},
  pages          = {40220--40229},
  url            = {https://ieeexplore.ieee.org/document/8669744/},
  urldate        = {2019-09-11},
  volume         = {7},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 04:36},
}

@Article{Massara-et-al-2016,
  author               = {Massara, Guido P. and Di Matteo, T. and Aste, Tomaso},
  date                 = {2016-06},
  journaltitle         = {Journal of Complex Networks},
  title                = {Network Filtering for Big Data: Triangulated Maximally Filtered Graph},
  doi                  = {10.1093/comnet/cnw015},
  issn                 = {2051-1310},
  pages                = {cnw015+},
  abstract             = {We propose a network-filtering method, the Triangulated Maximally Filtered Graph (TMFG), that provides an approximate solution to the Weighted Maximal Planar Graph problem. The underlying idea of TMFG consists in building a triangulation that maximizes a score function associated with the amount of information retained by the network. TMFG uses as weights any arbitrary similarity measure to arrange data into a meaningful network structure that can be used for clustering, community detection and modelling. The method is fast, adaptable and scalable to very large datasets; it allows online updating and learning as new data can be inserted and deleted with combinations of local and non-local moves. Further, TMFG permits readjustments of the network in consequence of changes in the strength of the similarity measure. The method is based on local topological moves and can therefore take advantage of parallel and GPUs computing. We discuss how this network-filtering method can be used intuitively and efficiently for big data studies and its significance from an information-theoretic perspective.},
  citeulike-article-id = {14444476},
  citeulike-linkout-0  = {http://dx.doi.org/10.1093/comnet/cnw015},
  day                  = {21},
  posted-at            = {2017-10-03 08:44:21},
  timestamp            = {2020-02-27 04:36},
}

@Article{Matsunaga-et-al-2019,
  author         = {Matsunaga, Daiki and Suzumura, Toyotaro and Takahashi, Toshihiro},
  date           = {2019-09-24},
  journaltitle   = {arXiv e-Print},
  title          = {Exploring Graph Neural Networks for Stock Market Predictions with Rolling Window Analysis},
  url            = {https://arxiv.org/abs/1909.10660},
  urldate        = {2019-10-02},
  abstract       = {Recently, there has been a surge of interest in the use of machine learning to help aid in the accurate predictions of financial markets. Despite the exciting advances in this cross-section of finance and AI, many of the current approaches are limited to using technical analysis to capture historical trends of each stock price and thus limited to certain experimental setups to obtain good prediction results. On the other hand, professional investors additionally use their rich knowledge of inter-market and inter-company relations to map the connectivity of companies and events, and use this map to make better market predictions. For instance, they would predict the movement of a certain company's stock price based not only on its former stock price trends but also on the performance of its suppliers or customers, the overall industry, macroeconomic factors and trade policies. This paper investigates the effectiveness of work at the intersection of market predictions and graph neural networks, which hold the potential to mimic the ways in which investors make decisions by incorporating company knowledge graphs directly into the predictive model. The main goal of this work is to test the validity of this approach across different markets and longer time horizons for backtesting using rolling window analysis. In this work, we concentrate on the prediction of individual stock prices in the Japanese Nikkei 225 market over a period of roughly 20 years. For the knowledge graph, we use the Nikkei Value Search data, which is a rich dataset showing mainly supplier relations among Japanese and foreign companies. Our preliminary results show a 29.5\% increase and a 2.2-fold increase in the return ratio and Sharpe ratio, respectively, when compared to the market benchmark, as well as a 6.32\% increase and 1.3-fold increase, respectively, compared to the baseline LSTM model.},
  day            = {24},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 04:36},
}

@Article{Musmeci-et-al-2014,
  author               = {Musmeci, Nicolo and Aste, Tomaso and Di Matteo, Tiziana},
  date                 = {2014-10},
  journaltitle         = {arXiv e-Print},
  title                = {Risk diversification: a study of persistence with a filtered correlation-network approach},
  eprint               = {1410.5621},
  eprinttype           = {arXiv},
  url                  = {https://arxiv.org/abs/1410.5621},
  abstract             = {The evolution with time of the correlation structure of equity returns is studied by means of a filtered network approach investigating persistences and recurrences and their implications for risk diversification strategies. We build dynamically Planar Maximally Filtered Graphs from the correlation structure over a rolling window and we study the persistence of the associated Directed Bubble Hierarchical Tree (DBHT) clustering structure. We observe that the DBHT clustering structure is quite stable during the early 2000' becoming gradually less persistent before the unfolding of the 2007-2008 crisis. The correlation structure eventually recovers persistence in the aftermath of the crisis settling up a new phase, distinct from the pre-cysts structure, where the market structure is less related to industrial sector activity. Notably, we observe that - presently - the correlation structure is loosing again persistence indicating the building-up of another, different, phase. Such dynamical changes in persistence and their occurrence at the unfolding of financial crises rises concerns about the effectiveness of correlation-based portfolio management tools for risk diversification.},
  citeulike-article-id = {14292068},
  citeulike-linkout-0  = {http://arxiv.org/abs/1410.5621},
  citeulike-linkout-1  = {http://arxiv.org/pdf/1410.5621},
  day                  = {21},
  groups               = {Networks and investment management, Clustering and network analysis, Invest_Diversif},
  posted-at            = {2017-03-05 17:56:56},
  timestamp            = {2020-02-27 04:36},
}

@Article{Musmeci-et-al-2016,
  author               = {Musmeci, Nicolo and Aste, Tomaso and Di Matteo, Tiziana},
  date                 = {2016-05},
  journaltitle         = {arXiv e-Print},
  title                = {What does past correlation structure tell us about the future? An answer from network filtering},
  eprint               = {1605.08908},
  eprinttype           = {arXiv},
  url                  = {https://arxiv.org/abs/1605.08908},
  abstract             = {We discovered that past changes in the market correlation structure are significantly related with future changes in the market volatility. By using correlation-based information filtering networks we device a new tool for forecasting the market volatility changes. In particular, we introduce a new measure, the "correlation structure persistence", that quantifies the rate of change of the market dependence structure. This measure shows a deep interplay with changes in volatility and we demonstrate it can anticipate market risk variations. Notably, our method overcomes the curse of dimensionality that limits the applicability of traditional econometric tools to portfolios made of a large number of assets. We report on forecasting performances and statistical significance of this tool for two different equity datasets. We also identify an optimal region of parameters in terms of True Positive and False Positive trade-off, through a ROC curve analysis. We find that our forecasting method is robust and it outperforms predictors based on past volatility only. Moreover the temporal analysis indicates that our method is able to adapt to abrupt changes in the market, such as financial crises, more rapidly than methods based on past volatility.},
  citeulike-article-id = {14147196},
  citeulike-linkout-0  = {http://arxiv.org/abs/1605.08908},
  citeulike-linkout-1  = {http://arxiv.org/pdf/1605.08908},
  day                  = {28},
  groups               = {Networks and investment management},
  owner                = {cristi},
  posted-at            = {2016-09-27 14:49:33},
  timestamp            = {2020-02-27 04:36},
}

@Article{Namaki-et-al-2011,
  author               = {Namaki, A. and Shirazi, A. H. and Raei, R. and Jafari, G. R.},
  date                 = {2011-10},
  journaltitle         = {Physica A: Statistical Mechanics and its Applications},
  title                = {Network analysis of a financial market based on genuine correlation and threshold method},
  doi                  = {10.1016/j.physa.2011.06.033},
  issn                 = {0378-4371},
  number               = {21-22},
  pages                = {3835--3841},
  volume               = {390},
  abstract             = {A financial market is an example of an adaptive complex network consisting of many interacting units. This network reflects market's behavior. In this paper, we use Random Matrix Theory (RMT) notion for specifying the largest eigenvector of correlation matrix as the market mode of stock network. For a better risk management, we clean the correlation matrix by removing the market mode from data and then construct this matrix based on the residuals. We show that this technique has an important effect on correlation coefficient distribution by applying it for Dow Jones Industrial Average (DJIA). To study the topological structure of a network we apply the removing market mode technique and the threshold method to Tehran Stock Exchange (TSE) as an example. We show that this network follows a power-law model in certain interval. We also show the behavior of clustering coefficients and component numbers of this network for different thresholds. These outputs are useful for both theoretical and practical purposes such as asset allocation and risk management.},
  citeulike-article-id = {9485495},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.physa.2011.06.033},
  posted-at            = {2017-10-19 21:04:21},
  timestamp            = {2020-02-27 04:36},
}

@Article{Newman-Reinert-2016,
  author               = {Newman, M. . E. . J. and Reinert, Gesine},
  date                 = {2016-08},
  journaltitle         = {Physical Review Letters},
  title                = {Estimating the Number of Communities in a Network},
  doi                  = {10.1103/physrevlett.117.078301},
  issn                 = {0031-9007},
  number               = {7},
  volume               = {117},
  abstract             = {Community detection, the division of a network into dense subnetworks with only sparse connections between them, has been a topic of vigorous study in recent years. However, while there exist a range of effective methods for dividing a network into a specified number of communities, it is an open question how to determine exactly how many communities one should use. Here we describe a mathematically principled approach for finding the number of communities in a network by maximizing the integrated likelihood of the observed network structure under an appropriate generative model. We demonstrate the approach on a range of benchmark networks, both real and computer generated.},
  citeulike-article-id = {14449658},
  citeulike-linkout-0  = {http://dx.doi.org/10.1103/physrevlett.117.078301},
  day                  = {11},
  posted-at            = {2017-10-12 23:33:19},
  timestamp            = {2020-02-27 04:36},
}

@Article{Nie-Song-2017,
  author               = {Nie, Chun-Xiao and Song, Fu-Tie},
  date                 = {2017-12},
  journaltitle         = {Physica A: Statistical Mechanics and its Applications},
  title                = {Constructing financial network based on PMFG and threshold method},
  doi                  = {10.1016/j.physa.2017.12.037},
  issn                 = {0378-4371},
  abstract             = {Based on planar maximally filtered graph (PMFG) and threshold method, we introduced a new correlation-based network named PMFG-based threshold network (PTN). We studied the community structure of PTN and applied ISOMAP algorithm to represent PTN in low-dimensional Euclidean space. The results show that the community corresponds well to the cluster in the Euclidean space. Further, we studied the dynamics of the community structure and constructed the normalized mutual information (NMI) matrix. Based on the real data in the market, we found that the volatility of the market can lead to dramatic changes in the community structure, and the structure is more stable during the financial crisis.},
  citeulike-article-id = {14511169},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.physa.2017.12.037},
  groups               = {Factor based investing, Vol_Cluster},
  posted-at            = {2018-01-02 19:41:03},
  timestamp            = {2020-02-27 04:36},
}

@Article{Nie-Song-2018,
  author         = {Nie, Chun-Xiao and Song, Fu-Tie},
  date           = {2018-08},
  journaltitle   = {Chaos, Solitons \& Fractals},
  title          = {Analyzing the stock market based on the structure of kNN network},
  doi            = {10.1016/j.chaos.2018.05.018},
  issn           = {0960-0779},
  pages          = {148--159},
  url            = {https://linkinghub.elsevier.com/retrieve/pii/S0960077918302753},
  urldate        = {2019-04-19},
  volume         = {113},
  abstract       = {Abstract This paper systematically studies the structure of the financial kNN ( k -nearest neighbor) network. First, we use the eigenvalues and eigenvectors of the financial correlation matrix to analyze the structure of the network. We find that the degree is related to the average correlation coefficient, and furthermore, it also has a relationship between the components of the eigenvector corresponding to the maximum eigenvalue. We apply existing research to confirm that the community structure of the kNN network can be used to cluster financial time series. Finally, empirical studies based on financial markets in three countries show that there is a high correlation between the community structure and dimensions. Therefore, this study shows that the structure of the financial kNN network is related to the properties of the correlation matrix, and it extracts a meaningful correlation structure.},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 04:36},
}

@Article{Nikolaev-et-al-2013,
  author               = {Nikolaev, Nikolay and Tino, Peter and Smirnov, Evgueni},
  date                 = {2013-12},
  journaltitle         = {Neurocomputing},
  title                = {Time-dependent series variance learning with recurrent mixture density networks},
  doi                  = {10.1016/j.neucom.2013.05.014},
  issn                 = {0925-2312},
  pages                = {501--512},
  volume               = {122},
  abstract             = {This paper presents an improved nonlinear mixture density approach to modeling the time-dependent variance in time series. First, we elaborate a recurrent mixture density network for explicit modeling of the time conditional mixing coefficients, as well as the means and variances of its Gaussian mixture components. Second, we derive training equations with which all the network weights are inferred in the maximum likelihood framework. Crucially, we calculate temporal derivatives through time for dynamic estimation of the variance network parameters. Experimental results show that, when compared with a traditional linear heteroskedastic model, as well as with the nonlinear mixture density network trained with static derivatives, our dynamic recurrent network converges to more accurate results with better statistical characteristics and economic performance.},
  citeulike-article-id = {14503459},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.neucom.2013.05.014},
  posted-at            = {2017-12-15 15:27:34},
  timestamp            = {2020-02-27 04:36},
}

@Article{Ni-Zhang-2013,
  author               = {Ni, Li N. and Zhang, Jin Q.},
  date                 = {2013-02},
  journaltitle         = {Applied Mechanics and Materials},
  title                = {Portfolio Optimization for Index Investing Based on Self-Organizing Neural Network},
  doi                  = {10.4028/www.scientific.net/amm.303-306.1595},
  issn                 = {1662-7482},
  pages                = {1595--1598},
  volume               = {303-306},
  abstract             = {Index investing is an important issue for researchers and practitioners. This paper proposes an index portfolio optimization model for index investing via employing CSI 300 as underlying index. Firstly, a self-organizing neural network clustering model is constructed to complete the stock clustering based on stock trend which regards stock price as input. The index portfolio optimization model is proposed to determine the optimal investment proportion of each cluster sampling and achieve the minimum tracking error. The constraint BP algorithm is improved to benefit the optimization calculation of stock weights. Empirical results show that our approach achieves smaller tracking error and better index tracking effect than the random sampling.},
  citeulike-article-id = {14315759},
  citeulike-linkout-0  = {http://dx.doi.org/10.4028/www.scientific.net/amm.303-306.1595},
  groups               = {PortfOptim_Network},
  posted-at            = {2017-03-22 10:34:40},
  timestamp            = {2020-02-27 04:36},
}

@Article{Nobi-et-al-2014,
  author               = {Nobi, Ashadun and Lee, Sungmin and Kim, Doo H. and Lee, Jae W.},
  date                 = {2014-07},
  journaltitle         = {Physics Letters A},
  title                = {Correlation and network topologies in global and local stock indices},
  doi                  = {10.1016/j.physleta.2014.07.009},
  eprint               = {1402.1552},
  eprinttype           = {arXiv},
  issn                 = {0375-9601},
  number               = {34},
  pages                = {2482--2489},
  volume               = {378},
  abstract             = {We examine the structural changes of the correlation network due to the crisis in global and local stock market. The effects of the crisis in the local market are different to the global market. The abrupt changes of the network topologies occur during the crisis. The large change of the Jaccard index indicates the upcoming systemic risk in the markets. We examined how the correlation and network structure of the global indices and local Korean indices have changed during years 2000-2012. The average correlations of the global indices increased with time, while the local indices showed a decreasing trend except for drastic changes during the crises. A significant change in the network topologies was observed due to the financial crises in both markets. The Jaccard similarities identified the change in the market state due to a crisis in both markets. The dynamic change of the Jaccard index can be used as an indicator of systemic risk or precursors of the crisis.},
  citeulike-article-id = {14148610},
  citeulike-linkout-0  = {http://arxiv.org/abs/1402.1552},
  citeulike-linkout-1  = {http://arxiv.org/pdf/1402.1552},
  citeulike-linkout-2  = {http://dx.doi.org/10.1016/j.physleta.2014.07.009},
  day                  = {7},
  groups               = {Networks and investment management},
  owner                = {cristi},
  posted-at            = {2016-09-28 20:14:00},
  timestamp            = {2020-02-27 04:36},
}

@Article{Oya-et-al-2014,
  author               = {Oya, Shunsuke and Aihara, Kazuyuki and Hirata, Yoshito},
  date                 = {2014-11},
  journaltitle         = {New Journal of Physics},
  title                = {Forecasting abrupt changes in foreign exchange markets: method using dynamical network marker},
  doi                  = {10.1088/1367-2630/16/11/115015},
  issn                 = {1367-2630},
  number               = {11},
  pages                = {115015+},
  volume               = {16},
  abstract             = {We apply the idea of dynamical network markers (Chen et al 2012 Sci. Rep. 2 342) to foreign exchange markets so that early warning signals can be provided for any abrupt changes. The dynamical network marker constructed achieves a high odds ratio for forecasting these sudden changes. In addition, we also extend the notion of the dynamical network marker by using recurrence plots so that the notion can be applied to delay coordinates and point processes. Thus, the dynamical network marker is useful in a variety of contexts in science, technology, and society.},
  citeulike-article-id = {14067751},
  citeulike-linkout-0  = {http://dx.doi.org/10.1088/1367-2630/16/11/115015},
  day                  = {19},
  groups               = {FrcstQWIM_Other},
  owner                = {cristi},
  posted-at            = {2016-06-12 18:42:28},
  timestamp            = {2020-02-27 04:36},
}

@Article{Papana-et-al-2017,
  author               = {Papana, Angeliki and Kyrtsou, Catherine and Kugiumtzis, Dimitris and Diks, Cees},
  date                 = {2017-09},
  journaltitle         = {Physica A: Statistical Mechanics and its Applications},
  title                = {Financial networks based on Granger causality: A case study},
  doi                  = {10.1016/j.physa.2017.04.046},
  issn                 = {0378-4371},
  pages                = {65--73},
  volume               = {482},
  abstract             = {Connectivity analysis is performed on a long financial record of 21 international stock indices employing a linear and a nonlinear causality measure, the conditional Granger causality index (CGCI) and the partial mutual information on mixed embedding (PMIME), respectively. Both measures aim to specify the direction of the interrelationships among the international stock indexes and portray the links of the resulting networks, by the presence of direct couplings between variables exploiting all available information. However, their differences are assessed due to the presence of nonlinearity. The weighted networks formed with respect to the causality measures are transformed to binary ones using a significance test. The financial networks are formed on sliding windows in order to examine the network characteristics and trace changes in the connectivity structure. Subsequently, two statistical network quantities are calculated; the average degree and the average shortest path length. The empirical findings reveal interesting time-varying properties of the constructed network, which are clearly dependent on the nature of the financial cycle.},
  citeulike-article-id = {14433194},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.physa.2017.04.046},
  groups               = {Data_NonLinear},
  posted-at            = {2017-09-17 14:54:57},
  timestamp            = {2020-02-27 04:41},
}

@Article{Papenbrock-Schwendner-2015,
  author               = {Papenbrock, Jochen and Schwendner, Peter},
  date                 = {2015},
  journaltitle         = {Financial Markets and Portfolio Management},
  title                = {Handling risk-on/risk-off dynamics with correlation regimes and correlation networks},
  doi                  = {10.1007/s11408-015-0248-2},
  number               = {2},
  pages                = {125--147},
  volume               = {29},
  abstract             = {In this paper, we present a framework for detecting distinct correlation regimes and analyzing the emerging state dependences for a multi-asset futures portfolio from 1998 to 2013. These correlation regimes have been significantly different since the financial crisis of 2008 than they were previously; cluster tracking shows that asset classes are now less separated.

We identify distinct - risk-on- and - risk-off- assets with the help of correlation networks. In addition to visualizing, we quantify these observations using suitable metrics for the clusters and correlation networks. The framework will be useful for financial risk management, portfolio construction, and asset allocation.},
  citeulike-article-id = {13934872},
  citeulike-linkout-0  = {http://dx.doi.org/10.1007/s11408-015-0248-2},
  citeulike-linkout-1  = {http://link.springer.com/article/10.1007/s11408-015-0248-2},
  groups               = {Networks and investment management, Regime_Invest, Regime_Identif},
  owner                = {cristi},
  posted-at            = {2016-02-18 04:27:47},
  publisher            = {Springer US},
  timestamp            = {2020-02-27 04:41},
}

@Article{Patwary-et-al-2017,
  author               = {Patwary, EnayetUllah and Lee, JongYoul and Nobi, Ashadun and Kim, DooHwan and Lee, JaeWoo},
  date                 = {2017},
  journaltitle         = {Journal of the Korean Physical Society},
  title                = {Changes of hierarchical network in local and world stock market},
  doi                  = {10.3938/jkps.71.444},
  number               = {7},
  pages                = {444--451},
  volume               = {71},
  abstract             = {We consider the cross-correlation coefficients of the daily returns in the local and global stock markets. We generate the minimal spanning tree (MST) using the correlation matrix. We observe that the MSTs change their structure from chain-like networks to star-like networks during periods of market uncertainty. We quantify the measure of the hierarchical network utilizing the value of the hierarchy measured by the hierarchical path. The hierarchy and betweenness centrality characterize the state of the market regarding the impact of crises. During crises, the non-financial company is established as the central node of the MST. However, before the crisis and during stable periods, the financial company is occupying the central node of the MST in the Korean and the U.S. stock markets. The changes in the network structure and the central node are good indicators of an upcoming crisis.},
  citeulike-article-id = {14444510},
  citeulike-linkout-0  = {http://dx.doi.org/10.3938/jkps.71.444},
  citeulike-linkout-1  = {http://link.springer.com/article/10.3938/jkps.71.444},
  posted-at            = {2017-10-03 10:03:42},
  publisher            = {The Korean Physical Society},
  timestamp            = {2020-02-27 04:41},
}

@Article{Peralta-Zareei-2016,
  author               = {Peralta, Gustavo and Zareei, Abalfazl},
  date                 = {2016-09},
  journaltitle         = {Journal of Empirical Finance},
  title                = {A network approach to portfolio selection},
  doi                  = {10.1016/j.jempfin.2016.06.003},
  issn                 = {0927-5398},
  pages                = {157--180},
  volume               = {38},
  abstract             = {Low-central stocks receive higher weights in optimal allocation. Financial and market variables are major drivers of stocks' centrality. We construct a network-based investment strategy that performs well out-of-sample. Our network-based strategy results in positive and significant Carhart's alphas. In this study, a financial market is conceived as a network where the securities are nodes and the links account for returns' correlations. We theoretically prove the negative relationship between the centrality of assets in this financial market network and their optimal weights under the Markowitz framework. Therefore, optimal portfolios overweight low-central securities to avoid the large variances that result when highly influential stocks are included in the investor's opportunity set. Next, we empirically investigate the major financial and market determinants of stock's centralities. The evidence indicates that highly central nodes tend to coincide with older, larger-cap, cheaper and financially riskier securities. Finally, we explore by means of in-sample and out-of-sample analysis the extent to which the structure of the stock market network can be employed to improve the portfolio selection process. We propose a network-based investment strategy that outperforms well-known benchmarks while presenting positive and significant Carhart alphas. The major contribution of the paper is to employ the financial market network as a useful device to improve the portfolio selection process by targeting a group of assets according to their centrality.},
  citeulike-article-id = {14148613},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.jempfin.2016.06.003},
  groups               = {Networks and investment management, Invest_Network},
  owner                = {cristi},
  posted-at            = {2016-09-28 20:16:03},
  timestamp            = {2020-02-27 04:41},
}

@Article{Piccardi-et-al-2011,
  author               = {Piccardi, Carlo and Calatroni, Lisa and Bertoni, Fabio},
  date                 = {2011-01},
  journaltitle         = {International Journal of Modern Physics C},
  title                = {Clustering financial time series by network community analysis},
  doi                  = {10.1142/s012918311101604x},
  number               = {01},
  pages                = {35--50},
  volume               = {22},
  abstract             = {In this paper, we describe a method for clustering financial time series which is based on community analysis, a recently developed approach for partitioning the nodes of a network (graph). A network with N nodes is associated to the set of N time series. The weight of the link (i, j), which quantifies the similarity between the two corresponding time series, is defined according to a metric based on symbolic time series analysis, which has recently proved effective in the context of financial time series. Then, searching for network communities allows one to identify groups of nodes (and then time series) with strong similarity. A quantitative assessment of the significance of the obtained partition is also provided. The method is applied to two distinct case-studies concerning the US and Italy Stock Exchange, respectively. In the US case, the stability of the partitions over time is also thoroughly investigated. The results favorably compare with those obtained with the standard tools typically used for clustering financial time series, such as the minimal spanning tree and the hierarchical tree.},
  citeulike-article-id = {14150069},
  citeulike-linkout-0  = {http://dx.doi.org/10.1142/s012918311101604x},
  citeulike-linkout-1  = {http://www.worldscientific.com/doi/abs/10.1142/S012918311101604X},
  day                  = {1},
  groups               = {Networks and investment management, Clustering and network analysis},
  owner                = {cristi},
  posted-at            = {2016-10-01 20:30:18},
  publisher            = {World Scientific Publishing Co.},
  timestamp            = {2020-02-27 04:41},
}

@Article{Pichler-et-al-2018,
  author         = {Pichler, Anton and Poledna, Sebastian and Thurner, Stefan},
  date           = {2018-01-31},
  journaltitle   = {arXiv e-Print},
  title          = {Systemic-risk-efficient asset allocation: Minimization of systemic risk as a network optimization problem},
  url            = {https://arxiv.org/abs/1801.10515},
  abstract       = {Systemic risk arises as a multi-layer network phenomenon. Layers represent direct financial exposures of various types, including interbank liabilities, derivative- or foreign exchange exposures. Another network layer of systemic risk emerges through common asset holdings of financial institutions. Strongly overlapping portfolios lead to similar exposures that are caused by price movements of the underlying financial assets. Based on the knowledge of portfolio holdings of financial agents we quantify systemic risk of overlapping portfolios. We present an optimization procedure, where we minimize the systemic risk in a given financial market by optimally rearranging overlapping portfolio networks, under the constraints that the expected returns and risks of the individual portfolios are unchanged. We explicitly demonstrate the power of the method on the overlapping portfolio network of sovereign exposure between major European banks by using data from the European Banking Authority stress test of 2016. We show that systemic-risk-efficient allocations are accessible by the optimization. In the case of sovereign exposure, systemic risk can be reduced by more than a factor of two, with- out any detrimental effects for the individual banks. These results are confirmed by a simple simulation of fire sales in the government bond market. In particular we show that the contagion probability is reduced dramatically in the optimized network.},
  day            = {31},
  f1000-projects = {QuantInvest},
  groups         = {Invest_Risk, PortfOptim_Network},
  timestamp      = {2020-02-27 04:41},
}

@Article{Porter-Howison-2017,
  author               = {Porter, Mason A. and Howison, Sam D.},
  date                 = {2017-09},
  journaltitle         = {SSRN e-Print},
  title                = {The Role of Network Analysis in Industrial and Applied Mathematics: A Physical-Applied-Mathematics Perspective},
  eprint               = {1703.06843},
  eprinttype           = {arXiv},
  url                  = {https://arxiv.org/abs/1703.06843},
  abstract             = {Many problems in industry --- and in the social, natural, information, and medical sciences --- involve discrete data and benefit from approaches from subjects such as network science, information theory, optimization, probability, and statistics. Because the study of networks is concerned explicitly with connectivity between different entities, it has become very prominent in industrial settings, and this importance has been accentuated further amidst the modern data deluge. In this commentary, we discuss the role of network analysis in industrial and applied mathematics, and we give several examples of network science in industry. We focus, in particular, on discussing a physical-applied-mathematics approach to the study of networks.},
  citeulike-article-id = {14347356},
  citeulike-linkout-0  = {http://arxiv.org/abs/1703.06843},
  citeulike-linkout-1  = {http://arxiv.org/pdf/1703.06843},
  day                  = {14},
  posted-at            = {2017-10-05 11:50:54},
  timestamp            = {2020-02-27 04:41},
}

@Article{Qian-et-al-2017,
  author               = {Qian, Ya and Hardle, Wolfgang K. and Chen, Cathy Y.},
  date                 = {2017},
  journaltitle         = {SSRN e-Print},
  title                = {Industry Interdependency Dynamics in a Network Context},
  url                  = {https://ssrn.com/abstract=2961703},
  abstract             = {This paper contributes to model the industry interconnecting structure in a network context. General predictive model (Rapach et al. 2016) is extended to quantile LASSO regression so as to incorporate tail risks in the construction of industry interdependency networks. Empirical results show a denser network with heterogeneous central industries in tail cases. Network dynamics demonstrate the variety of interdependency across time. Lower tail interdependency structure gives the most accurate out-of-sample forecast of portfolio returns and network centrality-based trading strategies seem to outperform market portfolios, leading to the possible 'too central to fail' argument.},
  citeulike-article-id = {14350157},
  citeulike-linkout-0  = {https://papers.ssrn.com/sol3/papers.cfm?abstractid=2961703},
  groups               = {Networks and investment management},
  posted-at            = {2017-05-03 18:34:29},
  timestamp            = {2020-02-27 04:41},
}

@Article{Qiao-et-al-2016,
  author               = {Qiao, Haishu and Xia, Yue and Li, Ying},
  date                 = {2016-06},
  journaltitle         = {PLOS ONE},
  title                = {Can Network Linkage Effects Determine Return? Evidence from Chinese Stock Market},
  doi                  = {10.1371/journal.pone.0156784},
  number               = {6},
  pages                = {e0156784+},
  volume               = {11},
  abstract             = {This study used the dynamic conditional correlations (DCC) method to identify the linkage effects of Chinese stock market, and further detected the influence of network linkage effects on magnitude of security returns across different industries. Applying two physics-derived techniques, the minimum spanning tree and the hierarchical tree, we analyzed the stock interdependence within the network of the China Securities Index (CSI) industry index basket. We observed that that obvious linkage effects existed among stock networks. CII and CCE, CAG and ITH as well as COU, CHA and REI were confirmed as the core nodes in the three different networks respectively. We also investigated the stability of linkage effects by estimating the mean correlations and mean distances, as well as the normalized tree length of these indices. In addition, using the GMM model approach, we found inter-node influence within the stock network had a pronounced effect on stock returns. Our results generally suggested that there appeared to be greater clustering effect among the indexes belonging to related industrial sectors than those of diverse sectors, and network comovement was significantly affected by impactive financial events in the reality. Besides, stocks that were more central within the network of stock market usually had higher returns for compensation because they endured greater exposure to correlation risk.},
  citeulike-article-id = {14447481},
  citeulike-linkout-0  = {http://dx.doi.org/10.1371/journal.pone.0156784},
  day                  = {3},
  posted-at            = {2017-10-08 14:33:29},
  publisher            = {Public Library of Science},
  timestamp            = {2020-02-27 04:41},
}

@Article{Quiles-et-al-2016,
  author               = {Quiles, Marcos G. and Macau, Elbert E. N. and Rubido, Nicolas},
  date                 = {2016-05},
  journaltitle         = {Scientific Reports},
  title                = {Dynamical detection of network communities},
  doi                  = {10.1038/srep25570},
  issn                 = {2045-2322},
  pages                = {25570+},
  volume               = {6},
  abstract             = {A prominent feature of complex networks is the appearance of communities, also known as modular structures. Specifically, communities are groups of nodes that are densely connected among each other but connect sparsely with others. However, detecting communities in networks is so far a major challenge, in particular, when networks evolve in time. Here, we propose a change in the community detection approach. It underlies in defining an intrinsic dynamic for the nodes of the network as interacting particles (based on diffusive equations of motion and on the topological properties of the network) that results in a fast convergence of the particle system into clustered patterns. The resulting patterns correspond to the communities of the network. Since our detection of communities is constructed from a dynamical process, it is able to analyse time-varying networks straightforwardly. Moreover, for static networks, our numerical experiments show that our approach achieves similar results as the methodologies currently recognized as the most efficient ones. Also, since our approach defines an N-body problem, it allows for efficient numerical implementations using parallel computations that increase its speed performance.},
  citeulike-article-id = {14150014},
  citeulike-linkout-0  = {http://dx.doi.org/10.1038/srep25570},
  day                  = {9},
  owner                = {cristi},
  posted-at            = {2016-10-01 16:53:13},
  timestamp            = {2020-02-27 04:41},
}

@Article{Rahmede-et-al-2017,
  author               = {Rahmede, Christoph and Iacovacci, Jacopo and Arenas, Alex and Bianconi, Ginestra},
  date                 = {2017-03},
  journaltitle         = {arXiv e-Print},
  title                = {Centralities of Nodes and Influences of Layers in Large Multiplex Networks},
  eprint               = {1703.05833},
  eprinttype           = {arXiv},
  url                  = {https://arxiv.org/abs/1703.05833},
  abstract             = {We formulate and propose an algorithm (MultiRank) for the ranking of nodes and layers in large multiplex networks. MultiRank takes into account the full multiplex network structure of the data and exploits the dual nature of the network in terms of nodes and layers. The proposed centrality of the layers (influences) and the centrality of the nodes are determined by a coupled set of equations. The basic idea consists in assigning more centrality to nodes that receive links from highly influential layers and from already central nodes. The layers are more influential if highly central nodes are active in them. The algorithm applies to directed/undirected as well as to weighted/unweighted multiplex networks. We discuss the application of MultiRank to three major examples of multiplex network datasets: the European Air Transportation Multiplex Network, the Pierre Auger Multiplex Collaboration Network and the FAO Multiplex Trade Network.},
  citeulike-article-id = {14365613},
  citeulike-linkout-0  = {http://arxiv.org/abs/1703.05833},
  citeulike-linkout-1  = {http://arxiv.org/pdf/1703.05833},
  day                  = {16},
  posted-at            = {2017-05-31 15:05:14},
  timestamp            = {2020-02-27 04:41},
}

@Article{Riolo-et-al-2017,
  author               = {Riolo, Maria A. and Cantwell, George T. and Reinert, Gesine and Newman, M. E. J.},
  date                 = {2017-06},
  journaltitle         = {Physical Review E},
  title                = {Efficient method for estimating the number of communities in a network},
  doi                  = {10.1103/physreve.96.032310},
  issn                 = {2470-0045},
  number               = {3},
  volume               = {96},
  abstract             = {While there exist a wide range of effective methods for community detection in networks, most of them require one to know in advance how many communities one is looking for. Here we present a method for estimating the number of communities in a network using a combination of Bayesian inference with a novel prior and an efficient Monte Carlo sampling scheme. We test the method extensively on both real and computer-generated networks, showing that it performs accurately and consistently, even in cases where groups are widely varying in size or structure.},
  citeulike-article-id = {14379438},
  citeulike-linkout-0  = {http://arxiv.org/abs/1706.02324},
  citeulike-linkout-1  = {http://arxiv.org/pdf/1706.02324},
  citeulike-linkout-2  = {http://dx.doi.org/10.1103/physreve.96.032310},
  day                  = {7},
  posted-at            = {2017-10-12 23:36:26},
  timestamp            = {2020-02-27 04:41},
}

@Article{Rossetti-Cazabet-2017,
  author               = {Rossetti, Giulio and Cazabet, Remy},
  date                 = {2017-07},
  journaltitle         = {SSRN e-Print},
  title                = {Community Discovery in Dynamic Networks: a Survey},
  eprint               = {1707.03186},
  eprinttype           = {arXiv},
  url                  = {https://arxiv.org/abs/1707.03186},
  abstract             = {Networks built to model real world phenomena are characeterised by some properties that have attracted the attention of the scientific community: (i) they are organised according to community structure and (ii) their structure evolves with time. Many researchers have worked on methods that can efficiently unveil substructures in complex networks, giving birth to the field of community discovery. A novel and challenging problem started capturing researcher interest recently: the identification of evolving communities. To model the evolution of a system, dynamic networks can be used: nodes and edges are mutable and their presence, or absence, deeply impacts the community structure that composes them. The aim of this survey is to present the distinctive features and challenges of dynamic community discovery, and propose a classification of published approaches. As a "user manual", this work organizes state of art methodologies into a taxonomy, based on their rationale, and their specific instanciation. Given a desired definition of network dynamics, community characteristics and analytical needs, this survey will support researchers to identify the set of approaches that best fit their needs. The proposed classification could also help researchers to choose in which direction should future research be oriented.},
  citeulike-article-id = {14445691},
  citeulike-linkout-0  = {http://arxiv.org/abs/1707.03186},
  citeulike-linkout-1  = {http://arxiv.org/pdf/1707.03186},
  day                  = {11},
  posted-at            = {2017-10-05 11:54:36},
  timestamp            = {2020-02-27 04:41},
}

@Article{Saha-et-al-2015,
  author               = {Saha, Biswajit and Mandal, Amitabha and Tripathy, Soumendu B. and Mukherjee, Debaprasad},
  date                 = {2015-03},
  journaltitle         = {arXiv e-Print},
  title                = {Complex Networks, Communities and Clustering: A survey},
  eprint               = {1503.06277},
  eprinttype           = {arXiv},
  url                  = {https://arxiv.org/abs/1503.06277},
  abstract             = {This paper is an extensive survey of literature on complex network communities and clustering. Complex networks describe a widespread variety of systems in nature and society especially systems composed by a large number of highly interconnected dynamical entities. Complex networks like real networks can also have community structure. There are several types of methods and algorithms for detection and identification of communities in complex networks. Several complex networks have the property of clustering or network transitivity. Some of the important concepts in the field of complex networks are small-world and scale-robustness, degree distributions, clustering, network correlations, random graph models, models of network growth, dynamical processes on networks, etc. Some current areas of research on complex network communities are those on community evolution, overlapping communities, communities in directed networks, community characterization and interpretation, etc. Many of the algorithms or methods proposed for network community detection through clustering are modified versions of or inspired from the concepts of minimum-cut based algorithms, hierarchical connectivity based algorithms, the original GirvanNewman algorithm, concepts of modularity maximization, algorithms utilizing metrics from information and coding theory, and clique based algorithms.},
  citeulike-article-id = {14150071},
  citeulike-linkout-0  = {http://arxiv.org/abs/1503.06277},
  citeulike-linkout-1  = {http://arxiv.org/pdf/1503.06277},
  day                  = {21},
  groups               = {Clustering and network analysis},
  owner                = {cristi},
  posted-at            = {2016-10-01 20:32:16},
  timestamp            = {2020-02-27 04:41},
}

@Article{Sandoval-2017,
  author               = {Sandoval, Leonidas},
  date                 = {2017},
  journaltitle         = {The Journal of Network Theory in Finance},
  title                = {Networks of log returns and volatilities of international stock market indexes},
  doi                  = {10.21314/jntf.2017.033},
  issn                 = {2055-7795},
  abstract             = {n this paper, we build dynamic networks based on correlation and transfer entropy (TE), using both the log returns and the volatilities (here associated with absolute values) of ninety-seven stock market indexes from various parts of the world between 2000 and 2016. The topologies of these networks are analyzed using node strength for networks based on correlation, and in and out node strengths for networks based on TE. Our results indicate that node strengths peak in times of crisis, such as the global financial crisis of 2008 and the European sovereign debt crisis, as well as in the years after 2010. Our results for volatilities also indicate strong relations between the indexes of Arab countries.},
  citeulike-article-id = {14510346},
  citeulike-linkout-0  = {http://dx.doi.org/10.21314/jntf.2017.033},
  posted-at            = {2017-12-30 12:28:10},
  timestamp            = {2020-02-27 04:41},
}

@Article{Sarkar-et-al-2016,
  author               = {Sarkar, Somwrita and Chawla, Sanjay and Robinson, P. A. and Fortunato, Santo},
  date                 = {2016-06},
  journaltitle         = {Physical Review E},
  title                = {Eigenvector dynamics under perturbation of modular networks},
  doi                  = {10.1103/physreve.93.062312},
  issn                 = {2470-0045},
  number               = {6},
  volume               = {93},
  abstract             = {Rotation dynamics of eigenvectors of modular network adjacency matrices under random perturbations are presented. In the presence of q communities, the number of eigenvectors corresponding to the q largest eigenvalues form a "community" eigenspace and rotate together, but separately from that of the "bulk" eigenspace spanned by all the other eigenvectors. Using this property, the number of modules or clusters in a network can be estimated in an algorithm-independent way. A general argument and derivation for the theoretical detectability limit for sparse modular networks with q communities is presented, beyond which modularity persists in the system but cannot be detected. It is shown that for detecting the clusters or modules using the adjacency matrix, there is a "band" in which it is hard to detect the clusters even before the theoretical detectability limit is reached, and for which the theoretically predicted detectability limit forms the sufficient upper bound. Analytic estimations of these bounds are presented and empirically demonstrated.},
  citeulike-article-id = {14449656},
  citeulike-linkout-0  = {http://dx.doi.org/10.1103/physreve.93.062312},
  day                  = {20},
  posted-at            = {2017-10-12 23:30:53},
  timestamp            = {2020-02-27 04:41},
}

@Article{Schwab-et-al-2019,
  author         = {Schwab, Patrick and Miladinovic, Djordje and Karlen, Walter},
  date           = {2019-07-17},
  journaltitle   = {Proceedings of the AAAI Conference on Artificial Intelligence},
  title          = {Granger-Causal Attentive Mixtures of Experts: Learning Important Features with Neural Networks},
  doi            = {10.1609/aaai.v33i01.33014846},
  issn           = {2374-3468},
  pages          = {4846--4853},
  url            = {https://aaai.org/ojs/index.php/{AAAI}/article/view/4412},
  urldate        = {2019-10-09},
  volume         = {33},
  abstract       = {Knowledge of the importance of input features towards decisions made by machine-learning models is essential to increase our understanding of both the models and the underlying data. Here, we present a new approach to estimating feature importance with neural networks based on the idea of distributing the features of interest among experts in an attentive mixture of experts (AME). AMEs use attentive gating networks trained with a Granger-causal objective to learn to jointly produce accurate predictions as well as estimates of feature importance in a single model. Our experiments show (i) that the feature importance estimates provided by AMEs compare favourably to those provided by state-of-theart methods, (ii) that AMEs are significantly faster at estimating feature importance than existing methods, and (iii) that the associations discovered by AMEs are consistent with those reported by domain experts.},
  day            = {17},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 04:41},
}

@Article{Sengupta-2018,
  author         = {Sengupta, Srijan},
  date           = {2018-07-24},
  journaltitle   = {arXiv e-Print},
  title          = {Anomaly detection in static networks using egonets},
  url            = {https://arxiv.org/abs/1807.08925},
  abstract       = {Network data has rapidly emerged as an important and active area of statistical methodology. In this paper we consider the problem of anomaly detection in networks. Given a large background network, we seek to detect whether there is a small anomalous subgraph present in the network, and if such a subgraph is present, which nodes constitute the subgraph. We propose an inferential tool based on egonets to answer this question. The proposed method is computationally efficient and naturally amenable to parallel computing, and easily extends to a wide variety of network models. We demonstrate through simulation studies that the egonet method works well under a wide variety of network models. We obtain some fascinating empirical results by applying the egonet method on several well-studied benchmark datasets.},
  day            = {24},
  f1000-projects = {QuantInvest},
  groups         = {Anomaly_Detection},
  timestamp      = {2020-02-27 04:41},
}

@Article{Sensoy-et-al-2016,
  author               = {Sensoy, Ahmet and Ozturk, Kevser and Hacihasanoglu, Erk and Tabak, Benjamin M.},
  date                 = {2016-06},
  journaltitle         = {Journal of Financial Stability},
  title                = {Not all emerging markets are the same: A classification approach with correlation based networks},
  doi                  = {10.1016/j.jfs.2016.06.009},
  issn                 = {1572-3089},
  abstract             = {Use networks to compare the integration of emerging sovereign bond markets. The average DCC between bond yields significantly increases after the crisis. Increase is caused by clusters of countries having high within-cluster correlation. Geographic proximity plays important role in high within-cluster co-movement. Budget balance to GDP ratio plays an important role in post-crisis segmentation. Using dynamic conditional correlations and network theory, this study brings a novel interdisciplinary framework to define the integration and segmentation of emerging countries. The individual EMBI+ spreads of 13 emerging countries from January 2003 to December 2013 are used to compare their interaction structure before (phase 1) and after (phase 2) the global financial crisis. Accordingly, the unweighted average of dynamic conditional correlations between cross country bond returns significantly increases in phase 2. At first glance, the increased co-movement degree suggests an integration of the sample countries after the crisis. However, using correlation based stable networks, we show that this is not enough to make such a strong conclusion. In particular, we reveal that the increased average correlation is more likely to be caused by clusters of countries that exhibit high within-cluster co-movement but not between-cluster co-movement. Potential reasons for the post-crisis segmentation and important implications for international investors and policymakers are discussed.},
  citeulike-article-id = {14324926},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.jfs.2016.06.009},
  groups               = {Networks and investment management, Clustering and network analysis},
  posted-at            = {2017-03-30 22:20:51},
  timestamp            = {2020-02-27 04:41},
}

@Article{Sensoy-Tabak-2014,
  author               = {Sensoy, Ahmet and Tabak, Benjamin M.},
  date                 = {2014-11},
  journaltitle         = {Physica A: Statistical Mechanics and its Applications},
  title                = {Dynamic spanning trees in stock market networks: The case of Asia-Pacific},
  doi                  = {10.1016/j.physa.2014.07.067},
  issn                 = {0378-4371},
  pages                = {387--402},
  volume               = {414},
  abstract             = {We propose a new procedure called Dynamic Spanning Trees (DST). Stock market inter-connectedness in the Asia-Pacific region is analyzed. The DST significantly shrinks over time. Hong Kong is found to be the key financial market. DST has a significantly increased stability in the last few years. This article proposes a new procedure to evaluate Asia Pacific stock market interconnections using a dynamic setting. Dynamic spanning trees (DST) are constructed using an ARMA-FIEGARCH-cDCC process. The main results show that: 1. the DST significantly shrinks over time; 2. Hong Kong is found to be the key financial market; 3. the DST has a significantly increased stability in the last few years; 4. the removal of the key player has two effects: there is no clear key market any longer and the stability of the DST significantly decreases. These results are important for the design of policies that help develop stock markets and for academics and practitioners.},
  citeulike-article-id = {14149944},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.physa.2014.07.067},
  groups               = {Networks and investment management, Invest_Network},
  owner                = {cristi},
  posted-at            = {2016-10-01 13:45:15},
  timestamp            = {2020-02-27 04:41},
}

@Article{Shai-et-al-2017,
  author               = {Shai, Saray and Stanley, Natalie and Granell, Clara and Taylor, Dane and Mucha, Peter J.},
  date                 = {2017-05},
  journaltitle         = {SSRN e-Print},
  title                = {Case studies in network community detection},
  eprint               = {1705.02305},
  eprinttype           = {arXiv},
  url                  = {https://arxiv.org/abs/1705.02305},
  abstract             = {Community structure describes the organization of a network into subgraphs that contain a prevalence of edges within each subgraph and relatively few edges across boundaries between subgraphs. The development of community-detection methods has occurred across disciplines, with numerous and varied algorithms proposed to find communities. As we present in this Chapter via several case studies, community detection is not just an "end game" unto itself, but rather a step in the analysis of network data which is then useful for furthering research in the disciplinary domain of interest. These case-study examples arise from diverse applications, ranging from social and political science to neuroscience and genetics, and we have chosen them to demonstrate key aspects of community detection and to highlight that community detection, in practice, should be directed by the application at hand.},
  citeulike-article-id = {14445685},
  citeulike-linkout-0  = {http://arxiv.org/abs/1705.02305},
  citeulike-linkout-1  = {http://arxiv.org/pdf/1705.02305},
  day                  = {5},
  posted-at            = {2017-10-05 11:52:13},
  timestamp            = {2020-02-27 04:41},
}

@Article{Silva-et-al-2015,
  author               = {Silva, Filipi N. and Comin, Cesar H. and Thomas and Rodrigues, Francisco A. and Ye, Cheng and Wilson, Richard C. and Hancock, Edwin and Luciano da},
  date                 = {2015-07},
  journaltitle         = {SSRN e-Print},
  title                = {Modular Dynamics of Financial Market Networks},
  eprint               = {1501.05040},
  eprinttype           = {arXiv},
  url                  = {https://arxiv.org/abs/1501.05040},
  abstract             = {The financial market is a complex dynamical system composed of a large variety of intricate relationships between several entities, such as banks, corporations and institutions. At the heart of the system lies the stock exchange mechanism, which establishes a time-evolving network of trades among companies and individuals. Such network can be inferred through correlations between time series of companies stock prices, allowing the overall system to be characterized by techniques borrowed from network science. Here we study the presence of communities in the inferred stock market network, and show that the knowledge about the communities alone can provide a nearly complete representation of the system topology. This is done by defining a simple null model, a randomized version of the studied network sharing only the sizes and interconnectivity between communities observed. We show that many topological characteristics of the inferred networks are carried over the networks generated by the null model. In particular, we find that in periods of instability, such as during a financial crisis, the network strays away from a state of well-defined community structure to a much more uniform topological organization. We show that the framework presented here provides a good null model representation of topological variations taking place in the market during crises. Also, the general approach used in this work can be extended to other systems.},
  citeulike-article-id = {14445698},
  citeulike-linkout-0  = {http://arxiv.org/abs/1501.05040},
  citeulike-linkout-1  = {http://arxiv.org/pdf/1501.05040},
  day                  = {28},
  posted-at            = {2017-10-05 12:05:47},
  timestamp            = {2020-02-27 04:41},
}

@TechReport{Smith-2018b,
  author      = {Russell Smith},
  date        = {2018},
  institution = {FactSet},
  title       = {Network risk: the unseen aspect of portfolio risk},
  url         = {https://insight.factset.com/network-risk-the-unseen-aspect-of-portfolio-risk},
  timestamp   = {2020-02-27 04:41},
}

@Article{Sondhi-Shojaie-2019,
  author         = {Sondhi, Arjun and Shojaie, Ali},
  date           = {2019},
  journaltitle   = {Journal of Machine Learning Research},
  title          = {The Reduced PC-Algorithm: Improved Causal Structure Learning in Large Random Networks},
  url            = {http://www.jmlr.org/papers/v20/17-601.html},
  urldate        = {2020-01-17},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 04:41},
}

@Article{Soramaki-et-al-2016,
  author               = {Soramaki, Kimmo and Cook, Samantha and Laubsch, Alan},
  date                 = {2016},
  journaltitle         = {Journal of Network Theory in Finance},
  title                = {A network-based method for visual identification of systemic risks},
  abstract             = {Financial markets provide vast numbers of signals about the performance of companies, banks, assets and economies. These signals can be used by risk managers and regulators to better understand economic dependencies, correlations and phase transitions. In this paper, we present a methodology for mapping multiple dimensions of time series data into two-dimensional visual layouts by applying methods from statistics and network theory. The methodology involves identifying important correlations between the time series as well as monitoring individual series to determine which ones have extreme return values compared with their past performance. Analysis is presented visually to give quick insight into a complex system moving in time; for example, systemically important assets are easily recognizable as those that are central in the minimum spanning tree structure of the correlation matrix, and systemic events are visible as large numbers of assets having extreme values. We present historical scenarios to illustrate the methodology},
  citeulike-article-id = {14438479},
  posted-at            = {2017-09-26 20:55:02},
  timestamp            = {2020-02-27 04:41},
}

@Article{Stavroglou-et-al-2017,
  author               = {Stavroglou, Stavros and Pantelous, Athanasios and Soramaki, Kimmo and Zuev, Konstantin},
  date                 = {2017},
  journaltitle         = {The Journal of Network Theory in Finance},
  title                = {Causality networks of financial assets},
  doi                  = {10.21314/jntf.2017.029},
  issn                 = {2055-7795},
  number               = {2},
  pages                = {17--67},
  volume               = {3},
  abstract             = {Through financial network analysis we ascertain the existence of important causal behavior between certain financial assets, as inferred from eight different causality methods. To the best of our knowledge, this is the first extensive comparative analysis of financial networks as produced by various causality methods. In addition, some specific nonlinear causalities are used for the first time in financial network research. Our results contradict the efficient market hypothesis and open new horizons for further investigation and possible arbitrage opportunities. Moreover, we find some evidence that two of the causality methods used, at least to some extent, could have warned us about the financial crisis of 2007-2009. Furthermore, we test the similarity percentage of the eight causality methods and we find that the most similar pair of causality-induced networks is on average less than 50\% similar throughout the time period examined, thus rendering the comparability and substitutability of these causality methods rather dubious. We also rank the assets in terms of overall out-strength centrality and we find that there is an underlying bonds regime almost monopolizing (in some cases) the causality methods. Finally, using network visualization, we observe an established pattern (ie, across all causalities) of oil's increasing role as the financial network faced the Chinese stock market crash.},
  citeulike-article-id = {14444463},
  citeulike-linkout-0  = {http://dx.doi.org/10.21314/jntf.2017.029},
  posted-at            = {2017-10-03 08:12:03},
  timestamp            = {2020-02-27 04:41},
}

@Article{Summer-2013,
  author         = {Summer, Martin},
  date           = {2013-11},
  journaltitle   = {Annual Review of Financial Economics},
  title          = {Financial contagion and network analysis},
  doi            = {10.1146/annurev-financial-110112-120948},
  issn           = {1941-1367},
  number         = {1},
  pages          = {277--297},
  volume         = {5},
  f1000-projects = {QuantInvest},
  groups         = {Contagion},
  timestamp      = {2020-02-27 04:41},
}

@Article{Sun-Lau-2017,
  author               = {Sun, Ariel J. and Chan-Lau, Jorge A.},
  date                 = {2017-10},
  journaltitle         = {Quantitative Finance},
  title                = {Financial networks and interconnectedness in an advanced emerging market economy},
  doi                  = {10.1080/14697688.2017.1357976},
  issn                 = {1469-7688},
  number               = {12},
  pages                = {1833--1858},
  volume               = {17},
  abstract             = {Financial networks could become fragile during periods of economic and financial distress, since interconnectedness among participating firms could transmit and amplify adverse shocks. Relying on balance sheet data, complemented with information on interbank exposures, this paper analyses interconnectedness in an advanced emerging market economy, using two complementary approaches. The first approach focuses on the financial network topology, and finds that the financial system resembles a highly clustered small world network, with in and out-degrees, connectivity and exposures exhibiting a (double) heavy tail behaviour, which favours the formation of strong community structure and preferential attachment in the network. Bank size is Pareto distributed, and highly correlated with centrality. The second approach focuses on how the network topology contributes to the transmission of shocks by modelling default contagion, using balance sheet network analysis. It finds that direct counterparty credit exposure poses less risk to the banking system than fire sale losses triggered by liquidity shocks. Both approaches, either using a topological or induced system losses perspective, identify systemically important financial institutions (SIFIs) consistently by accounting for both macro and microprudential risk dimensions.},
  citeulike-article-id = {14497276},
  citeulike-linkout-0  = {http://dx.doi.org/10.1080/14697688.2017.1357976},
  day                  = {05},
  posted-at            = {2017-12-06 03:37:48},
  timestamp            = {2020-02-27 04:41},
}

@InCollection{Tang-Bu-2017,
  author               = {Tang, Wenjin and Bu, Hui},
  booktitle            = {International Conference on Service Systems and Service Management},
  date                 = {2017-06},
  title                = {On building causal networks for Chinese stock market understanding},
  doi                  = {10.1109/icsssm.2017.7996308},
  isbn                 = {978-1-5090-6370-3},
  location             = {Dalian, China},
  pages                = {1--6},
  publisher            = {IEEE},
  abstract             = {This study proposes a causal network construction method based on Granger causality test rather than correlation coefficient to investigate the inherent structure of the stock market. We analyze the characteristics, community structure and nodes' influence of the network formed following our method. Furthermore, this study investigates the reasons why the stock market performs a certain relationship among stocks and why some stocks can be important in the stock market. This is the first paper tries to illustrate the mechanism of stock network. This paper proposes a new way to analyze the formation reasons of community structure and important nodes of the stock market, which integrates complex networks and financial econometric methods. This paper conducts empirical study for Chinese stock market to illustrate the usefulness and advantage of our new methods. The empirical results show that it is the pricing factors such as yearly abnormal returns, price volatility, price level and leverage that drives the stocks rather than industry sector. Our study provides new evidence to help us understand the stock market and stock pricing. Particularly, the results of this paper can help understand the sector rotation effect in the stock market.},
  citeulike-article-id = {14445607},
  citeulike-linkout-0  = {http://dx.doi.org/10.1109/icsssm.2017.7996308},
  posted-at            = {2017-10-05 09:04:10},
  timestamp            = {2020-02-27 04:41},
}

@Article{Taylor-et-al-2017,
  author               = {Taylor, Dane and Myers, Sean A. and Clauset, Aaron and Porter, Mason A. and Mucha, Peter J.},
  date                 = {2017-01},
  journaltitle         = {Multiscale Modeling \& Simulation},
  title                = {Eigenvector-Based Centrality Measures for Temporal Networks},
  doi                  = {10.1137/16m1066142},
  issn                 = {1540-3459},
  number               = {1},
  pages                = {537--574},
  volume               = {15},
  abstract             = {Numerous centrality measures have been developed to quantify the importances of nodes in time-independent networks, and many of them can be expressed as the leading eigenvector of some matrix. With the increasing availability of network data that changes in time, it is important to extend such eigenvector-based centrality measures to time-dependent networks. In this paper, we introduce a principled generalization of network centrality measures that is valid for any eigenvector-based centrality. We consider a temporal network with N nodes as a sequence of T layers that describe the network during different time windows, and we couple centrality matrices for the layers into a supracentrality matrix of size NT x NT whose dominant eigenvector gives the centrality of each node i at each time t. We refer to this eigenvector and its components as a joint centrality, as it reflects the importances of both the node i and the time layer t. We also introduce the concepts of marginal and conditional centralities, which facilitate the study of centrality trajectories over time. We find that the strength of coupling between layers is important for determining multiscale properties of centrality, such as localization phenomena and the time scale of centrality changes. In the strong-coupling regime, we derive expressions for time-averaged centralities, which are given by the zeroth-order terms of a singular perturbation expansion. We also study first-order terms to obtain first-order-mover scores, which concisely describe the magnitude of the nodes' centrality changes over time. As examples, we apply our method to three empirical temporal networks: the United States Ph.D. exchange in mathematics, costarring relationships among top-billed actors during the Golden Age of Hollywood, and citations of decisions from the United States Supreme Court.},
  citeulike-article-id = {14449645},
  citeulike-linkout-0  = {http://dx.doi.org/10.1137/16m1066142},
  posted-at            = {2017-10-12 22:57:55},
  timestamp            = {2020-02-27 04:41},
}

@Article{Tse-et-al-2010,
  author               = {Tse, Chi K. and Liu, Jing and Lau, Francis C. M.},
  date                 = {2010-09},
  journaltitle         = {Journal of Empirical Finance},
  title                = {A network perspective of the stock market},
  doi                  = {10.1016/j.jempfin.2010.04.008},
  issn                 = {0927-5398},
  number               = {4},
  pages                = {659--667},
  volume               = {17},
  abstract             = {Complex networks are constructed to study correlations between the closing prices for all US stocks that were traded over two periods of time (from July 2005 to August 2007; and from June 2007 to May 2009). The nodes are the stocks, and the connections are determined by cross correlations of the variations of the stock prices, price returns and trading volumes within a chosen period of time. Specifically, a winner-take-all approach is used to determine if two nodes are connected by an edge. So far, no previous work has attempted to construct a full network of US stock prices that gives full information about their interdependence. We report that all networks based on connecting stocks of highly correlated stock prices, price returns and trading volumes, display a scalefree degree distribution. The results from this work clearly suggest that the variation of stock prices are strongly influenced by a relatively small number of stocks. We propose a new approach for selecting stocks for inclusion in a stock index and compare it with existing indexes. From the composition of the highly connected stocks, it can be concluded that the market is heavily dominated by stocks in the financial sector.},
  citeulike-article-id = {7272716},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.jempfin.2010.04.008},
  day                  = {16},
  groups               = {Returns_Not_Normal},
  posted-at            = {2017-10-19 16:36:58},
  timestamp            = {2020-02-27 04:41},
}

@Article{Tumminello-et-al-2007,
  author               = {Tumminello, M. and Di Matteo, T. and Aste, T. and Mantegna, R. N.},
  date                 = {2007},
  journaltitle         = {The European Physical Journal B},
  title                = {Correlation based networks of equity returns sampled at different time horizons},
  doi                  = {10.1140/epjb/e2006-00414-4},
  number               = {2},
  pages                = {209--217},
  volume               = {55},
  abstract             = {We investigate the planar maximally filtered graphs of the portfolio of the 300 most capitalized stocks traded at the New York Stock Exchange during the time period 2001-2003. Topological properties such as the average length of shortest paths, the betweenness and the degree are computed on different planar maximally filtered graphs generated by sampling the returns at different time horizons ranging from 5 min up to one trading day. This analysis confirms that the selected stocks compose a hierarchical system progressively structuring as the sampling time horizon increases. Finally, a cluster formation, associated to economic sectors, is quantitatively investigated.},
  citeulike-article-id = {14150051},
  citeulike-linkout-0  = {http://dx.doi.org/10.1140/epjb/e2006-00414-4},
  citeulike-linkout-1  = {http://link.springer.com/article/10.1140/epjb/e2006-00414-4},
  groups               = {Networks and investment management},
  owner                = {cristi},
  posted-at            = {2016-10-01 20:05:58},
  publisher            = {EDP Sciences},
  timestamp            = {2020-02-27 04:41},
}

@Article{Tumminello-et-al-2010,
  author               = {Tumminello, Michele and Lillo, Fabrizio and Mantegna, Rosario N.},
  date                 = {2010-07},
  journaltitle         = {Journal of Economic Behavior and Organization},
  title                = {Correlation, hierarchies, and networks in financial markets},
  doi                  = {10.1016/j.jebo.2010.01.004},
  issn                 = {0167-2681},
  number               = {1},
  pages                = {40--58},
  volume               = {75},
  abstract             = {We discuss some methods to quantitatively investigate the properties of correlation matrices. Correlation matrices play an important role in portfolio optimization and in several other quantitative descriptions of asset price dynamics in financial markets. Here, we discuss how to define and obtain hierarchical trees, correlation based trees and networks from a correlation matrix. The hierarchical clustering and other procedures performed on the correlation matrix to detect statistically reliable aspects of it are seen as filtering procedures of the correlation matrix. We also discuss a method to associate a hierarchically nested factor model to a hierarchical tree obtained from a correlation matrix. The information retained in filtering procedures and its stability with respect to statistical fluctuations is quantified by using the Kullback-Leibler distance.},
  citeulike-article-id = {7539837},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.jebo.2010.01.004},
  groups               = {Networks and investment management, Clustering and network analysis, PortfOptim_Network},
  owner                = {cristi},
  posted-at            = {2016-06-20 03:13:56},
  timestamp            = {2020-02-27 04:41},
}

@Article{Wang-et-al-2014c,
  author               = {Wang, Gang-Jin and Xie, Chi and Zhang, Peng and Han, Feng and Chen, Shou},
  date                 = {2014},
  journaltitle         = {Discrete Dynamics in Nature and Society},
  title                = {Dynamics of Foreign Exchange Networks: A Time-Varying Copula Approach},
  doi                  = {10.1155/2014/170921},
  issn                 = {1026-0226},
  pages                = {1--11},
  volume               = {2014},
  abstract             = {Based on a time-varying copula approach and the minimum spanning tree (MST) method, we propose a time-varying correlation network-based approach to investigate dynamics of foreign exchange (FX) networks. In piratical terms, we choose the daily FX rates of 42 major currencies in the international FX market during the period of 2005-2012 as the empirical data. The empirical results show that (i) the distributions of cross-correlation coefficients (distances) in the international FX market (network) are fat-tailed and negatively skewed; (ii) financial crises during the analyzed period have a great effect on the FX network's topology structure and lead to the US dollar becoming more centered in the MST; (iii) the topological measures of the FX network show a large fluctuation and display long-range correlations; (iv) the FX network has a long-term memory effect and presents a scale-free behavior in the most of time; and (v) a great majority of links between currencies in the international FX market survive from one time to the next, and multistep survive rates of FX networks drop sharply as the time increases.},
  citeulike-article-id = {14150062},
  citeulike-linkout-0  = {http://dx.doi.org/10.1155/2014/170921},
  owner                = {cristi},
  posted-at            = {2016-10-01 20:25:25},
  timestamp            = {2020-02-27 04:41},
}

@Article{Wang-et-al-2016b,
  author               = {Wang, Gang-Jin and Xie, Chi and Chen, Shou},
  date                 = {2016},
  journaltitle         = {Journal of Economic Interaction and Coordination},
  title                = {Multiscale correlation networks analysis of the US stock market: a wavelet analysis},
  doi                  = {10.1007/s11403-016-0176-x},
  pages                = {1--34},
  abstract             = {We investigate the interaction among stocks in the US market over various time horizons from a network perspective. Unlike the high-frequency data-driven multiscale correlation networks used in previous works, we propose method-driven multiscale correlation networks that are constructed by wavelet analysis and topological methods of minimum spanning tree (MST) and planar maximally filtered graph (PMFG). Using these techniques, we construct MST and PMFG networks of the US stock market at different time scales. The key empirical results show that (1) the topological structures and properties of networks vary across time horizons, (2) there is a sectoral clustering effect in the networks at small time scales, and (3) only a part of connections in the networks survives from one time scale to the next. Our results in terms of MSTs and PMFGs for different time scales supply a new perspective for participants in financial markets, especially for investors or hedgers who have different investment or hedging horizons.},
  citeulike-article-id = {14357942},
  citeulike-linkout-0  = {http://dx.doi.org/10.1007/s11403-016-0176-x},
  citeulike-linkout-1  = {http://link.springer.com/article/10.1007/s11403-016-0176-x},
  groups               = {Networks and investment management},
  posted-at            = {2017-05-16 14:20:38},
  publisher            = {Springer Berlin Heidelberg},
  timestamp            = {2020-02-27 04:41},
}

@Article{Wang-et-al-2016e,
  author               = {Wang, Gang-Jin and Xie, Chi and Stanley},
  date                 = {2016},
  journaltitle         = {Computational Economics},
  title                = {Correlation Structure and Evolution of World Stock Markets: Evidence from Pearson and Partial Correlation-Based Networks},
  doi                  = {10.1007/s10614-016-9627-7},
  pages                = {1--29},
  abstract             = {We construct a Pearson correlation-based network and a partial correlation-based network, i.e., two minimum spanning trees (MST-Pearson and MST-Partial), to analyze the correlation structure and evolution of world stock markets. We propose a new method for constructing the MST-Partial. We use daily price indices of 57 stock markets from 2005 to 2014 and find (i) that the distributions of the Pearson correlation coefficient and the partial correlation coefficient differ completely, which implies that the correlation between pairs of stock markets is greatly affected by other markets, and (ii) that both MSTs are scale-free networks and that the MST-Pearson network is more compact than the MST-Partial. Depending on the geographical locations of the stock markets, two large clusters (i.e., European and Asia-Pacific) are formed in the MST-Pearson, but in the MST-Partial the European cluster splits into two subgroups bridged by the American cluster with the USA at its center. We also find (iii) that the centrality structure indicates that outcomes obtained from the MST-Partial are more reasonable and useful than those from the MST-Pearson, e.g., in the MST-Partial, markets of the USA, Germany, and Japan clearly serve as hubs or connectors in world stock markets, (iv) that during the 2008 financial crisis the time-varying topological measures of the two MSTs formed a valley, implying that during a crisis stock markets are tightly correlated and information (e.g., about price fluctuations) is transmitted quickly, and (v) that the presence of multi-step survival ratios indicates that network stability decreases as step length increases. From these findings we conclude that the MST-Partial is an effective new tool for use by international investors and hedge-fund operators.},
  citeulike-article-id = {14398613},
  citeulike-linkout-0  = {http://dx.doi.org/10.1007/s10614-016-9627-7},
  citeulike-linkout-1  = {http://link.springer.com/article/10.1007/s10614-016-9627-7},
  groups               = {Networks and investment management},
  posted-at            = {2017-07-23 14:25:20},
  publisher            = {Springer US},
  timestamp            = {2020-02-27 04:41},
}

@Article{Wang-Wang-2019a,
  author         = {Wang, Jifei and Wang, Lingjing},
  date           = {2019-10-16},
  journaltitle   = {arXiv e-Print},
  title          = {Residual Switching Network for Portfolio Optimization},
  url            = {https://arxiv.org/abs/1910.07564},
  urldate        = {2019-10-24},
  abstract       = {This paper studies deep learning methodologies for portfolio optimization in the US equities market. We present a novel residual switching network that can automatically sense changes in market regimes and switch between momentum and reversal predictors accordingly. The residual switching network architecture combines two separate residual networks (ResNets), namely a switching module that learns stock market conditions, and the main module that learns momentum and reversal predictors. We demonstrate that over-fitting noisy financial data can be controlled with stacked residual blocks and further incorporating the attention mechanism can enhance powerful predictive properties. Over the period 2008 to H12017, the residual switching network (Switching-ResNet) strategy verified superior out-of-sample performance with an average annual Sharpe ratio of 2.22, compared with an average annual Sharpe ratio of 0.81 for the ANN-based strategy and 0.69 for the linear model.},
  day            = {16},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 04:41},
}

@Article{Wang-Xie-2016,
  author               = {Wang, Gang-Jin and Xie, Chi},
  date                 = {2016-03},
  journaltitle         = {Expert Systems with Applications},
  title                = {Tail dependence structure of the foreign exchange market: A network view},
  doi                  = {10.1016/j.eswa.2015.10.037},
  issn                 = {0957-4174},
  pages                = {164--179},
  volume               = {46},
  abstract             = {Tail dependence structure of the FX market is studied from a perspective of network. A concept of lower- and upper-tail dependence networks is proposed. The networks are built by combining the SJC copula with the MST and PMFG. Some notable differences between the two tail dependence networks are found. 3-cliques, 4-cliques and communities are detected in the two tail dependence PMFGs. Tail dependence of financial entities describes when the price of one financial asset has an extreme fluctuation (e.g., price sharply rises or falls), the degree of its effect on the price fluctuation of another asset. Under the background of the global financial crisis, tail dependence structure of financial entities plays an important role in financial risk management, portfolio selection, and asset pricing. In this paper, we propose a concept of tail dependence networks to investigate the tail dependence structure of the foreign exchange (FX) market. Lower- and upper-tail dependence networks for 42 major currencies in the FX market from 2005 to 2012 are constructed by combing the symmetrized Joe-Clayton copula model and two filtered graph algorithms, i.e., the minimum spanning tree (MST) and the planar maximally filtered graph (PMFG). We also construct the tail dependence hierarchical trees (HTs) associated with the MSTs to analyze the currency clusters. We find that (1) the two series of lower- and upper-tail dependence coefficients present different statistical properties; (2) the upper-tail dependence networks are tighter than the lower-tail dependence networks; and (3) different currency clusters, cliques and communities are respectively found in the two tail dependence networks. The key empirical results indicate that market participants should consider the different topological features at different market situations (e.g., a booming market or a recession market) to make decisions on the investing or hedging strategies. Overall, our obtained results based on the tail dependence networks are new insights in financial management and supply a novel analytical tool for market participants.},
  citeulike-article-id = {14147197},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.eswa.2015.10.037},
  groups               = {Networks and investment management},
  owner                = {cristi},
  posted-at            = {2016-09-27 14:50:24},
  timestamp            = {2020-02-27 04:41},
}

@Article{Wen-et-al-2019,
  author         = {Wen, Fenghua and Yang, Xin and Zhou, Wei-Xing},
  date           = {2019-01},
  journaltitle   = {International Journal of Finance \& Economics},
  title          = {Tail dependence networks of global stock markets},
  doi            = {10.1002/ijfe.1679},
  issn           = {1076-9307},
  number         = {1},
  pages          = {558--567},
  urldate        = {2019-04-19},
  volume         = {24},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 04:41},
}

@Article{Yan-et-al-2014a,
  author               = {Yan, Xin-Guo and Xie, Chi and Wang, Gang-Jin},
  date                 = {2014-08},
  journaltitle         = {EPL (Europhysics Letters)},
  title                = {The stability of financial market networks},
  doi                  = {10.1209/0295-5075/107/48002},
  issn                 = {0295-5075},
  number               = {4},
  pages                = {48002+},
  volume               = {107},
  abstract             = {We investigate the stability of a financial market network by measuring its topological robustness, namely the ability of the network to resist structural or topological changes. The closing prices of 710 stocks in the Shanghai Stock Exchange (SSE) from 2005 to 2011 are chosen as the empirical data. We divide the period into three sub-periods: before, during, and after the US sub-prime crisis. By monitoring the size of the clusters which fall apart from the network after removing the nodes (i.e., the listed companies in the SSE), we find that: (i) the SSE network is sensitive to the nodes' failure, which implies that the network is unstable. (ii) the SSE network before the financial crisis has the strongest robustness against the intentional topological damage; (iii) the hubs (i.e., highly connected nodes) connect with each other directly and play a vital important role in maintaining SSE network's stability.},
  citeulike-article-id = {14292070},
  citeulike-linkout-0  = {http://dx.doi.org/10.1209/0295-5075/107/48002},
  day                  = {01},
  groups               = {Networks and investment management},
  posted-at            = {2017-03-05 17:59:36},
  timestamp            = {2020-02-27 04:41},
}

@Article{Yan-vanSerooskerken-2015,
  author       = {Yan, Wanfeng and { van Serooskerken}, Edgar van Tuyll},
  date         = {2015},
  journaltitle = {PLOS One},
  title        = {Forecasting Financial Extremes: A Network Degree Measure of Super-Exponential Growth},
  doi          = {10.1371/journal.pone.0128908},
  url          = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0128908},
  abstract     = {Investors in stock market are usually greedy during bull markets and scared during bear markets. The greed or fear spreads across investors quickly. This is known as the herding effect, and often leads to a fast movement of stock prices. During such market regimes, stock prices change at a super-exponential rate and are normally followed by a trend reversal that corrects the previous overreaction.

In this paper, we construct an indicator to measure the magnitude of the super-exponential growth of stock prices, by measuring the degree of the price network, generated from the price time series.

Twelve major international stock indices have been investigated. Error diagram tests show that this new indicator has strong predictive power for financial extremes, both peaks and troughs. By varying the parameters used to construct the error diagram, we show the predictive power is very robust. The new indicator has a better performance than the LPPL pattern recognition indicator.},
  groups       = {Invest_Network},
  howpublished = {Available at http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0128908},
  owner        = {zkgst0c},
  timestamp    = {2020-02-27 04:41},
}

@Article{You-et-al-2015,
  author               = {You, Tao and Fiedor, Pawel and Holda, Artur},
  date                 = {2015-06},
  journaltitle         = {Journal of Risk and Financial Management},
  title                = {Network Analysis of the Shanghai Stock Exchange Based on Partial Mutual Information},
  doi                  = {10.3390/jrfm8020266},
  number               = {2},
  pages                = {266--284},
  volume               = {8},
  abstract             = {Analyzing social systems, particularly financial markets, using a complex network approach has become one of the most popular fields within econophysics. A similar trend is currently appearing within the econometrics and finance communities, as well. In this study, we present a state-of-the-artmethod for analyzing the structure and risk within stockmarkets, treating them as complex networks using model-free, nonlinear dependency measures based on information theory. This study is the first network analysis of the stockmarket in Shanghai using a nonlinear network methodology. Further, it is often assumed that markets outside the United States and Western Europe are inherently riskier. We find that the Chinese stock market is not structurally risky, contradicting this popular opinion. We use partial mutual information to create filtered networks representing the Shanghai stock exchange, comparing them to networks based on Pearson's correlation. Consequently, we discuss the structure and characteristics of both the presented methods and the Shanghai stock exchange. This paper provides an insight into the cutting edge methodology designed for analyzing complex financial networks, as well as analyzing the structure of the market in Shanghai and, as such, is of interest to both researchers and financial analysts.},
  citeulike-article-id = {14445595},
  citeulike-linkout-0  = {http://dx.doi.org/10.3390/jrfm8020266},
  citeulike-linkout-1  = {http://www.mdpi.com/1911-8074/8/2/266},
  citeulike-linkout-2  = {http://www.mdpi.com/1911-8074/8/2/266/pdf},
  day                  = {01},
  posted-at            = {2017-10-05 08:48:12},
  timestamp            = {2020-02-27 04:41},
}

@Article{Zanin-et-al-2016,
  author               = {Zanin, M. and Papo, D. and Sousa, P. A. and Menasalvas, E. and Nicchi, A. and Kubik, E. and Boccaletti, S.},
  date                 = {2016-05},
  journaltitle         = {Physics Reports},
  title                = {Combining complex networks and data mining: Why and how},
  doi                  = {10.1016/j.physrep.2016.04.005},
  issn                 = {0370-1573},
  pages                = {1--44},
  volume               = {635},
  abstract             = {The increasing power of computer technology does not dispense with the need to extract meaningful information out of data sets of ever growing size, and indeed typically exacerbates the complexity of this task. To tackle this general problem, two methods have emerged, at chronologically different times, that are now commonly used in the scientific community: data mining and complex network theory. Not only do complex network analysis and data mining share the same general goal, that of extracting information from complex systems to ultimately create a new compact quantifiable representation, but they also often address similar problems too. In the face of that, a surprisingly low number of researchers turn out to resort to both methodologies. One may then be tempted to conclude that these two fields are either largely redundant or totally antithetic. The starting point of this review is that this state of affairs should be put down to contingent rather than conceptual differences, and that these two fields can in fact advantageously be used in a synergistic manner. An overview of both fields is first provided, some fundamental concepts of which are illustrated. A variety of contexts in which complex network theory and data mining have been used in a synergistic manner are then presented. Contexts in which the appropriate integration of complex network metrics can lead to improved classification rates with respect to classical data mining algorithms and, conversely, contexts in which data mining can be used to tackle important issues in complex network theory applications are illustrated. Finally, ways to achieve a tighter integration between complex networks and data mining, and open lines of research are discussed.},
  citeulike-article-id = {14438481},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.physrep.2016.04.005},
  posted-at            = {2017-09-26 21:02:03},
  timestamp            = {2020-02-27 04:41},
}

@Article{Zareei-2015a,
  author               = {Zareei, Abalfazl},
  date                 = {2015-12},
  journaltitle         = {The Journal of Network Theory in Finance},
  title                = {Network centrality, failure prediction and systemic risk},
  doi                  = {10.21314/jntf.2015.013},
  issn                 = {2055-7795},
  number               = {4},
  pages                = {73--97},
  volume               = {1},
  abstract             = {A financial market can be expressed as a network structure in which the stocks reside as nodes and the links account for returns correlation. The centrality measure in the financial network structure captures firms' embeddedness and connectivity in the capital market structure. This paper investigates firms' centrality in the financial network as an explanatory variable in corporate-failure prediction as well as a measure of firms' systemic importance. First, when analyzing credit default swap (CDS) spreads, we find that peripheral firms in the network have higher average CDS spreads and a higher propensity for CDS jump events. Second, centrality is found to increase the explanatory power of default prediction models, and, moreover, it is negatively related to failure and bankruptcy probability. This implies that peripheral firms in the network are more likely to fail. Finally, by examining the out-of-sample performance of centrality as a measure of systemic importance, we find that centrality correctly distinguishes the firms that suffered higher losses during the 2007-8 crisis period.},
  citeulike-article-id = {14148619},
  citeulike-linkout-0  = {http://dx.doi.org/10.21314/jntf.2015.013},
  groups               = {Networks and investment management},
  owner                = {cristi},
  posted-at            = {2016-09-28 20:22:18},
  timestamp            = {2020-02-27 04:41},
}

@Article{Zareei-2019,
  author         = {Zareei, Abalfazl},
  date           = {2019-12},
  journaltitle   = {Journal of banking \& finance},
  title          = {Network origins of portfolio risk},
  doi            = {10.1016/j.jbankfin.2019.105663},
  issn           = {0378-4266},
  pages          = {105663},
  url            = {https://linkinghub.elsevier.com/retrieve/pii/S0378426619302389},
  urldate        = {2019-12-03},
  volume         = {109},
  abstract       = {This paper shows that shocks, in the presence of asymmetric propagation structures, diminish investors diversification benefits. First, we construct an interdependency network with assets as nodes, and links corresponding to cross-dependency in returns. Second, we show that higher heterogeneity in the structure of the network increases portfolio risk. In particular, diversification among assets with star-like network structures, where a central asset cross-affects other assets in the portfolio, results in the lowest level of diversification benefits. Finally, we empirically demonstrate that two distinct datasets of U.S. industries and international stock markets greatly resemble star-like network structures.},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 04:41},
}

@Article{Zhang-et-al-2017g,
  author               = {Zhang, Xi and Shi, Jiawei and Wang, Di and Fang, Binxing},
  date                 = {2017-11},
  journaltitle         = {Journal of Computational Science},
  title                = {Exploiting investors social network for stock prediction in China's market},
  doi                  = {10.1016/j.jocs.2017.10.013},
  issn                 = {1877-7503},
  abstract             = {Recent works have shown that social media platforms are able to influence the trends of stock price movements. However, existing works have major focused on the U.S. stock market and lacked attention to certain emerging countries such as China, where retail investors dominate the market. In this regard, as retail investors are prone to be influenced by news or other social media, psychological and behavioral features extracted from social media platforms are thought to well predict stock price movements in the China's market. Recent advances in the investor social network in China enables the extraction of such features from web-scale data. In this paper, on the basis of tweets from Xueqiu, a popular Chinese Twitter-like social platform specialized for investors, we analyze features with regard to collective sentiment and perception on stock relatedness and predict stock price movements by employing nonlinear models. The features of interest prove to be effective in our experiments.},
  citeulike-article-id = {14503711},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.jocs.2017.10.013},
  posted-at            = {2017-12-16 12:05:33},
  timestamp            = {2020-02-27 04:41},
}

@Article{Zhang-et-al-2019s,
  author         = {Zhang, Jeffrey O. and Sax, Alexander and Zamir, Amir and Guibas, Leonidas and Malik, Jitendra},
  date           = {2019-12-31},
  journaltitle   = {arXiv e-Print},
  title          = {Side-Tuning: Network Adaptation via Additive Side Networks},
  url            = {https://arxiv.org/abs/1912.13503v1},
  urldate        = {2020-01-19},
  abstract       = {When training a neural network for a desired task, one may prefer to adapt a pre-trained network rather than start with a randomly initialized one -- due to lacking enough training data, performing lifelong learning where the system has to learn a new task while being previously trained for other tasks, or wishing to encode priors in the network via preset weights. The most commonly employed approaches for network adaptation are fine-tuning and using the pre-trained network as a fixed feature extractor, among others. In this paper, we propose a straightforward alternative: Side-Tuning. Side-tuning adapts a pre-trained network by training a lightweight "side" network that is fused with the (unchanged) pre-trained network using summation. This simple method works as well as or better than existing solutions while it resolves some of the basic issues with fine-tuning, fixed features, and several other common baselines. In particular, side-tuning is less prone to overfitting when little training data is available, yields better results than using a fixed feature extractor, and does not suffer from catastrophic forgetting in lifelong learning. We demonstrate the performance of side-tuning under a diverse set of scenarios, including lifelong learning (iCIFAR, Taskonomy), reinforcement learning, imitation learning (visual navigation in Habitat), NLP question-answering (SQuAD v2), and single-task transfer learning (Taskonomy), with consistently promising results.},
  day            = {31},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 04:41},
}

@Article{Zhang-Gencay-2019,
  author         = {Zhang, Keyi and Gencay, Ramazan},
  date           = {2019-03},
  journaltitle   = {The Singapore Economic Review},
  title          = {Mutual fund performance in developing and advanced world networks},
  doi            = {10.1142/S0217590817500254},
  issn           = {0217-5908},
  number         = {02},
  pages          = {399--421},
  urldate        = {2019-06-15},
  volume         = {64},
  abstract       = {We propose a new determinant of mutual fund performance persistence. We argue that different funds have different abilities to generate persistent performance and that such heterogeneity across funds can be explained by fund manager access to market information. To justify this hypothesis, we construct a network of mutual funds based on the commonality of their stock holdings and use network features to characterize how well a fund acquires and utilizes market information. Based on a sample of U.S. equity funds from 2001 to 2014, we find that a mutual fund with more complete information is more likely to possess momentum in performance.},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 04:41},
}

@Article{Zhao-et-al-2016b,
  author         = {Zhao, Zoey Yi and Xie, Meng and West, Mike},
  date           = {2016-05},
  journaltitle   = {Applied Stochastic Models in Business and Industry},
  title          = {Dynamic dependence networks: Financial time series forecasting and portfolio decisions},
  doi            = {10.1002/asmb.2161},
  issn           = {1524-1904},
  number         = {3},
  pages          = {311--332},
  urldate        = {2019-04-20},
  volume         = {32},
  abstract       = {We discuss Bayesian forecasting of increasingly high-dimensional time series, a key area of application of stochastic dynamic models in the financial industry and allied areas of business. Novel state-space models characterizing sparse patterns of dependence among multiple time series extend existing multivariate volatility models to enable scaling to higher numbers of individual time series. The theory of these "dynamic dependence network" models shows how the individual series can be "decoupled" for sequential analysis, and then "recoupled" for applied forecasting and decision analysis. Decoupling allows fast, efficient analysis of each of the series in individual univariate models that are linked-- for later recoupling-- through a theoretical multivariate volatility structure defined by a sparse underlying graphical model. Computational advances are especially significant in connection with model uncertainty about the sparsity patterns among series that define this graphical model; Bayesian model averaging using discounting of historical information builds substantially on this computational advance. An extensive, detailed case study showcases the use of these models, and the improvements in forecasting and financial portfolio investment decisions that are achievable. Using a long series of daily international currency, stock indices and commodity prices, the case study includes evaluations of multi-day forecasts and Bayesian portfolio analysis with a variety of practical utility functions, as well as comparisons against commodity trading advisor benchmarks.},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 04:41},
}

@Article{Zhao-et-al-2017c,
  author               = {Zhao, Longfeng and Wang, Gang-Jin and Wang, Mingang and Bao, Weiqi and Li, Wei and Stanley, H. Eugene},
  date                 = {2017-12-13},
  journaltitle         = {arXiv e-Print},
  title                = {Stock market as temporal network},
  eprint               = {1712.04863},
  eprinttype           = {arXiv},
  url                  = {https://arxiv.org/abs/1712.04863},
  abstract             = {Financial networks have become extremely useful in characterizing the structure of complex financial systems. Meanwhile, the time evolution property of the stock markets can be described by temporal networks. We utilize the temporal network framework to characterize the time-evolving correlation-based networks of stock markets. The market instability can be detected by the evolution of the topology structure of the financial networks. We employ the temporal centrality as a portfolio selection tool. Those portfolios, which are composed of peripheral stocks with low temporal centrality scores, have consistently better performance under different portfolio optimization schemes, suggesting that the temporal centrality measure can be used as new portfolio optimization and risk management tools. Our results reveal the importance of the temporal attributes of the stock markets, which should be taken serious consideration in real life applications.},
  citeulike-article-id = {14510852},
  citeulike-linkout-0  = {http://arxiv.org/abs/1712.04863},
  citeulike-linkout-1  = {http://arxiv.org/pdf/1712.04863},
  day                  = {13},
  groups               = {PortfOptim_Network},
  posted-at            = {2018-01-02 01:42:36},
  timestamp            = {2020-02-27 04:41},
}

@Article{Zhao-et-al-2018c,
  author               = {Zhao, Longfeng and Wang, Gang-Jin and Wang, Mingang and Bao, Weiqi and Li, Wei and Stanley, H. Eugene},
  date                 = {2018},
  journaltitle         = {Physica A: Statistical Mechanics and its Applications},
  title                = {Stock market as temporal network},
  url                  = {https://www.sciencedirect.com/science/article/pii/S0378437118305752},
  abstract             = {Financial networks have become extremely useful in characterizing the structure of complex financial systems. Meanwhile, the time evolution property of the stock markets can be described by temporal networks. We utilize the temporal network framework to characterize the time-evolving correlation-based networks of stock markets. The market instability can be detected by the evolution of the topology structure of the financial networks. We employ the temporal centrality as a portfolio selection tool. Those portfolios, which are composed of peripheral stocks with low temporal centrality scores, have consistently better performance under different portfolio optimization schemes, suggesting that the temporal centrality measure can be used as new portfolio optimization and risk management tools. Our results reveal the importance of the temporal attributes of the stock markets, which should be taken serious consideration in real life applications.},
  citeulike-article-id = {14510852},
  citeulike-linkout-0  = {http://arxiv.org/abs/1712.04863},
  citeulike-linkout-1  = {http://arxiv.org/pdf/1712.04863},
  day                  = {13},
  groups               = {PortfOptim_Network},
  posted-at            = {2018-01-02 01:42:36},
  timestamp            = {2020-02-27 04:41},
}

@Article{Zhou-et-al-2018a,
  author         = {Zhou, Li and Qiu, Lu and Gu, Changgui and Yang, Huijie},
  date           = {2018-02-01},
  journaltitle   = {EPL (Europhysics Letters)},
  title          = {Immediate causality network of stock markets},
  doi            = {10.1209/0295-5075/121/48002},
  issn           = {0295-5075},
  number         = {4},
  pages          = {48002},
  url            = {http://stacks.iop.org/0295-5075/121/i=4/a=48002?key=crossref.cbf328bf62329def52fe9afa5e04bd02},
  urldate        = {2019-04-19},
  volume         = {121},
  day            = {1},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 04:41},
}

@InCollection{Zhou-et-al-2019e,
  author         = {Zhou, Bolei and Bau, David and Oliva, Aude and Torralba, Antonio},
  booktitle      = {Explainable AI: interpreting, explaining and visualizing deep learning},
  date           = {2019},
  title          = {Comparing the interpretability of deep networks via network dissection},
  doi            = {10.1007/978-3-030-28954-6\_12},
  editor         = {Samek, Wojciech and Montavon, Gregoire and Vedaldi, Andrea and Hansen, Lars Kai and Muller, Klaus-Robert},
  isbn           = {978-3-030-28953-9},
  pages          = {243--252},
  publisher      = {Springer International Publishing},
  series         = {Lecture notes in computer science},
  url            = {http://link.springer.com/10.1007/978-3-030-28954-6\_12},
  urldate        = {2019-11-10},
  volume         = {11700},
  abstract       = {In this chapter, we introduce Network Dissection (The complete paper and code are available at http://netdissect.csail.mit.edu), a general framework to quantify the interpretability of the units inside a deep convolutional neural networks (CNNs). We compare the different vocabularies of interpretable units as concept detectors emerged from the networks trained to solve different supervised learning tasks such as object recognition on ImageNet and scene classification on Places. The network dissection is further applied to analyze how the units acting as semantic detectors grow and evolve over the training iterations both in the scenario of the train-from-scratch and in the stage of the fine-tuning between data sources. Our results highlight that interpretability is an important property of deep neural networks that provides new insights into their hierarchical structure.},
  f1000-projects = {QuantInvest},
  issn           = {0302-9743},
  timestamp      = {2020-02-27 04:41},
}

@Article{Zhu-2019,
  author         = {Zhu, Wu},
  date           = {2019},
  journaltitle   = {SSRN e-Print},
  title          = {Networks, linking complexity, and cross predictability},
  doi            = {10.2139/ssrn.3512490},
  issn           = {1556-5068},
  url            = {https://www.ssrn.com/abstract=3512490},
  urldate        = {2020-01-22},
  abstract       = {This paper provides evidence that network complexity limits investors' ability to process non-local information, through the lens of return cross predictability. Using firm-to-firm citation networks, we find that the non-local indirectly-linked firms can well predict the return of the focal firm, while the predictability of the local directly-linked firms is weak. A long-short strategy using the indirect links yields a risk-adjusted monthly alpha of 198 (164) basis points with equal (value) weights. We further find that (i) the indirect citation links are much more complex than direct ones, (ii) the magnitude of cross predictability increases with the degree of link complexity, (iii) institutional investors don't adjust their positions in a stock with complex links, but in one with simple links immediately, (iv) firms with more complex links receive more public attention, are much larger in size, and exhibit less idiosyncratic volatility than those with simple links, (v) there is little role of the usual proxies for limited investor attention and arbitrage cost in explaining our anomalies, once controlling for the linking complexity, and (vi) there are no differences in expected returns of stocks with various link complexity.},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 04:41},
}

@Article{Zou-et-al-2018,
  author         = {Zou, Yong and Donner, Reik V. and Marwan, Norbert and Donges, Jonathan F. and Kurths, Jurgen},
  date           = {2018-11},
  journaltitle   = {Physics Reports},
  title          = {Complex network approaches to nonlinear time series analysis},
  doi            = {10.1016/j.physrep.2018.10.005},
  issn           = {0370-1573},
  url            = {https://linkinghub.elsevier.com/retrieve/pii/{S037015731830276X}},
  urldate        = {2019-09-06},
  abstract       = {In the last decade, there has been a growing body of literature addressing the utilization of complex network methods for the characterization of dynamical systems based on time series. While both nonlinear time series analysis and complex network theory are widely considered to be established fields of complex systems sciences with strong links to nonlinear dynamics and statistical physics, the thorough combination of both approaches has become an active field of nonlinear time series analysis, which has allowed addressing fundamental questions regarding the structural organization of nonlinear dynamics as well as the successful treatment of a variety of applications from a broad range of disciplines. In this report, we provide an in-depth review of existing approaches of time series networks, covering their methodological foundations, interpretation and practical considerations with an emphasis on recent developments. After a brief outline of the state-of-the-art of nonlinear time series analysis and the theory of complex networks, we focus on three main network approaches, namely, phase space based recurrence networks, visibility graphs and Markov chain based transition networks, all of which have made their way from abstract concepts to widely used methodologies. These three concepts, as well as several variants thereof will be discussed in great detail regarding their specific properties, potentials and limitations. More importantly, we emphasize which fundamental new insights complex network approaches bring into the field of nonlinear time series analysis. In addition, we summarize examples from the wide range of recent applications of these methods, covering rather diverse fields like climatology, fluid dynamics, neurophysiology, engineering and economics, and demonstrating the great potentials of time series networks for tackling real-world contemporary scientific problems. The overall aim of this report is to provide the readers with the knowledge how the complex network approaches can be applied to their own field of real-world time series analysis.},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 04:41},
}

@Article{Kaya-2015,
  author       = {Kaya, Hakan},
  date         = {2015},
  journaltitle = {Journal of Network Theory in Finance},
  title        = {Eccentricity in Asset Management},
  number       = {1},
  pages        = {1--32},
  url          = {https://ssrn.com/abstract=2350429},
  volume       = {1},
  abstract     = {We describe how networks based on information theory can help measure and visualize systemic risk, enhance diversification, and help price assets. To do this, we first define a distance measure based on the mutual information between asset pairs and use this measure in the construction of minimum spanning trees. The dynamics of the shape and the descriptive statistics of these trees are analyzed in various investment domains. The method provides evidence of regime changes in dependency structures prior to market sell-offs, and as such, it is a potential candidate for monitoring systemic risk. We also provide empirical evidence that the assets that are located towards the center of the network tend to have higher returns. Finally, an investment strategy that utilizes network centrality information is shown to add value historically.},
  day          = {7},
  groups       = {Networks and investment management},
  owner        = {cristi},
  posted-at    = {2016-05-18 22:44:58},
  timestamp    = {2020-02-27 04:56},
  year         = {2015},
}

@Article{SardaEspinosa-2019,
  author       = {Alexis Sarda-Espinosa},
  date         = {2019},
  journaltitle = {SSRN e-Print},
  title        = {Comparing Time-Series Clustering Algorithms in R Using the dtwclust Package},
  url          = {https://cran.r-project.org/web/packages/dtwclust/vignettes/dtwclust.pdf},
  abstract     = {Most clustering strategies have not changed considerably since their initial definition. Most of the improvements are either related to the distance measure used to assess dissimilarity, or the function used to calculate prototypes or centroids. Time-series clustering is no exception, with the Dynamic Time Warping distance being particularly popular in that context. This distance is computationally expensive, so many related optimizations have been developed over the years. Since no single clustering algorithm can be said to perform best on all datasets, different strategies must be tested and compared, so a common infrastructure can be advantageous. In this manuscript, a general overview of time-series clustering is given, including many specifics related to Dynamic Time Warping and other recently proposed techniques. At the same time, a description of the dtwclust package for the R statistical software is provided, showcasing how it can be used to evaluate many different time-series clustering procedures.},
  howpublished = {Available at https://cran.r-project.org/web/packages/dtwclust/vignettes/dtwclust.pdf},
  timestamp    = {2020-02-27 04:57},
}

@Article{Raffinot-2018a,
  author         = {Raffinot, Thomas},
  date           = {2018-08-23},
  journaltitle   = {SSRN e-Print},
  title          = {The Hierarchical Equal Risk Contribution Portfolio},
  url            = {https://ssrn.com/abstract=3237540},
  abstract       = {Building upon the fundamental notion of hierarchy, the "Hierarchical Risk Parity" (HRP) and the "Hierarchical Clustering based Asset Allocation" (HCAA), the Hierarchical Equal Risk Contribution Portfolio (HERC) aims at diversifying capital allocation and risk allocation. HERC merges and enhances the machine learning approach of HCAA and the Top-Down recursive bisection of HRP. In more detail, the modified Top-Down recursive division is based on the shape of dendrogram, follows an Equal Risk Contribution allocation and is extended to downside risk measures such as conditional value at risk (CVaR) and Conditional Drawdown at Risk (CDaR). The out-of-sample performances of hierarchical clustering based portfolios are evaluated across two empirical datasets, which differ in terms of number of assets and composition of the universe (multi-assets and individual stocks). Empirical results highlight that HERC Portfolios based on downside risk measures achieve statistically better risk-adjusted performances, especially those based on the CDaR.},
  day            = {23},
  f1000-projects = {QuantInvest},
  groups         = {Invest_Risk, ML_Network_QWIM, ML_InvestSelect},
  timestamp      = {2020-02-27 04:57},
}

@Article{Clemente-et-al-2018,
  author         = {Clemente, Gian Paolo and Grassi, Rosanna and Hitaj, Asmerilda},
  date           = {2018-10-20},
  journaltitle   = {arXiv e-Print},
  title          = {Asset allocation: new evidence through network approaches},
  url            = {https://arxiv.org/abs/1810.09825},
  urldate        = {2018-11-11},
  abstract       = {The main contribution of the paper is to employ the financial market network as a useful tool to improve the portfolio selection process, where nodes indicate securities and edges capture the dependence structure of the system. Three different methods are proposed in order to extract the dependence structure between assets in a network context. Starting from this modified structure, we formulate and then we solve the asset allocation problem. We find that the portfolios obtained through a network-based approach are composed mainly of peripheral assets, which are poorly connected with the others. These portfolios, in the majority of cases, are characterized by an higher trade-off between performance and risk with respect to the traditional Global Minimum Variance (GMV) portfolio. Additionally, this methodology benefits of a graphical visualization of the selected portfolio directly over the graphic layout of the network, which helps in improving our understanding of the optimal strategy.},
  day            = {20},
  f1000-projects = {QuantInvest},
  groups         = {Invest_Network},
  timestamp      = {2020-02-27 04:58},
}

@Article{dePrado-2016,
  author               = {{de Prado}, Marcos Lopez},
  date                 = {2016-07},
  journaltitle         = {The Journal of Portfolio Management},
  title                = {Building Diversified Portfolios that Outperform Out of Sample},
  doi                  = {10.3905/jpm.2016.42.4.059},
  issn                 = {0095-4918},
  number               = {4},
  pages                = {59--69},
  volume               = {42},
  abstract             = {In this article, the author introduces the Hierarchical Risk Parity (HRP) approach to address three major concerns of quadratic optimizers, in general, and Markowitz's critical line algorithm (CLA), in particular: instability, concentration, and underperformance. HRP applies modern mathematics (graph theory and machine-learning techniques) to build a diversified portfolio based on the information contained in the covariance matrix. However, unlike quadratic optimizers, HRP does not require the invertibility of the covariance matrix. In fact, HRP can compute a portfolio on an ill-degenerated or even a singular covariance matrix an impossible feat for quadratic optimizers. Monte Carlo experiments show that HRP delivers lower out-ofsample variance than CLA, even though minimum variance is CLA's optimization objective. HRP also produces less risky portfolios out of sample compared to traditional risk parity methods.},
  citeulike-article-id = {14130574},
  citeulike-linkout-0  = {http://dx.doi.org/10.3905/jpm.2016.42.4.059},
  groups               = {Networks and investment management, Clustering and network analysis, Machine learning and investment strategies, ML_Network_QWIM, ML_PerfMetrics, Invest_Diversif},
  owner                = {zkgst0c},
  posted-at            = {2016-09-05 23:17:29},
  timestamp            = {2020-02-27 04:59},
}

@Article{Baitinger-Papenbrock-2017,
  author       = {Baitinger, Eduard and Papenbrock, Jochen},
  date         = {2017-06},
  journaltitle = {Journal of Investment Strategies},
  title        = {Interconnectedness Risk and Active Portfolio Management},
  doi          = {10.21314/JOIS.2017.081},
  number       = {2},
  pages        = {63--90},
  url          = {https://www.risk.net/journal-of-investment-strategies/3916861/interconnectedness-risk-and-active-portfolio-management},
  volume       = {6},
  abstract     = {Interconnectedness is an alternative risk concept that so far has earned little attention in the asset management academia and industry. In this paper, we show that this neglect is not justified, as interconnectedness risk (i) has only moderate or no connection to conventional portfolio optimization inputs and (ii) active investment strategies based on interconnectedness information outperform their conventional peers. Utilizing a multi asset dataset, we measure interconnectedness risk by the embeddedness intensity, i.e. centrality, of assets in a correlation network, a concept from graph theory. Using the most common centrality measures, we first conduct empirical similarity studies analyzing how different centrality scores relate to each other and to conventional portfolio optimization inputs. Next, we outline how centrality can be incorporated in a risk-based as well as in a risk-return-based framework. Out-of-sample performance studies of centrality-optimized portfolios prove their competitiveness.},
  day          = {16},
  groups       = {Networks and investment management},
  owner        = {cristi},
  posted-at    = {2016-09-27 14:52:12},
  timestamp    = {2020-02-27 05:00},
  year         = {2017},
}

@Article{Lohre-et-al-2020,
  author         = {Lohre, Harald and Rother, Carsten and Schafer, Kilian Axel},
  date           = {2020},
  journaltitle   = {SSRN e-Print},
  title          = {Hierarchical Risk Parity: Accounting for Tail Dependencies in Multi-Asset Multi-Factor Allocations},
  doi            = {10.2139/ssrn.3513399},
  issn           = {1556-5068},
  url            = {https://www.ssrn.com/abstract=3513399},
  urldate        = {2020-01-19},
  abstract       = {We investigate portfolio diversification strategies based on hierarchical clustering. These hierarchical risk parity strategies use graph theory and unsupervised machine learning to build diversified portfolios by acknowledging the hierarchical structure of the investment universe. In this chapter, we consider two dissimilarity measures for clustering a multi-asset multi-factor universe. While the Pearson correlation coefficient is a popular choice, we are especially interested in a measure based on the lower tail dependence coefficient. Such innovation is expected to achieve better tail risk management in the context of allocating to skewed style factor strategies. Indeed, the corresponding hierarchical risk parity strategies seem to have been navigating the associated downside risk better, yet come at the cost of high turnover. A comparison based on block-bootstrapping evidences alternative risk parity strategies along economic factors to be on par in terms of downside risk with those based on statistical clusters.},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 05:01},
}

@Article{Eom-Park-2017,
  author               = {Eom, Cheoljun and Park, Jong W.},
  date                 = {2017-01},
  journaltitle         = {International Review of Financial Analysis},
  title                = {Effects of common factors on stock correlation networks and portfolio diversification},
  doi                  = {10.1016/j.irfa.2016.11.007},
  issn                 = {1057-5219},
  pages                = {1--11},
  volume               = {49},
  abstract             = {Common factors significantly affect the connectivity of the network among stocks. Common factors significantly affect the distribution of investment weight for stocks. The devised method substantially assist in constructing a more diversified portfolio. This diversified portfolio achieves better out-of-sample performance. These results are robust in both the Korean and the U.S. stock markets. This study empirically investigates the effects of common factors on the connectivity of the network among stocks and on the distribution of the investment weights for stocks. The network is defined as a stock correlation network from the minimal spanning tree (MST), and portfolio is defined as an efficient portfolio from the Markowitz mean-variance (MV) optimization function (MVOF). For these research goals, we devise a method using the comparative correlation matrix (C-CM), which does not have the property of a single common factor included in the sample correlation matrix (S-CM). The results reveal that common factors clearly affect the changes of connectivity among stocks in the networks, and that their influence is much greater on stocks with many links to other stocks in the network. Further, common factors significantly affect the determination of the investment weight's distribution for stocks from the MVOF. In particular, among the common factors, a market factor plays a dominant role in both structuring the network among stocks and in constructing the well-diversified portfolio. In addition, the devised method of the C-CM without the property of the market factor in the S-CM plays a crucial role in constructing a more diversified portfolio with better out-of-sample performance in the future period. These results are robust in both the Korean and the U.S. stocks markets.},
  citeulike-article-id = {14292066},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.irfa.2016.11.007},
  groups               = {Networks and investment management, PortfOptim_Network, Invest_Diversif},
  posted-at            = {2017-03-05 17:50:24},
  timestamp            = {2020-02-27 05:01},
}

@Article{NoelKoide-2016,
  author               = {Noel-Koide, Kevin},
  date                 = {2016-09},
  journaltitle         = {SSRN e-Print},
  title                = {Application of Machine Learning to Systematic Allocation Strategies},
  url                  = {https://ssrn.com/abstract=2837664},
  abstract             = {We investigate the use of machine learning techniques into building statistically stable systematic allocation strategies. Traditionally, allocation processes usually rely on variations of Markowitz framework such as Mean Variance allocation, Maximum Diversity, Risk Parity, Conditional Value at Risk, ie convex frontier optimization. Although those methods show some efficiency to allocate assets through the convex efficient frontier, they usually rely deeply on the estimation and the usage of the covariance matrix. Being no stationary and having multiple range memory (ie FIGARCH), the statistical estimation of covariance may lead to biases and errors and in the end, bias conclusions. Very extensive literature in econo-metrics, econo-physics, quantitative allocation cover this problem in order to remedy to the statistical estimation of covariance and his bias and issues.Here, our emphasis is not a new estimator of the covariance matrix, or a variant of Risk Parity but an application of Machine Learning techniques to infer no-linear relationships and long range memory between the assets.It has the advantage to remove the linear projection of the assets onto the covariance framework and then capture no-linear relationships between at various time periods.Recent advances in Neural Network, Deep Learning and Machine Learning allows a more efficient modeling of the no-linear statistical relationships between data (ie price, dividends,....). Among them, we can mention Restricted Boltzman Machines, Variationnal Auto-encoders and variations of Recurrent Neural Network, Highway Long Short Term Memory Network as well as Factorization Machines for projection on local sub-spaces.Thus, we investigate some of the techniques to develop practical systematic allocation strategies by reducing risks and estimations biases and show the results.},
  citeulike-article-id = {14136316},
  citeulike-linkout-0  = {http://ssrn.com/abstract=2837664},
  citeulike-linkout-1  = {http://papers.ssrn.com/sol3/Delivery.cfm/SSRNID2837664code2498383.pdf?abstractid=2837664 and mirid=1},
  day                  = {12},
  groups               = {ML_AssetPricing},
  owner                = {cristi},
  posted-at            = {2016-09-13 15:30:32},
  timestamp            = {2020-02-27 05:02},
}

@Article{NoelKoide-2016a,
  author               = {Noel-Koide, Kevin},
  date                 = {2016-03},
  journaltitle         = {SSRN e-Print},
  title                = {Asset Allocation and Intra-Sector Allocation Using Covariance and Precision Matrix Clustering},
  url                  = {https://ssrn.com/abstract=2745727},
  abstract             = {We investigate simply the usage of clustering method by inverse covariance estimation for asset allocation in finance. Allocation across various sectors of the market (i.e. sector ETF) is usually well understood through the usage of mean variance allocation. However, stocks inside a same sector (i.e. Biotechnology, IT,..) are little studied in the literature since high correlation between stocks leads to poor differentiation across stocks.Here, we apply covariance clustering method to study the behavior of one specific market sector: US listed biotechnology stocks.

We show that this methodology allows differentiating highly correlated stocks and it provides more detailed information inside this sector. Additionally, the use of higher frequency data (intraday data) allows refining the clustering method and characterizing further each stock. This method can give further insight to select stocks inside a same sector.},
  citeulike-article-id = {13978597},
  citeulike-linkout-0  = {http://ssrn.com/abstract=2745727},
  citeulike-linkout-1  = {http://papers.ssrn.com/sol3/Delivery.cfm/SSRNID2745727code2498383.pdf?abstractid=2745727 and mirid=1},
  day                  = {11},
  groups               = {Invest_Network},
  owner                = {cristi},
  posted-at            = {2016-03-12 22:40:18},
  timestamp            = {2020-02-27 05:02},
}

@Article{Iorio-et-al-2018,
  author               = {Iorio, Carmela and Frasso, Gianluca and D'Ambrosio, Antonio and Siciliano, Roberta},
  date                 = {2018-04},
  journaltitle         = {Expert Systems with Applications},
  title                = {A P-spline based clustering approach for portfolio selection},
  doi                  = {10.1016/j.eswa.2017.11.031},
  issn                 = {0957-4174},
  pages                = {88--103},
  volume               = {95},
  abstract             = {In the last years, many clustering techniques dealing with time course data have been proposed due to recent interests in studying phenomena that change over time. A new clustering method suitable for time series applications has been recently proposed by exploiting the properties of the P-splines approach. This semi-parametric tool has several advantages, i.e. it facilitates the removal of noise from time series and it ensures a computational time saving. In this paper, we propose to use this clustering approach on financial data with the aim of building a financial portfolio. Our proposal works directly on time series without any pre-processing, except for the computation of the spline coefficients and, eventually, normalizing the series. We show that our strategy is useful to support the investment decisions of financial practitioners.},
  citeulike-article-id = {14500412},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.eswa.2017.11.031},
  posted-at            = {2017-12-11 07:37:10},
  timestamp            = {2020-02-27 05:03},
}

@Article{Bokde-et-al-2017,
  author         = {Bokde, Neeraj and Asencio-Cortes, Gualberto and Martinez-Alvarez, Francisco and Kulat, Kishore},
  date           = {2017},
  journaltitle   = {The R journal},
  title          = {PSF: introduction to R package for pattern sequence based forecasting algorithm},
  doi            = {10.32614/{RJ}-2017-021},
  issn           = {2073-4859},
  number         = {1},
  pages          = {324},
  url            = {https://journal.r-project.org/archive/2017/{RJ}-2017-021/index.html},
  urldate        = {2019-05-04},
  volume         = {9},
  abstract       = {This paper discusses about an R package that implements the Pattern Sequence based Forecasting (PSF) algorithm, which was developed for univariate time series forecasting. This algorithm has been successfully applied to many different fields. The PSF algorithm consists of two major parts: clustering and prediction. The clustering part includes selection of the optimum number of clusters. It labels time series data with reference to such clusters. The prediction part includes functions like optimum window size selection for specific patterns and prediction of future values with reference to past pattern sequences. The PSF package consists of various functions to implement the PSF algorithm. It also contains a function which automates all other functions to obtain optimized prediction results. The aim of this package is to promote the PSF algorithm and to ease its implementation with minimum efforts. This paper describes all the functions in the PSF package with their syntax. It also provides a simple example of usage. Finally, the usefulness of this package is discussed by comparing it to auto.arima and ets, well-known time series forecasting functions available on CRAN repository.},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 05:04},
}

@Article{Serr-Arcos-2014,
  author         = {Serra, Joan and Arcos, Josep Ll.},
  date           = {2014-09},
  journaltitle   = {Knowledge-Based Systems},
  title          = {An empirical evaluation of similarity measures for time series classification},
  doi            = {10.1016/j.knosys.2014.04.035},
  issn           = {0950-7051},
  pages          = {305--314},
  url            = {http://linkinghub.elsevier.com/retrieve/pii/S0950705114001658},
  urldate        = {2019-12-22},
  volume         = {67},
  abstract       = {Time series are ubiquitous, and a measure to assess their similarity is a core part of many computational systems. In particular, the similarity measure is the most essential ingredient of time series clustering and classification systems. Because of this importance, countless approaches to estimate time series similarity have been proposed. However, there is a lack of comparative studies using empirical, rigorous, quantitative, and large-scale assessment strategies. In this article, we provide an extensive evaluation of similarity measures for time series classification following the aforementioned principles. We consider 7 different measures coming from alternative measure `families', and 45 publicly-available time series data sets coming from a wide variety of scientific domains. We focus on out-of-sample classification accuracy, but in-sample accuracies and parameter choices are also discussed. Our work is based on rigorous evaluation methodologies and includes the use of powerful statistical significance tests to derive meaningful conclusions. The obtained results show the equivalence, in terms of accuracy, of a number of measures, but with one single candidate outperforming the rest. Such findings, together with the followed methodology, invite researchers on the field to adopt a more consistent evaluation criteria and a more informed decision regarding the baseline measures to which new developments should be compared.},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 05:04},
}

@Online{Vigen-2019,
  author    = {Tyler Vigen},
  date      = {2019},
  title     = {Spurious Correlations},
  url       = {https://www.tylervigen.com/spurious-correlations},
  abstract  = {Military intelligence analyst and Harvard Law student Tyler Vigen illustrates the golden rule that "correlation does not equal causation" through hilarious graphs.

Is there a correlation between Nic Cage films and swimming pool accidents? What about beef consumption and people getting struck by lightning? Absolutely not. But that hasn't stopped millions of people from going to tylervigen.com and asking, "Wait, what?" Vigen has designed software that scours enormous data sets to find unlikely statistical correlations. He began pulling the funniest ones for his website and has since gained millions of views, hundreds of thousands of likes, and tons of media coverage. Subversive and clever, Spurious Correlations is geek humor at its finest, nailing our obsession with data and conspiracy theory.},
  timestamp = {2020-09-09 17:54},
}

@Article{Laurinaityte-et-al-2019,
  author               = {Laurinaityte, Nora and Meinerding, Christoph and Schlag, Christian and Thimme, Julian},
  date                 = {2019},
  journaltitle         = {SSRN e-Print},
  title                = {Elephants and the Cross-Section of Expected Returns},
  url                  = {https://ssrn.com/abstract=3073197},
  abstract             = {The population growth of captive Asian elephants explains the cross-section of expected returns of size-value sorted portfolios with a cross-sectional R2 of 93\% and a t-statistic of 4.0 for the market price of risk. One may be tempted to conclude that elephants are the new outstanding factor in empirical asset pricing. We argue that one has to be careful with such conclusions. Standard GMM cross-sectional asset pricing tests can generate spurious explanatory power for factor models when the weight on certain moment conditions is set inappropriately. In fact, by shifting the weights in the GMM, any desired level of cross-sectional fit can be attained at the price of not matching the factor means. We run placebo tests with factors that by construction do not explain the cross-section of expected returns and obtain spuriously high cross-sectional R2's. Finally, we document some examples of factor models proposed in the literature that suffer from this bias.},
  citeulike-article-id = {14487873},
  citeulike-linkout-0  = {https://papers.ssrn.com/sol3/papers.cfm?abstractid=3073197},
  posted-at            = {2017-12-03 23:39:42},
  timestamp            = {2020-09-09 17:55},
}

@Article{Curran-Zalla-2020,
  author         = {Curran, Michael and Zalla, Ryan},
  date           = {2020-05-07},
  journaltitle   = {arXiv e-Print},
  title          = {Can Volatility Solve the Naive Diversification Puzzle?},
  eprint         = {2005.03204v1},
  eprinttype     = {arxiv},
  url            = {https://arxiv.org/abs/2005.03204},
  abstract       = {We investigate whether sophisticated volatility estimation improves the out-of-sample performance of mean-variance portfolio strategies relative to the naive 1/N strategy. The portfolio strategies rely solely upon second moments. Using a diverse group of econometric and portfolio models across multiple datasets, we show that a majority of models achieve significantly higher Sharpe ratios and lower portfolio volatility relative to the naive rule, even after controlling for turnover costs. Our results suggest that there are benefits to employing more sophisticated econometric models than the sample covariance matrix, and that mean-variance strategies often outperform the naive portfolio across multiple datasets and assessment criteria.},
  day            = {7},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-09-09 17:55},
}

@Article{Elkamhi-Salerno-2020,
  author       = {Redouane Elkamhi and Marco Salerno},
  date         = {2020},
  journaltitle = {SSRN e-Print},
  title        = {The Jury is Still Out On the Performance of Naive Diversification (1/N rule)},
  doi          = {10.2139/ssrn.3638713},
  url          = {https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3638713},
  abstract     = {For many decades academics have presented theories on optimal portfolio allocations, with mean-variance models at their front and center. However, the work of DeMiguel et al. (2009) has made a compelling case that estimation error completely dwarfs the benefits of these optimal allocation rules, making naive diversification (1/N) a dominating strategy. In this paper, we show that the jury is still out on the relative performance of 1/N. We demonstrate that risk-based diversification - which rely solely on the variance-covariance matrix - strongly outperform the 1/N naive rule in terms of Sharpe ratio, certainty equivalent returns and turnover. We also show that machine learning and clustering techniques can be used to enhance the benefits from diversification when using risk-based allocation rules. We present simulation exercises that illustrate the source of the outperformance of risk-based diversification and the importance of clustering. Our results are robust across different asset types by considering portfolios of (a) equities only, (b) equities and bonds, and (c) a large set of equity anomalies.},
  timestamp    = {2020-09-09 17:55},
}

@Article{Fusai-et-al-2020a,
  author       = {Gianluca Fusai and Domenico Mignacca and Andrea Nardon and Ben Human},
  date         = {2020},
  journaltitle = {SSRN e-Print},
  title        = {Equally Diversified or Equally Weighted?},
  doi          = {10.2139/ssrn.3628585},
  url          = {https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3628585},
  abstract     = {The aim of this paper is to shed new light on the concept of diversification showing that it is not necessarily related to the reduction of the volatility of a portfolio, as it is commonly perceived. We introduce a diversification index that exploits the decomposition of portfolio volatility into undiversified volatility and a diversification component. The diversification component offsets the undiversified part leaving as a final result the portfolio volatility itself. Our decomposition has a clear statistical interpretation because it relates the diversification component to the so-called partial covariances, i.e. the covariances between the residuals of the regressions of the weighted asset returns with respect to the portfolio return. On this basis, we advocate the construction of an equally diversified portfolio versus an equally weighted portfolio. An empirical analysis illustrates the superior performance of the equally diversified portfolios with respect to the equally weighted portfolio.},
  timestamp    = {2020-09-09 17:56},
}

@Article{Fusai-et-al-2020,
  author       = {Gianluca Fusai and Domenico Mignacca and Andrea Nardon and Ben Human},
  date         = {2020},
  journaltitle = {Risk (Cutting Edge)},
  title        = {Equally Diversified or Equally Weighted?},
  doi          = {10.2139/ssrn.3628585},
  url          = {https://www.risk.net/cutting-edge/investments/7675551/equally-diversified-or-equally-weighted},
  abstract     = {Gianluca Fusai, Domenico Mignacca, Andrea Nardon and Ben Human show how to decompose portfolio volatility into undiversified volatility and a diversification component. The authors' decomposition has a clear statistical interpretation because it relates the diversification component to partial covariances. On this basis, they advocate the construction of an equally diversified portfolio. An empirical analysis illustrates the superior out-of-sample performance of the equally diversified portfolio with respect to an equally weighted portfolio},
  timestamp    = {2020-09-09 17:56},
}

@Article{Koumou-2020,
  author       = {Gilles Boevi Koumou},
  date         = {2020},
  journaltitle = {Financial Markets and Portfolio Management},
  title        = {Diversification and portfolio theory: a review},
  doi          = {10.1007/s11408-020-00352-6},
  pages        = {267-312},
  url          = {https://link.springer.com/article/10.1007/s11408-020-00352-6},
  volume       = {34},
  abstract     = {Diversification is one of the major components of investment decision-making under risk or uncertainty. However, paradoxically, as the 2007-2009 financial crisis revealed, the concept remains misunderstood. Our goal in writing this paper is to correct this issue by reviewing the concept in portfolio theory. The core of our review focuses on the following diversification principles: law of large numbers, correlation, capital asset pricing model and risk contribution or risk parity diversification principles. These four diversification principles are the DNA of the existing portfolio selection rules and asset pricing theories and are instrumental to the understanding of diversification in portfolio theory. We review their definition. We also review their optimality, with respect to expected utility theory, and their usefulness. Finally, we explore their measurement.},
  timestamp    = {2020-09-09 17:56},
}

@Article{Jaeger-et-al-2020,
  author         = {Jaeger, Markus and Krugel, Stephan and Marinelli, Dimitri and Papenbrock, Jochen and Schwendner, Peter},
  date           = {2020},
  journaltitle   = {SSRN e-Print},
  title          = {Understanding machine learning for diversified portfolio construction by explainable AI},
  doi            = {10.2139/ssrn.3528616},
  issn           = {1556-5068},
  url            = {https://www.ssrn.com/abstract=3528616},
  urldate        = {2020-03-06},
  abstract       = {In this paper, we construct a pipeline to investigate heuristic diversification strategies in asset allocation. We use machine learning concepts ("explainable AI") to compare the robustness of different strategies and back out implicit rules for decision making.In a first step, we augment the asset universe (the empirical dataset) with a range of scenarios generated with a block bootstrap from the empirical dataset.Second, we backtest the candidate strategies over a long period of time, checking their performance variability. Third, we use XGBoost as a regression model to connect the difference between the measured performances between two strategies to a pool of statistical features of the portfolio universe tailored to the investigated strategy. Finally, we employ the concept of Shapley values to extract the relationships that the model could identify between the portfolio characteristics and the statistical properties of the asset universe. We test this pipeline for studying risk-parity strategies with a volatility target, and in particular, comparing the machine learning-driven Hierarchical Risk Parity (HRP) to the classical Equal Risk Contribution (ERC) strategy.In the augmented dataset built from a multi-asset investment universe of commodities, equities and fixed income futures, we find that HRP better matches the volatility target, and shows better risk-adjusted performances. Finally, we train XGBoost to learn the difference between the realized Calmar ratios of HRP and ERC and extract explanations.The explanations provide fruitful ex-post indications of the connection between the statistical properties of the universe and the strategy performance in the training set. For example, the model confirms that features addressing the hierarchical properties of the universe are connected to the relative performance of HRP respect to ERC.},
  f1000-projects = {QuantInvest},
  groups         = {Scenario_ML},
  timestamp      = {2020-09-09 17:56},
}

@Article{Abboud-et-al-2019,
  author       = {Amir Abboud and Vincent Cohen-Addad and Hussein Houdrouge},
  date         = {2019},
  journaltitle = {Advances in Neural Information Processing Systems 32 (NIPS 2019)},
  title        = {Subquadratic High-Dimensional Hierarchical Clustering},
  url          = {http://papers.nips.cc/paper/9333-subquadratic-high-dimensional-hierarchical-clustering},
  abstract     = {We consider the widely-used average-linkage, single-linkage, and Ward's methods for computing hierarchical clusterings of high-dimensional Euclidean inputs. It is easy to show that there is no efficient implementation of these algorithms in high dimensional Euclidean space since it implicitly requires to solve the closest pair problem, a notoriously difficult problem. However, how fast can these algorithms be implemented if we allow approximation? More precisely: these algorithms successively merge the clusters that are at closest average (for average-linkage), minimum distance (for single-linkage), or inducing the least sum-of-square error (for Ward's). We ask whether one could obtain a significant running-time improvement if the algorithm can merge  -approximate closest clusters (namely, clusters that are at distance (average, minimum, or sum-of-square error) at most times the distance of the closest clusters). We show that one can indeed take advantage of the relaxation and compute the approximate hierarchical clustering tree using approximate nearest neighbor queries. This leads to an algorithm running in time  for dimensional Euclidean space. We then provide experiments showing that these algorithms perform as well as the non-approximate version for classic classification tasks while achieving a significant speedup.},
  timestamp    = {2020-09-09 17:57},
}

@Article{Bandara-et-al-2020a,
  author       = {Kasun Bandara and Christoph Bergmeir and Slawek Smyl},
  date         = {2020},
  journaltitle = {Expert Systems with Applications},
  title        = {Forecasting across time series databases using recurrent neural networks on groups of similar series: A clustering approach},
  doi          = {10.1016/j.eswa.2019.112896},
  pages        = {112896+},
  url          = {https://www.sciencedirect.com/science/article/abs/pii/S0957417419306128},
  volume       = {140},
  abstract     = {With the advent of Big Data, nowadays in many applications databases containing large quantities of similar time series are available. Forecasting time series in these domains with traditional univariate forecasting procedures leaves great potentials for producing accurate forecasts untapped. Recurrent neural networks (RNNs), and in particular Long Short Term Memory (LSTM) networks, have proven recently that they are able to outperform state-of-the-art univariate time series forecasting methods in this context, when trained across all available time series. However, if the time series database is heterogeneous, accuracy may degenerate, so that on the way towards fully automatic forecasting methods in this space, a notion of similarity between the time series needs to be built into the methods. To this end, we present a prediction model that can be used with different types of RNN models on subgroups of similar time series, which are identified by time series clustering techniques. We assess our proposed methodology using LSTM networks, a widely popular RNN variant, together with various clustering algorithms, such as kMeans, DBScan, Partition Around Medoids (PAM), and Snob. Our method achieves competitive results on benchmarking datasets under competition evaluation procedures. In particular, in terms of mean sMAPE accuracy it consistently outperforms the baseline LSTM model, and outperforms all other methods on the CIF2016 forecasting competition dataset.},
  timestamp    = {2020-09-09 17:59},
}

@Article{Adolfsson-et-al-2019,
  author         = {Adolfsson, Andreas and Ackerman, Margareta and Brownstein, Naomi C.},
  date           = {2019-04},
  journaltitle   = {Pattern recognition},
  title          = {To cluster, or not to cluster: An analysis of clusterability methods},
  doi            = {10.1016/j.patcog.2018.10.026},
  issn           = {0031-3203},
  pages          = {13--26},
  url            = {https://linkinghub.elsevier.com/retrieve/pii/S0031320318303777},
  urldate        = {2020-03-15},
  volume         = {88},
  abstract       = {Abstract Clustering is an essential data mining tool that aims to discover inherent cluster structure in data. For most applications, applying clustering is only appropriate when cluster structure is present. As such, the study of clusterability, which evaluates whether data possesses such structure, is an integral part of cluster analysis. However, methods for evaluating clusterability vary radically, making it challenging to select a suitable measure. In this paper, we perform an extensive comparison of measures of clusterability and provide guidelines that clustering users can reference to select suitable measures for their applications.},
  f1000-projects = {QuantInvest},
  groups         = {Cluster_TimeSeries, Cluster_FinTimeSrs},
  timestamp      = {2020-09-09 17:59},
}

@Article{Puerto-et-al-2020,
  author       = {Justo Puerto and Moises Rodriguez-Madrena and Andrea Scozzari},
  date         = {2020},
  journaltitle = {Computers \& Operations Research},
  title        = {Clustering and portfolio selection problems: A unified framework},
  doi          = {10.1016/j.cor.2020.104891},
  pages        = {104891},
  url          = {https://www.sciencedirect.com/science/article/abs/pii/S0305054820300083},
  volume       = {117},
  abstract     = {Given a set of assets and an investment capital, the classical portfolio selection problem consists in determining the amount of capital to be invested in each asset in order to build the most profitable portfolio. The portfolio optimization problem is naturally modeled as a mean-risk bi-criteria optimization problem where the mean rate of return of the portfolio must be maximized whereas a given risk measure must be minimized. Several mathematical programming models and techniques have been presented in the literature in order to efficiently solve the portfolio problem. A relatively recent promising line of research is to exploit clustering information of an assets network in order to develop new portfolio optimization paradigms. In this paper we endow the assets network with a metric based on correlation coefficients between assets' returns, and show how classical location problems on networks can be used for clustering assets. In particular, by adding a new criterion to the portfolio selection problem based on an objective function of a classical location problem, we are able to measure the effect of clustering on the selected assets with respect to the non-selected ones. Most papers dealing with clustering and portfolio selection models solve these problems in two distinct steps: cluster first and then selection. The innovative contribution of this paper is that we propose a Mixed-Integer Linear Programming formulation for dealing with this problem in a unified phase. The effectiveness of our approach is validated reporting some computational experiments on some real financial datasets.},
  journal      = {Computers {\&} Operations Research},
  month        = {may},
  publisher    = {Elsevier {BV}},
  timestamp    = {2020-09-09 18:00},
  year         = {2020},
}

@Article{Duarte-DeCastro-2020,
  author       = {Flavio Gabriel Duarte and Leandro Nunes De Castro},
  date         = {2020},
  journaltitle = {IEEE Access: Practical Iinnovations, Open Solutions},
  title        = {A Framework to Perform Asset Allocation Based on Partitional Clustering},
  doi          = {10.1109/access.2020.3001944},
  pages        = {110775--110788},
  url          = {https://ieeexplore.ieee.org/abstract/document/9115637},
  volume       = {8},
  abstract     = {Over the past years, many approaches to perform asset allocation have been proposed in the literature. Most of them tackle this problem as an optimization task, where the goal is to maximize return, whilst minimizing the risk. However, such approaches require the inversion of a positive-definite covariance matrix, usually resulting in the concentration of allocation, instability and low performance. Some methods have been recently introduced to solve this problem by facing it as a clustering problem. This paper introduces a framework for asset allocation based on partitional clustering algorithms. The idea is to segment the assets into clusters of correlated assets, allocate resources for each cluster and then within each cluster. The framework allows the use of different partitional clustering algorithms, intragroup and intergroup allocation methods. Also, various assessment criteria are considered, and a specialized initialization method is proposed for the clustering algorithm. The framework is evaluated with the Brazilian Stock Exchange (B3) data from the period 12/2005 to 04/2020. Different initialization methods are used for the clustering algorithm together with two intergroup and two intragroup techniques, resulting in five experimental scenarios. The results are compared with the Ibovespa index, the mean-variance model of Markowitz, and the risk-parity model recently proposed by Lopez de Prado.},
  timestamp    = {2020-09-09 18:00},
}

@Article{Molyboga-2020,
  author       = {Marat Molyboga},
  date         = {2020},
  journaltitle = {The Journal of Financial Data Science},
  title        = {A Modified Hierarchical Risk Parity Framework for Portfolio Management},
  number       = {3},
  pages        = {128-139},
  url          = {https://jfds.pm-research.com/content/early/2020/07/03/jfds.2020.1.038},
  volume       = {2},
  abstract     = {This article introduces a modified hierarchical risk parity (MHRP) approach that extends the HRP approach by incorporating three intuitive elements commonly used by practitioners. The new approach (1) replaces the sample covariance matrix with an exponentially weighted covariance matrix with Ledoit-Wolf shrinkage; (2) improves diversification across portfolio constituents both within and across clusters by relying on an equal volatility, rather than an inverse variance, allocation approach; and (3) improves diversification across time by applying volatility targeting to portfolios. The author examines the impact of the enhancements on portfolios of commodity trading advisors within a large-scale Monte Carlo simulation framework that accounts for the realistic constraints of institutional investors. The author finds a striking improvement in the out-of-sample Sharpe ratio of 50\%, on average, along with a reduction in downside risk.},
  timestamp    = {2020-09-09 18:00},
}

@Article{Konstantinov-Simonian-2020,
  author         = {Konstantinov, Gueorgui S. and Simonian, Joseph},
  date           = {2020-06-23},
  journaltitle   = {The Journal of Financial Data Science},
  title          = {A Network Approach to Analyzing Hedge Fund Connectivity},
  issue          = {3},
  number         = {3},
  pages          = {55-72},
  url            = {https://jfds.pm-research.com/content/2/3/55},
  urldate        = {2020-06-23},
  volume         = {22},
  abstract       = {In this article, the authors investigate the hedge fund market as a network of interacting individual funds. The authors identify and analyze the most important hedge fund styles that could both affect the market and transmit systemwide shocks to other funds, individual asset classes, and beyond. The authors find that the most connected hedge fund database categories are global macro and equity long-short funds. A central result of the article is a classification of funds using clustering, in which seemingly different funds are shown to cluster based on their shared factor exposures. This finding demonstrates that investors should consider fund connectivity and their attendant importance scores rather than database classifications when measuring hedge fund risk across the business cycle. The authors also provide a forecasting framework that can be used to predict hedge fund network behavior and the impact of individual factors on the network.},
  day            = {23},
  f1000-projects = {QuantInvest},
  publisher      = {Institutional Investor Journals Umbrella},
  timestamp      = {2020-09-09 18:01},
}

@Article{Millington-Niranjan-2020,
  author         = {Millington, Tristan and Niranjan, Mahesan},
  date           = {2020-05-08},
  journaltitle   = {arXiv e-Print},
  title          = {Construction of Minimum Spanning Trees from Financial Returns using Rank Correlation},
  eprint         = {2005.03963v1},
  eprinttype     = {arxiv},
  url            = {https://arxiv.org/abs/2005.03963},
  abstract       = {The construction of minimum spanning trees ({MSTs}) from correlation matrices is an often used method to study relationships in the financial markets. However most of the work on this topic tends to use the Pearson correlation coefficient, which relies on the assumption of normality and can be brittle to the presence of outliers, neither of which is ideal for the study of financial returns. In this paper we study the inference of {MSTs} from daily {US} financial returns using Pearson and two rank correlation methods, Spearman and Kendall's tau. We find that the trees constructed using these rank methods tend to be more stable and maintain more edges over the dataset than those constructed using Pearson correlation, that there are significant differences in the agreement of the centrality of various sectors and that despite these, the trees tend to have similar topologies.},
  day            = {8},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-09-09 18:01},
}

@Article{Loistl-Konstantinov-2020,
  author         = {Loistl, Otto and Konstantinov, Gueorgui S.},
  date           = {2020-03-06},
  journaltitle   = {The Journal of Financial Data Science},
  title          = {Interactions and Interconnectedness Shape Financial Market Research},
  eid            = {2},
  number         = {2},
  pages          = {51-63},
  url            = {https://jfds.pm-research.com/content/early/2020/03/05/jfds.2020.1.026},
  urldate        = {2020-03-07},
  abstract       = {In this article the authors investigate two fields that might be relevant for financial data sciences. The first issue covers the entire production chain from orders to prices  by realistically modeling stock exchange microstructure (e.g., NASDAQ and Xetra). Specifically, the authors show how data-driven research can model decisions to place orders and to generate prices by matching orders accordingly. The other issue is price interconnectedness at markets by networks. The authors show that interactions shape a market  performance. Emergence comprises the interactions at markets; as such, the collective may not be equal to the sum of individual activities. As a consequence, the assumption that markets are in equilibrium and that arbitrage opportunities do not exist can be replaced by more realistic working hypotheses. The authors show with the two examples that market participants interact, learn, and trade. These individual interactions can be described as organized complexity. Whereas calculus may not support explicit modeling of interactions, the age of big data permits their modeling and application of innovative concepts, such as network solutions for asset allocation, which can be modeled using machine learning. This article illustrates that assertion with concrete examples.},
  day            = {6},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-09-09 18:01},
}

@Article{Konstantinov-et-al-2020,
  author         = {Konstantinov, Gueorgui and Chorus, Andreas and Rebmann, Jonas},
  date           = {2020-03-06},
  journaltitle   = {The Journal of Portfolio Management},
  title          = {A network and machine learning approach to factor, asset, and blended allocation},
  doi            = {10.3905/jpm.2020.1.147},
  issue          = {6},
  pages          = {54-71},
  url            = {https://jpm.pm-research.com/content/46/6/54},
  urldate        = {2020-03-07},
  volume         = {46},
  abstract       = {The main idea of this article is to approach and compare factor and asset allocation portfolios using both traditional and alternative allocation techniques: inverse variance optimization, minimum-variance optimization, and centrality-based techniques from network science. Analysis of the interconnectedness between assets and factors shows that their relationship is strong. The authors compare the allocation techniques, considering centrality and hierarchal-based networks. They demonstrate the advantages of graph theory to explain the advantages to portfolio management and the dynamic nature of assets and factors with their importance score. They find that asset allocation can be efficiently derived using directed networks, dynamically driven by both US Treasuries and currency returns with significant centrality scores. Alternatively, the inverse variance weight estimation and correlation-based networks generate factor allocation with favorable risk-return parameters. Furthermore, factor allocation is driven mostly by the importance scores of the Fama-French-Carhart factors: SMB, HML, CMA, RMW, and MOM. The authors confirm previous results and argue that both factors and assets are interconnected with different value and momentum factors. Therefore, a blended strategy comprising factors and assets can be defensible for investors. As argued in previous research, factors are much more overcrowded than assets. Therefore, the centrality scores help to identify the crowded exposure and build diversified allocation. The authors run LASSO regressions and show how the network-based allocation can be implemented using machine learning.},
  day            = {6},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-09-09 18:02},
}

@Article{Konstantinov-Rusev-2020,
  author         = {Konstantinov, Gueorgui and Rusev, Mario},
  date           = {2020-01-31},
  journaltitle   = {The Journal of Financial Data Science},
  title          = {The Bond-Equity-Fund Relation Using the Fama-French-Carhart Factors: A Practical Network Approach},
  issue          = {1},
  pages          = {24-44},
  url            = {https://jfds.pm-research.com/content/2/1/24},
  urldate        = {2020-03-07},
  volume         = {2},
  abstract       = {The main goal of this article is to show the relation between global equity and bond funds from a network perspective. The authors demonstrate the advantages of graph theory to explain the collective fund dynamics. The results show that equity and bond funds have a significant exposure to the Fama-French-Carhart factors. The authors argue that the network is dynamically driven by equity funds with their centrality scores and risk factor exposure and can transmit and amplify system-wide stress or inefficiencies in the factor bets. Using graph theory, the authors demonstrate that the return-based relationships between bond and equity funds are asymmetrical and the network is sufficiently clustered. Specifically, equity funds connect the different clusters. The HML factor is significant both on a single-fund level and as a web determinant. Therefore, investors should pay close attention to it when managing funds and deriving asset allocations. Finally, the authors provide a machine learning approach to how fund managers, plan sponsors, and analysts can derive equity-bond allocations, based on centrality scores, factor exposure, and hierarchical clustering of asymmetrically connected assets.},
  day            = {31},
  f1000-projects = {QuantInvest},
  publisher      = {Institutional Investor Journals Umbrella},
  timestamp      = {2020-09-09 18:02},
}

@Article{deCarvalho-Gupta-2018,
  author       = {{de Carvalho}, Pablo Jose Campos and Gupta, Aparna},
  date         = {2018},
  journaltitle = {Journal of Banking \& Finance},
  title        = {A network approach to unravel asset price comovement using minimal dependence structure},
  doi          = {https://doi.org/10.1016/j.jbankfin.2018.04.012},
  pages        = {119-132},
  volume       = {91},
  abstract     = {We develop a network representation-based methodology to aid an exploratory analysis of temporally evolving comovement in asset prices. This parsimonious order-n representation of the most significant comovement in asset prices, filtered by common factors, allows tackling a large number of assets and unraveling their complex comovement structure. Flexibility in choosing explanatory factors to suit the specific objectives of a study makes this methodology useful for portfolio analysis, risk parity approaches, and risk management decisions. We illustrate the features of the methodology for a set of major industry equity indices and to blue chip stocks, where we analyze the dynamic relevance of Fama-French factors. Investigating the network for more than 20 years, including the dot-com bust, global financial crisis, and European debt crisis, helps draw many insights. For instance, unexpected industries are seen to connect idiosyncratically through the dot-com bust. We demonstrate that a network factor model based portfolio allocation performs better than a regular factor model based allocation.},
  timestamp    = {2020-09-09 18:03},
}

@Article{Giudici-et-al-2019,
  author         = {Giudici, Paolo and Polinesi, Gloria and Spelta, Alessandro},
  date           = {2019},
  journaltitle   = {SSRN e-Print},
  title          = {Network models to improve robot advisory portfolio asset allocation},
  doi            = {10.2139/ssrn.3556381},
  issn           = {1556-5068},
  url            = {https://www.ssrn.com/abstract=3556381},
  urldate        = {2020-03-28},
  abstract       = {Robot advisory services are rapidly expanding, responding to a growing interest people have in directly managing their savings. Robot advisors may reduce costs and improve the quality of the service, making user involvement more transparent. However, they may underestimate market risks, especially when highly correlated assets are being considered, leading to a mismatch between investors' expected and actual risk. The aim of the paper is to enhance robot advisory portfolio allocation, taking users' preference into account. In particular, we demonstrate how Random Matrix Theory and Network models can be combined to construct investment portfolios that provide lower risks with respect to standard Markovitz portfolios. To demonstrate the advantages of this approach we employ the observed returns of a large set of ETFs, which is representative of the financial products at the ground of the activity of robot advisors.},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-09-09 18:03},
}

@InProceedings{Begusic-Kostanjcar-2019,
  author         = {Begusic, Stjepan and Kostanjcar, Zvonko},
  booktitle      = {11th International Symposium on Image and Signal Processing and Analysis (ISPA)},
  date           = {2019-09-23},
  title          = {Cluster-Based Shrinkage of Correlation Matrices for Portfolio Optimization},
  doi            = {10.1109/{ISPA}.2019.8868482},
  isbn           = {978-1-7281-3140-5},
  pages          = {301--305},
  publisher      = {IEEE},
  url            = {https://ieeexplore.ieee.org/document/8868482/},
  urldate        = {2020-01-13},
  abstract       = {The estimation of correlation and covariance matrices from asset return time series is a critical step in financial portfolio optimization. Although sample estimates are reliable when the length of time series is very large compared to the number of assets, in high-dimensional settings estimation issues arise. To reduce estimation errors and mitigate their propagation to out-of-sample performance of portfolios based on noisy estimates, shrinkage methods are applied. In this paper we consider several shrinkage methods for correlation matrix estimation and define a cluster-based shrinkage procedure which introduces information about the structures of communities identified in asset dependence graphs. To test the considered shrinkage methods we apply them in a portfolio optimization scenario using the global minimum variance portfolio, and perform backtests on a large sample of NYSE daily stock return data. We find that shrinkage methods generally improve out-of-sample portfolio performance, and the proposed cluster-based method yields improved results and portfolios which outperform other considered methods.},
  day            = {23},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-09-09 18:15},
}

@Article{Bouchey-et-al-2017,
  author         = {Bouchey, Paul and Li, Tianchuan and Nemtchinov, Vassilii},
  date           = {2017-08-31},
  journaltitle   = {The Journal of Investing},
  title          = {Systematic Diversification Using Beta},
  number         = {3},
  pages          = {144-151},
  url            = {https://joi.pm-research.com/content/26/3/144},
  volume         = {26},
  abstract       = {Higher-risk investments deserve higher expected returns to compensate for the extra risk, or so theory tells us. Historically, this has not always been the case for U.S. and other developed-market stocks. This anomaly, which is now well established by academics, has started to gain traction with investors. This is demonstrated by the large flows into the numerous low-beta and low-volatility strategies that were established in the wake of the Global Financial Crisis. The authors examine the beta anomaly in the academic literature and provide an empirical analysis for stocks in the U.S., developed, and emerging markets. Their primary finding is that beta is not a strong predictor of expected returns, but it is useful when used to help reduce risk in a portfolio. The authors present results for an investment strategy that filters out the highest-beta stocks while controlling for concentration risks by country and sector. The study finds mixed results for beta as an anomaly: Low beta outperforms for international markets, underperforms in emerging markets, and is flat in the U.S. markets. None of these differences appears to be statistically significant. The authors find beta is very useful, however, as a tool for controlling risk, especially in the context of strategies that diversify across countries and sectors.},
  day            = {31},
  f1000-projects = {QuantInvest},
  groups         = {Diversified_Invest, Invest_Diversif},
  publisher      = {Institutional Investor Journals Umbrella},
  timestamp      = {2020-09-09 18:17},
}

@Article{Carmichael-et-al-2017,
  author               = {Carmichael, Benoit and Koumou, Gilles B. and Moran, Kevin},
  date                 = {2018},
  journaltitle         = {Quantitative Finance},
  title                = {Rao's quadratic entropy and maximum diversification indexation},
  doi                  = {10.1080/14697688.2017.1383625},
  number               = {6},
  pages                = {1017-10311--15},
  url                  = {https://www.tandfonline.com/doi/full/10.1080/14697688.2017.1383625},
  volume               = {18},
  abstract             = {This paper proposes a new formulation of the maximum diversification indexation strategy based on Rao's Quadratic Entropy. It clarifies the investment problem underlying this diversification strategy, identifies the source of its out-of-sample performance, and suggests new dimensions along which this performance can be improved. We show that these potential improvements are quantitatively important and are robust to portfolio turnover, portfolio risk, estimation window, and covariance matrix estimation.},
  citeulike-article-id = {14486031},
  citeulike-linkout-0  = {http://dx.doi.org/10.1080/14697688.2017.1383625},
  citeulike-linkout-1  = {http://www.tandfonline.com/doi/abs/10.1080/14697688.2017.1383625},
  day                  = {24},
  groups               = {Invest_Diversif},
  posted-at            = {2017-11-29 23:50:51},
  publisher            = {Routledge},
  timestamp            = {2020-09-09 18:19},
}

@Article{duPlessis-vanRensburg-2017,
  author               = {{du Plessis}, Hannes and {van Rensburg}, Paul},
  date                 = {2017-06},
  journaltitle         = {Investment Analysts Journal},
  title                = {Diversification and the realised volatility of equity portfolios},
  doi                  = {10.1080/10293523.2017.1335367},
  number               = {3},
  pages                = {213-234},
  volume               = {46},
  abstract             = {In Markowitz's (1952) portfolio theory, a reduction in volatility for a given level of expected return is implied as being equivalent to an increase in diversification. The recent development of risk-based portfolio construction methods, which emphasise diversification separately from volatility reduction, challenges this equivalence. Using a point-in-time database of liquid equities listed on the Johannesburg Stock Exchange between 1998 and 2016, a numerical simulation technique is employed to study the behaviour of a range of diversification measures as a portfolio-level attribute and assess and compare their usefulness in estimating out-of-sample portfolio volatility. The empirical performance of maximum diversification portfolios based on each measure is then investigated. It is found that a portfolio?s diversification level is a significant predictor of future portfolio risk beyond that of historic volatility, and that the empirical performance of maximum diversification portfolios, attractive in all cases, depends critically on the definition of diversification applied.},
  citeulike-article-id = {14388871},
  citeulike-linkout-0  = {http://dx.doi.org/10.1080/10293523.2017.1335367},
  citeulike-linkout-1  = {http://www.tandfonline.com/doi/abs/10.1080/10293523.2017.1335367},
  day                  = {26},
  groups               = {Diversification_Measure, Diversified_Invest, Invest_Diversif},
  posted-at            = {2017-07-06 13:37:32},
  publisher            = {Routledge},
  timestamp            = {2020-09-09 18:20},
}

@Article{Heckel-et-al-2019,
  author       = {Heckel, Thomas and Amghar, Zine and Haik, Isaac and Laplenie, Olivier and de Carvalho, Raul Leote},
  date         = {2020},
  journaltitle = {The Journal of Fixed Income},
  title        = {Factor Investing in Corporate Bond Markets: Enhancing Efficacy Through Diversification and Purification!},
  number       = {3},
  pages        = {6-21},
  url          = {https://jfi.pm-research.com/content/early/2019/10/03/jfi.2019.1.074},
  volume       = {29},
  abstract     = {We show that factors from value, quality, low risk, and momentum styles play an important role in explaining the cross-section of corporate bond expected returns for the US and Euro Investment Grade and US BB-B Nonfinancial High Yield universes. We demonstrate the importance of purifying factor data by neutralizing a number of risk biases that are present in the factors: controlling for sectors, option-adjusted spread (OAS), duration, and size biases significantly increase the predictive power of style factors. We propose a new simple approach for efficiently neutralizing the biases from multiple risk variables and demonstrate its superiority relative to stratified sampling and optimization as alternative control methods. We also measure the added value from diversifying the number of factors in each style. Finally, we show that the results are robust in relation to transaction costs and can be used to design strategies that aim at outperforming traditional benchmark indexes.},
  groups       = {Invest_Diversif},
  timestamp    = {2020-09-09 18:22},
}

@Article{Leon-et-al-2017a,
  author       = {Leon, Carlos and Kim, Geun-Young and Martinez, Constanza and Lee, Daeyup},
  date         = {2017-08},
  journaltitle = {Quantitative Finance},
  title        = {Equity markets' clustering and the global financial crisis},
  doi          = {10.1080/14697688.2017.1357970},
  issue        = {12},
  pages        = {1905-1922},
  url          = {https://www.tandfonline.com/doi/full/10.1080/14697688.2017.1357970},
  volume       = {17},
  abstract     = {The effect of the Global Financial Crisis (GFC) has been substantial across markets and countries worldwide. We examine how the GFC has changed the way equity markets group together based on the similarity of stock indices? daily returns. Our examination is based on agglomerative clustering methods, which yield a hierarchical structure that represents how stock markets relate to each other based on their cross-section similarity. Main results show that both hierarchical structures, before and after the GFC, are readily interpretable, and indicate that geographical factors dominate the hierarchy. The main features of equity markets? hierarchical structure agree with most stylized facts reported in related literature. The most noticeable change after the GFC is an increase in (geographical) clustering. However, the increase in clusters? compactness and the decrease in clusters? separateness point out that world equity markets became more interconnected after the GFC. Some changes in the hierarchy that do not conform to geographical clustering are explained by well-known idiosyncratic features or shocks.},
  timestamp    = {2020-09-09 18:24},
}

@Article{Louton-Saraoglu-2008,
  author         = {Louton, David and Saraoglu, Hakan},
  date           = {2008-08-31},
  journaltitle   = {The Journal of Investing},
  title          = {How Many Mutual Funds Are Needed to Form a Well- Diversified Asset Allocated Portfolio?},
  doi            = {10.3905/joi.2008.710919},
  issn           = {1068-0896},
  number         = {3},
  pages          = {47--63},
  url            = {https://joi.pm-research.com/content/17/3/47},
  urldate        = {2019-07-15},
  volume         = {17},
  abstract       = {Funds of funds, which have become a popular investment vehicle in recent years, diversify across asset classes as well as managers with different styles and expertise. Lifecycle funds are a good example of funds of funds where investors can invest in a group of asset classes in different proportions depending on their investment horizon, risk tolerance, and objectives. Given that the number of mutual funds in the portfolios of lifecycle funds offered by different investment companies varies significantly even for those with similar targets, it is important to investigate the relationship between the number of mutual funds in an asset allocated portfolio and the resulting diversification benefits. As investors in lifecycle funds are concerned with the level of wealth they will have accumulated as of a target date, the variability of terminal wealth at the end of a given holding period is a relevant measure of risk for their portfolios. In this article the authors use a survivorship-bias-free sample to assess the impact of the number of mutual funds in an asset allocated portfolio on the variability and shortfall risk of its terminal wealth. Specifically, they run simulations to generate a large number of terminal wealth level outcomes for a portfolio with a given number of funds. Then, they obtain the frequency distribution of the terminal wealth outcomes, and use its dispersion as the risk measure in the analysis. The findings for three asset allocation scenarios, which are reported for 5-year and 10-year investment horizons, indicate that holding 10 to 12 funds in the portfolio instead of the minimum possible 2 funds as dictated by the asset allocation to equity and bonds reduces the standard deviation of terminal wealth by about 60\%. This reduction can be obtained without sacrificing expected terminal wealth levels, and hence without a reduction in total returns. Similarly, the mean shortfall of terminal wealth and the semivariance of terminal wealth are reduced by 60\% and 85\%, respectively.},
  day            = {31},
  f1000-projects = {QuantInvest},
  groups         = {Invest_Diversif},
  timestamp      = {2020-09-09 18:25},
}

@Book{Maharaj-et-al-2019,
  author    = {Elizabeth Ann Maharaj and Pierpaolo {D'Urso} and Jorge Caiado},
  date      = {2019},
  title     = {Time Series Clustering and Classification},
  publisher = {CRC Press},
  url       = {https://www.crcpress.com/Time-Series-Clustering-and-Classification/Maharaj-DUrso-Caiado/p/book/9781498773218},
  abstract  = {Time Series Clustering and Classification includes relevant developments on observation-based, feature-based and model-based traditional and fuzzy clustering methods, feature-based and model-based classification methods, and machine learning methods. It presents a broad and self-contained overview of techniques for both researchers and students.},
  timestamp = {2020-09-09 18:26},
}

@InCollection{Markovic-et-al-2019,
  author         = {Markovic, Ivana P. and Stankovic, Jelena Z. and Stojanovic, Milos B. and Stankovic, Jovica M.},
  booktitle      = {ICT innovations 2019. big data processing and mining: 11th international conference},
  date           = {2019},
  title          = {A Hybrid Model for Financial Portfolio Optimization Based on LS-SVM and a Clustering Algorithm},
  doi            = {10.1007/978-3-030-33110-8\_15},
  editor         = {Gievska, Sonja and Madjarov, Gjorgji},
  isbn           = {978-3-030-33109-2},
  pages          = {173--186},
  publisher      = {Springer International Publishing},
  url            = {http://link.springer.com/10.1007/978-3-030-33110-8\_15},
  urldate        = {2019-12-14},
  volume         = {1110},
  abstract       = {An investment decision is one of the most important financial decisions. With the aim of optimizing investment in securities from the aspect of return and risk, investors usually diversify their portfolio securities. This paper presents a hybrid model for portfolio optimization, which consist of two steps. The first step predicts future returns on the shares, and the second step, by applying hierarchical clustering algorithm, identifies various groups of shares. The test results indicate that the suggested model is suitable for optimization of a financial portfolio as a hybrid model based on selected shares, which if included in the portfolio, enable the diversification of risk.},
  f1000-projects = {QuantInvest},
  issn           = {1865-0929},
  timestamp      = {2020-09-09 18:27},
}

@Article{Martellini-Milhau-2017,
  author         = {Martellini, Lionel and Milhau, Vincent},
  date           = {2018},
  journaltitle   = {The Journal of Portfolio Management},
  title          = {Proverbial Baskets Are Uncorrelated Risk Factors! A Factor-Based Framework for Measuring and Managing Diversification in Multi-Asset Investment Solutions},
  doi            = {10.3905/jpm.2018.44.2.008},
  number         = {2},
  pages          = {8-22},
  url            = {https://jpm.pm-research.com/content/44/2/8},
  volume         = {44},
  abstract       = {Multi-asset investment solutions have become increasingly popular among sophisticated institutional investors focusing on efficient harvesting of risk premia across and within asset classes. One key challenge in the construction of diversified multi-asset portfolio strategies is that even a seemingly well-balanced allocation to many asset classes can eventually translate into a portfolio with a very concentrated set of underlying risk exposures. The authors suggest using a factor-based framework to more effectively measure and manage diversification in multi-asset portfolios.},
  day            = {31},
  f1000-projects = {QuantInvest},
  groups         = {Diversified_Invest, Invest_Risk, Effective_Dim_Diversif, MultiFactor_Invest, Invest_Diversif},
  publisher      = {Institutional Investor Journals Umbrella},
  timestamp      = {2020-09-09 18:28},
}

@Article{Mikalsen-et-al-2018,
  author               = {Mikalsen, Karl O. and Bianchi, Filippo M. and Soguero-Ruiz, Cristina and Jenssen, Robert},
  date                 = {2018-12},
  journaltitle         = {Pattern Recognition},
  title                = {Time Series Cluster Kernel for Learning Similarities between Multivariate Time Series with Missing Data},
  doi                  = {10.1016/j.patcog.2017.11.030},
  issn                 = {0031-3203},
  pages                = {569-581},
  volume               = {76},
  abstract             = {Similarity-based approaches represent a promising direction for time series analysis. However, many such methods rely on parameter tuning, and some have shortcomings if the time series are multivariate (MTS), due to dependencies between attributes, or the time series contain missing data. In this paper, we address these challenges within the powerful context of kernel methods by proposing the robust time series cluster kernel (TCK). The approach taken leverages the missing data handling properties of Gaussian mixture models (GMM) augmented with informative prior distributions. An ensemble learning approach is exploited to ensure robustness to parameters by combining the clustering results of many GMM to form the final kernel.

We evaluate the TCK on synthetic and real data and compare to other state-of-the-art techniques. The experimental results demonstrate that the TCK is robust to parameter choices, provides competitive results for MTS without missing data and outstanding results for missing data.},
  citeulike-article-id = {14499039},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.patcog.2017.11.030},
  groups               = {Scenario_TimeSeries, ML_ClustTimeSrs, Scenario_SynthData},
  posted-at            = {2017-12-07 22:36:35},
  timestamp            = {2020-09-09 18:30},
}

@InCollection{Mitra-et-al-2019,
  author         = {Mitra, Arup and Das, Abhra and Goswami, Saptarsi and Mustafi, Joy and Jalan, A. K.},
  booktitle      = {Computational intelligence, communications, and business analytics: second international conference, CICBA 2018},
  date           = {2019},
  title          = {Portfolio management by time series clustering using correlation for stocks},
  doi            = {10.1007/978-981-13-8581-0\_11},
  editor         = {Mandal, Jyotsna Kumar and Mukhopadhyay, Somnath and Dutta, Paramartha and Dasgupta, Kousik},
  isbn           = {978-981-13-8580-3},
  location       = {Singapore},
  pages          = {134--144},
  publisher      = {Springer Singapore},
  url            = {http://link.springer.com/10.1007/978-981-13-8581-0\_11},
  urldate        = {2019-12-14},
  volume         = {1031},
  abstract       = {Investment diversification and portfolio building has been a great interest for share market investors, so as to minimize risk and maximize profit in a sensitive stock market. This paper gives an inside view of application of clustering for grouping 79 stocks (NSE), which can be used to build a diversified portfolio. Manually trying out different groupings to diversify portfolio is a computationally expensive task. In this paper, the closing price, time series of the stocks have been considered. Common effect due to market has been discounted using partial correlation, and a correlation based dissimilarity measure has been used for clustering. An equal investment strategy has been adopted to compare the portfolio performance with SENSEX. The empirical results of the portfolios have been studied and presented in details.},
  f1000-projects = {QuantInvest},
  issn           = {1865-0929},
  timestamp      = {2020-09-09 18:31},
}

@Article{Pola-2016,
  author               = {Pola, Gianni},
  date                 = {2016-04},
  journaltitle         = {Journal of Asset Management},
  title                = {On entropy and portfolio diversification},
  doi                  = {10.1057/jam.2016.10},
  issn                 = {1470-8272},
  pages                = {218-228},
  url                  = {https://link.springer.com/article/10.1057/jam.2016.10},
  volume               = {17},
  abstract             = {Entropy, a term used in Physics to quantify the degree of randomness in a complex system, is shown to be relevant for portfolio diversification. The link between entropy and diversification lies in the notion of uncertainty. We introduce the concept of available diversification in an investment universe and of diversification curves. We build a framework for assembling a fully diversified risk parity-like portfolio with a fundamental-based high-conviction strategy, through a constrained entropy-maximisation process by which a portion of potential portfolio return is swapped for extra diversification. The main results of this study are: mean-variance optimised portfolios are highly concentrated and scarcely related to the asset return assumptions; few basis points of expected returns can be converted into a huge amount of extra diversification that making the portfolio allocation more robust to parameter uncertainty; on a more conceptual ground, we investigate the relationship between portfolio risk and diversification concluding that they should be managed distinctly. The empirical analysis presented in this work shows that entropy is a useful means to alleviate the lack of diversification of portfolios on the efficient frontier.},
  citeulike-article-id = {14030186},
  citeulike-linkout-0  = {http://dx.doi.org/10.1057/jam.2016.10},
  day                  = {07},
  groups               = {Diversified_Invest, Invest_Diversif},
  owner                = {cristi},
  posted-at            = {2016-05-08 22:55:09},
  timestamp            = {2020-09-09 18:32},
}

@Article{Simonian-Wu-2019b,
  author         = {Simonian, Joseph and Wu, Chenwei},
  date           = {2019},
  journaltitle   = {The Journal of Portfolio Management},
  title          = {Great Expectations: A Tactical Asset Allocation Framework for Diversified Real Asset Portfolios},
  doi            = {10.3905/jpm.2018.45.2.038},
  number         = {2},
  pages          = {38-45},
  url            = {https://jpm.pm-research.com/content/45/2/38},
  volume         = {45},
  abstract       = {Diversified real return strategies are multi-asset portfolios structured to possess a heightened sensitivity to inflation relative to traditional stocks and bonds. The majority of such strategies focus on a single measure of inflation, the Consumer Price Index. However, a more comprehensive way to construct inflation-sensitive portfolios is in terms of expected and unexpected inflation, the latter defined as the difference between a particular measure of inflation expectations and realized inflation. To that end, in this article, the authors describe an investment framework that dynamically classifies each type of inflation as belonging to one type of state: a stable state, in which inflation continues its longer-term trend, and a deviant state, in which expected or unexpected inflation departs significantly from its longer-term average. The authors show how the framework can be used to build portfolios using information from both stable and deviant states to outperform realized inflation through different market environments.},
  day            = {31},
  f1000-projects = {QuantInvest},
  groups         = {TAA, Invest_Diversif},
  publisher      = {Institutional Investor Journals Umbrella},
  timestamp      = {2020-09-09 18:35},
}

@Article{Simonian-Wu-2019,
  author         = {Simonian, Joseph and Wu, Chenwei},
  date           = {2019-02-28},
  journaltitle   = {The Journal of Portfolio Management},
  title          = {Factors in Time: Fine-Tuning Hedge Fund Replication},
  issue          = {3},
  pages          = {159-164},
  url            = {https://jpm.iijournals.com/content/45/3/159},
  urldate        = {2019-03-08},
  volume         = {45},
  abstract       = {Hedge fund replication has become a cottage industry in investing. Among the most popular hedge fund replication frameworks are factor models based on ordinary least squares (OLS) regression, a development that is no doubt due to its simplicity and familiarity among investment practitioners. Despite their widespread use, the OLS regression-based factor models that form the basis for many hedge fund replication programs are often overfitted to a single sample, severely undercutting their predictive effectiveness. As a remedy to the latter shortcoming, in this article the authors apply the regularization method known as  regression to the replication of hedge fund strategies. Ridge regression works by formally imbuing a regression with additional bias in exchange for a reduction in the variance between training and test samples. Using a simple yet robust methodology, the authors show how to dynamically calibrate the predictively optimal level of bias without significantly reducing the backward-looking explanatory power of a given model. In doing so, the authors demonstrate that ridge regression can help produce generalizable models that are useful in both the ex post risk analysis and ex ante replication of hedge fund strategies.},
  day            = {28},
  f1000-projects = {QuantInvest},
  groups         = {Hedge_Funds},
  publisher      = {Institutional Investor Journals Umbrella},
  timestamp      = {2020-09-09 18:35},
}

@Article{Simonian-Wu-2019a,
  author         = {Simonian, Joseph and Wu, Chenwei},
  date           = {2019-03-28},
  journaltitle   = {The Journal of Financial Data Science},
  title          = {Minsky vs. Machine: New Foundations for Quant-Macro Investing},
  number         = {2},
  pages          = {94-110},
  url            = {https://jfds.pm-research.com/content/1/2/94},
  urldate        = {2019-04-02},
  volume         = {1},
  abstract       = {Systematic macro investors use of the regime-switching models that have been developed in academia over the last several decades is infrequent at best and, when used, generally tangential to their core investment process. The roots of this less-than-enthusiastic uptake can be found in two familiar sources: models that possess an overly complex formal structure and poor predictive ability. As a remedy to the current state of affairs, the authors present a new foundation for regime-based investing, one based on spectral clustering, a graph theoretic approach to classifying data. Drawing inspiration from the work of Hyman Minsky and John Geanakoplos, the authors present a macro framework that uses measures of growth, inflation, and leverage to define regimes and drive portfolio decisions. To the latter end, the authors show how the framework can be used to build portfolios using information about regimes as defined, to outperform a no-information equal-weight portfolio both out-of-sample and in bootstrapped and cross-validated simulations. The authors thus show that spectral clustering can provide both an elegant mathematical description of the leverage cycle and a robust foundation for quant-macro investing.},
  day            = {28},
  f1000-projects = {QuantInvest},
  publisher      = {Institutional Investor Journals Umbrella},
  timestamp      = {2020-09-09 18:35},
}

@Article{Vyrost-et-al-2019,
  author       = {Tomas V{\'{y}}rost and {\v{S}}tefan Ly{\'{o}}csa and Eduard Baumohl},
  date         = {2019},
  journaltitle = {The North American Journal of Economics and Finance},
  title        = {Network-based asset allocation strategies},
  doi          = {10.1016/j.najef.2018.06.008},
  pages        = {516--536},
  volume       = {47},
  abstract     = {In this study, we construct financial networks in which nodes are represented by assets and where edges are based on long-run correlations. We construct four networks (complete graph, a minimum spanning tree, a planar maximally filtered graph, and a threshold significance graph) and use three centrality measures (betweenness, eigenvalue centrality, and the expected force). To improve risk-return characteristics of well-known return maximization and risk minimization benchmark portfolios, we propose simple adjustments to portfolio selection strategies that utilize centralization measures from financial networks. From a sample of 45 assets (stock market indices, bond and money market instruments, commodities, and foreign exchange rates) and from data for 1999 to 2015, we show that irrespective of the network and centrality employed, the proposed network-based asset allocation strategies improve key portfolio return characteristics in an out-of-sample framework, most notably, risk and left-tail risk-adjusted returns. Resolving portfolio model selection uncertainties further improves risk-return characteristics. Improvements made to portfolio strategies based on risk minimization are also robust to transaction costs.},
  journal      = {The North American Journal of Economics and Finance},
  month        = {jan},
  publisher    = {Elsevier {BV}},
  timestamp    = {2020-09-09 18:41},
  year         = {2019},
}

@Article{Bacidore-et-al-2012,
  author         = {Bacidore, Jeff and Berkow, Kathryn and Polidore, Ben and Saraiya, Nigam},
  date           = {2012},
  journaltitle   = {The Journal of Trading},
  title          = {Cluster Analysis for Evaluating Trading Strategies},
  doi            = {10.3905/jot.2012.7.3.006},
  number         = {3},
  pages          = {6-11},
  url            = {https://jot.pm-research.com/content/7/3/6},
  volume         = {7},
  abstract       = {In this article, we introduce a new methodology to empirically identify the primary strategies used by a trader using only post-trade fill data. To do this, we apply a well-established statistical clustering technique called k-means to a sample of progress charts, representing the portion of the order completed by each point in the day as a measure of a trade aggressiveness. Our methodology identifies the primary strategies used by a trader and determines which strategy the trader used for each order in the sample. Having identified the strategy used for each order, trading cost analysis can be performed by strategy. We also discuss ways to exploit this technique to characterize trader behavior, assess trader performance, and suggest the appropriate benchmarks for each distinct trading strategy.},
  day            = {31},
  f1000-projects = {QuantInvest},
  groups         = {Invest_Network},
  publisher      = {Institutional Investor Journals Umbrella},
  timestamp      = {2020-09-09 18:47},
}

@Book{dePrado-2020f,
  author         = {{de Prado}, Marcos Lopez},
  date           = {2020-04-30},
  title          = {Machine learning for asset managers},
  doi            = {10.1017/9781108883658},
  isbn           = {9781108792899},
  publisher      = {Cambridge University Press},
  url            = {https://www.cambridge.org/core/product/identifier/9781108883658/type/element},
  urldate        = {2020-04-09},
  abstract       = {Successful investment strategies are specific implementations of general theories. An investment strategy that lacks a theoretical justification is likely to be false. Hence, an asset manager should concentrate her efforts on developing a theory rather than on backtesting potential trading rules. The purpose of this Element is to introduce machine learning ({ML}) tools that can help asset managers discover economic and financial theories. {ML} is not a black box, and it does not necessarily overfit. {ML} tools complement rather than replace the classical statistical methods. Some of {ML}'s strengths include (1) a focus on out-of-sample predictability over variance adjudication; (2) the use of computational methods to avoid relying on (potentially unrealistic) assumptions; (3) the ability to  learn  complex specifications, including nonlinear, hierarchical, and noncontinuous interaction effects in a high-dimensional space; and (4) the ability to disentangle the variable search from the specification search, robust to multicollinearity and other substitution effects.},
  day            = {30},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-09-09 18:52},
}

@Article{Clemente-et-al-2021,
  author           = {Clemente, Gian Paolo and Grassi, Rosanna and Hitaj, Asmerilda},
  date             = {2021},
  journaltitle     = {Annals of Operations Research},
  title            = {Asset allocation: new evidence through network approaches},
  doi              = {10.1007/s10479-019-03136-y},
  pages            = {61-80},
  volume           = {299},
  abstract         = {The main contribution of the paper is to unveil the role of the network structure in the financial markets to improve the portfolio selection process, where nodes indicate securities and edges capture the dependence structure of the system. Three different methods are proposed in order to extract the dependence structure between assets in a network context. Starting from this modified structure, we formulate and then we solve the asset allocation problem. We find that the optimal portfolios obtained through a network-based approach are composed mainly of peripheral assets, which are poorly connected with the others. These portfolios, in the majority of cases, are characterized by an higher trade-off between performance and risk with respect to the traditional global minimum variance portfolio. Additionally, this methodology benefits of a graphical visualization of the selected portfolio directly over the graphic layout of the network, which helps in improving our understanding of the optimal strategy.},
  creationdate     = {2021-07-02T23:03:30},
  day              = {20},
  f1000-projects   = {QuantInvest},
  groups           = {Invest_Network},
  modificationdate = {2021-07-02T23:03:30},
  timestamp        = {2019-11-28 13:50},
}

@Article{Giudici-et-al-2020,
  author           = {Giudici, Paolo and Polinesi, Gloria and Spelta, Alessandro},
  date             = {2020},
  journaltitle     = {SSRN e-Print},
  title            = {Network models to improve robot advisory portfolio asset allocation},
  doi              = {10.2139/ssrn.3556381},
  issn             = {1556-5068},
  url              = {https://www.ssrn.com/abstract=3556381},
  urldate          = {2020-03-28},
  abstract         = {Robot advisory services are rapidly expanding, responding to a growing interest people have in directly managing their savings. Robot advisors may reduce costs and improve the quality of the service, making user involvement more transparent. However, they may underestimate market risks, especially when highly correlated assets are being considered, leading to a mismatch between investors' expected and actual risk. The aim of the paper is to enhance robot advisory portfolio allocation, taking users' preference into account. In particular, we demonstrate how Random Matrix Theory and Network models can be combined to construct investment portfolios that provide lower risks with respect to standard Markovitz portfolios. To demonstrate the advantages of this approach we employ the observed returns of a large set of ETFs, which is representative of the financial products at the ground of the activity of robot advisors.},
  creationdate     = {2021-07-02T23:03:51},
  f1000-projects   = {QuantInvest},
  modificationdate = {2021-07-02T23:03:51},
  timestamp        = {2020-07-23 13:38},
}

@Article{Laur-2020,
  author           = {Bhanu Laur},
  date             = {2020},
  journaltitle     = {SSRN e-Print},
  title            = {Portfolio Optimization - Can Optimizing Portfolio Outperform Naive Diversification?},
  doi              = {10.2139/ssrn.3524277},
  url              = {https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3524277},
  abstract         = {In this study we examined the performances of mean-variance and tangency portfolio investment strategies in order to determine if optimal diversification has benefits over 1/N strategy.},
  creationdate     = {2021-07-02T23:04:07},
  modificationdate = {2021-07-02T23:04:07},
  timestamp        = {2020-08-11 00:20},
}

@Article{Snow-2020,
  author           = {Snow, Derek},
  date             = {2020-01-31},
  journaltitle     = {The Journal of Financial Data Science},
  title            = {Machine Learning in Asset Management Part 1: Portfolio Construction Trading Strategies},
  issue            = {1},
  number           = {1},
  pages            = {10-23},
  url              = {https://jfds.pm-research.com/content/2/1/10},
  urldate          = {2020-03-07},
  volume           = {2},
  abstract         = {This is the first in a series of articles dealing with machine learning in asset management. Asset management can be broken into the following tasks: (1) portfolio construction, (2) risk management, (3) capital management, (4) infrastructure and deployment, and (5) sales and marketing. This article focuses on portfolio construction using machine learning. Historically, algorithmic trading could be more narrowly defined as the automation of sell-side trade execution, but since the introduction of more advanced algorithms, the definition has grown to include idea generation, alpha factor design, asset allocation, position sizing, and the testing of strategies. Machine learning, from the vantage of a decision-making tool, can help in all these areas.},
  creationdate     = {2021-07-02T23:04:34},
  day              = {31},
  f1000-projects   = {QuantInvest},
  modificationdate = {2021-07-02T23:04:34},
  publisher        = {Institutional Investor Journals Umbrella},
}

@Article{Snow-2020b,
  author           = {Snow, Derek},
  date             = {2020-04-22},
  journaltitle     = {SSRN e-Print},
  title            = {DeltaPy: A Framework for Tabular Data Augmentation in Python},
  url              = {https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3582219},
  urldate          = {2020-05-03},
  abstract         = {A range of data abstractions have come to the fore since the re-emergence of machine learning. This includes procedures like feature engineering, extraction, transformation, and selection, as well as data preprocessing, generation, synthesisation, and augmentation. This report attempts to unify some of this terminology with the development of a bare-bones Python package, DeltaPy.},
  creationdate     = {2021-07-02T23:04:34},
  day              = {22},
  f1000-projects   = {QuantInvest},
  groups           = {ML_DataAugment},
  modificationdate = {2021-07-02T23:04:34},
}

@Article{Snow-2020a,
  author           = {Snow, Derek},
  date             = {2020-06-02},
  journaltitle     = {SSRN e-Print},
  title            = {{MTSS}-{GAN}: Multivariate Time Series Simulation Generative Adversarial Networks},
  url              = {https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3616557},
  urldate          = {2020-06-19},
  abstract         = {{MT\SS}-{GAN} is a new generative adversarial network ({GAN}) developed to simulate diverse multivariate time series ({MTS}) data with finance applications in mind. The purpose of this synthesiser is two-fold, we both want to generate data that accurately represents the original data, while also having the flexibility to generate data with novel and unique relationships that could help with model testing and robustness checks. The method is inspired by stacked {GANs} originally designed for image generation. Stacked {GANs} have produced some of the best quality images, for that reason {MT\SS}-{GAN} is expected to be a leading contender in multivariate time series generation.},
  creationdate     = {2021-07-02T23:04:34},
  day              = {2},
  f1000-projects   = {QuantInvest},
  modificationdate = {2021-07-02T23:04:34},
}

@Article{Snow-2020c,
  author           = {Snow, Derek},
  date             = {2020-01-31},
  journaltitle     = {The Journal of Financial Data Science},
  title            = {Machine Learning in Asset Management - Part 2: Portfolio Construction - Weight Optimization},
  issue            = {2},
  pages            = {17-24},
  url              = {https://jfds.pm-research.com/content/2/2/17},
  urldate          = {2020-03-07},
  volume           = {2},
  abstract         = {This is the second in a series of articles dealing with machine learning in asset management. This article focuses on portfolio weighting using machine learning. Following from the previous article (Snow 2020), which looked at trading strategies, this article identifies different weight optimization methods for supervised, unsupervised, and reinforcement learning frameworks. In total, seven submethods are summarized, with the code made available for further exploration.},
  creationdate     = {2021-07-02T23:04:34},
  day              = {31},
  f1000-projects   = {QuantInvest},
  modificationdate = {2021-07-02T23:04:35},
  publisher        = {Institutional Investor Journals Umbrella},
}

@Article{Elton-Gruber-2020,
  author           = {Edwin J. Elton and Martin J. Gruber},
  date             = {2020-05},
  journaltitle     = {Financial Analysts Journal},
  title            = {A Review of the Performance Measurement of Long-Term Mutual Funds},
  doi              = {10.1080/0015198x.2020.1738126},
  number           = {3},
  pages            = {22--37},
  volume           = {76},
  abstract         = {We review the major models of mutual fund performance: (1) using return data to evaluate equity funds-from single to multi-index models, (2) measuring passive portfolio performance, (3) using holdings-based performance measures, (4) measuring timing ability, and (5) measuring bond fund performance. We conclude with a discussion of issues affecting performance measurement: data sources and bias, missing factors, and improvements to benchmarks.},
  creationdate     = {2021-07-02T23:04:55},
  modificationdate = {2021-07-02T23:04:55},
  publisher        = {Informa {UK} Limited},
  timestamp        = {2020-10-25 00:24},
}

@Article{Jacobsen-Ma-2020,
  author           = {Jacobsen, Brian and Ma, Chao},
  date             = {2020-04-19},
  journaltitle     = {The Journal of Wealth Management},
  title            = {Alpha alchemy: diversifying without diluting alpha},
  doi              = {10.3905/jwm.2020.1.106},
  issn             = {1534-7524},
  number           = {2},
  pages            = {75-87},
  urldate          = {2020-05-05},
  volume           = {23},
  abstract         = {Managers are often evaluated and selected on the basis of their portfolio  stand-alone risk and return properties. This is too narrowly focused. The basics of mean-variance optimization tells us that covariance and the relationship of a security  returns within the context of the overall portfolio is important. We illustrate how the basics of portfolio construction with individual securities also applies to building a portfolio of managers. First, we show the statistical properties of different asset classes, as more and more managers are used to get exposure to the asset class. Depending on the asset class, tracking error, relative skewness, and relative excess kurtosis improve at different rates. This is important for considering how many managers may be necessary to get exposure to an asset class. We then illustrate an optimization method of combining the managers. Although the managers, on their own, may not have statistically positive alphas, they can be combined in a way to improve the likelihood that the portfolio of managers has a statistically positive alpha. The optimization method is not a substitute for traditional due diligence. It is just one additional tool useful for manager selection and portfolio construction.},
  creationdate     = {2021-07-02T23:05:21},
  day              = {19},
  f1000-projects   = {QuantInvest},
  modificationdate = {2021-07-02T23:05:21},
  timestamp        = {2020-08-31 23:09},
}

@Article{Jennings-et-al-2020,
  author           = {Jennings, William W. and O'Malley, Thomas C. and Payne, Brian C.},
  date             = {2020-04-04},
  journaltitle     = {The Journal of Wealth Management},
  title            = {Normal return gaps: dispersion illuminates diversification},
  doi              = {10.3905/jwm.2020.1.105},
  issn             = {1534-7524},
  number           = {2},
  pages            = {18-35},
  urldate          = {2020-05-05},
  volume           = {23},
  abstract         = {Despite ever more sophisticated risk management and measurement, investment professionals have generally overlooked a simple but powerful measure of relative performance and portfolio diversification the normal return gap. The authors develop a generalized specification of the expected difference in returns between two investments based on the folded normal distribution. Even highly correlated investments can have quite large expected return gaps. They then demonstrate the applicability of this dispersion to capital market forecasts, manager selection, performance evaluation, style tilts, sector bets, socially responsible investing, manager combinations, wash sale taxation, and rebalancing.},
  creationdate     = {2021-07-02T23:06:00},
  day              = {4},
  f1000-projects   = {QuantInvest},
  modificationdate = {2021-07-02T23:06:00},
  timestamp        = {2020-08-31 23:10},
}

@Article{Lassance-et-al-2020,
  author           = {Lassance, Nathan and DeMiguel, Victor and Vrins, Frederic Daniel},
  date             = {2020},
  journaltitle     = {SSRN e-Print},
  title            = {Optimal portfolio diversification via independent component analysis},
  doi              = {10.2139/ssrn.3285156},
  issn             = {1556-5068},
  url              = {https://ssrn.com/abstract=3285156},
  abstract         = {A popular approach for enhancing diversification in portfolio selection is to rely on the factor-risk-parity portfolio, which is often defined as the portfolio whose return variance is spread equally among the principal components (PCs) of asset returns. Although PCs are useful for dimensionality reduction, they are arbitrary because any rotation of the PC basis yields an equally uncorrelated basis. This is problematic because we theoretically demonstrate that any portfolio is the factor-risk-parity portfolio corresponding to a specific uncorrelated basis. To overcome this problem, we rely on the factor-risk-parity portfolio based on the independent components (ICs), which are the rotation of the PCs that are maximally independent, and thus, account for higher-order moments. We propose a shrinkage portfolio that is obtained by combining the minimum-variance portfolio and the IC-risk-parity portfolio. We also show how to exploit the near independence of the ICs to parsimoniously estimate the factor-risk-parity portfolio with respect to Value-at-Risk. Finally, we empirically demonstrate that shrinkage portfolios based on the IC basis outperform those based on the PC basis, as well as the minimum-variance portfolio.},
  creationdate     = {2021-07-02T23:06:39},
  f1000-projects   = {QuantInvest},
  groups           = {Diversified_Invest, Invest_Diversif},
  modificationdate = {2021-07-02T23:06:39},
  timestamp        = {2020-07-23 13:37},
}

@Article{Lassance-2020,
  author           = {Lassance, Nathan},
  date             = {2020},
  journaltitle     = {SSRN e-Print},
  title            = {Information-Theoretic Approaches to Portfolio Selection},
  doi              = {10.2139/ssrn.3500947},
  issn             = {1556-5068},
  url              = {https://www.ssrn.com/abstract=3500947},
  urldate          = {2020-01-19},
  abstract         = {Ever since modern portfolio theory was introduced by Harry Markowitz in 1952, a plethora of papers have been written on the mean-variance investment problem. However, due to the non-Gaussian nature of asset returns, the mean and variance statistics are insufficient to adequately represent their full distribution, which depends on higher moments too. Higher-moment portfolio selection is however more complex; a smaller literature has been dedicated to this problem and no consensus emerges about how investors should allocate their wealth when higher moments cannot be ignored. Among the proposed alternatives, researchers have recently considered information theory, and entropy in particular, as a new framework to tackle this problem. Entropy provides an appealing criterion as it measures the amount of randomness embedded in a random variable from the shape of its density function, thus accounting for all moments. The application of information theory to portfolio selection is however nascent and much remains to explore. Therefore, in this thesis, we aim to explore the portfolio-selection problem from an information-theoretic angle, accounting for higher moments. We review the relevant literature and mathematical concepts in Chapter 1. Then, we consider in Chapter 2 a natural alternative to the popular minimum-variance portfolio strategy using Renyi entropy as information-theoretic criterion. We show that the exponential Renyi entropy fulfills natural properties as a risk measure. However, although Renyi entropy has some nice features, we show that it can be an undesirable investment criterion because it may lead to portfolios with worse higher moments than minimizing the variance. For this reason, we turn in chapters 3 to 5 to different ways of applying entropy, thereby revisiting two popular frameworks -- risk parity and expected utility -- to account for higher moments. In Chapter 3, we investigate the factor-risk-parity portfolio -- a popular strategy among practitioners -- that aims to diversify the portfolio-return risk across uncorrelated factors underlying the asset returns. We show that although principal component analysis (PCA) is very useful for dimension reduction, its resulting factor-risk-parity portfolio is suboptimal. Indeed, PCA merely provides one choice of uncorrelated factors out of infinitely many others, and one would prefer to be diversified over independent factors rather than merely uncorrelated ones. Instead, thus, we propose to diversify the risk across maximally independent factors, provided by independent component analysis (ICA). We show theoretically that this solves the issues related to principal components and provides a natural way of reducing the kurtosis of portfolio returns. In Chapter 4, we apply ICA in a different way in order to obtain robust estimates of moment-based portfolios, such as those based on expected utility. It is well known that these portfolios are difficult to estimate, particularly in high dimensions, because the number of comoments quickly explodes with the number of assets. We propose to address this curse of dimensionality by projecting the asset returns on a small set of maximally independent factors provided by ICA, and neglecting their remaining dependence. In doing so, we obtain sparse approximations of the comoment tensors of asset returns. This drastically decreases the dimensionality of the problem and leads to well-performing and computationally efficient investment strategies with low turnover. In Chapter 5, we introduce an alternative approach to the utility function to capture investors' preferences. The latter is praised by academics but is difficult to specify when higher moments matter. Because investors ultimately care about the distribution of their portfolio returns, our proposal is to capture their preferences via a target-return distribution. The optimal portfolio is then the one whose distribution minimizes the Kullback-Leibler divergence with respect to the target distribution. Our theoretical exploration shows that Shannon entropy plays a central role as higher-moment criterion in this framework, and our empirical analysis confirms that this strategy outperforms mean-variance portfolios out of sample.},
  creationdate     = {2021-07-02T23:06:39},
  f1000-projects   = {QuantInvest},
  modificationdate = {2021-07-02T23:06:39},
  timestamp        = {2020-08-08 21:27},
}

@Article{Lassance-2020a,
  author           = {Nathan Lassance},
  date             = {2020},
  journaltitle     = {SSRN e-Print},
  title            = {Reconciling Mean-Variance Portfolio Theory with Non-gaussian Returns},
  doi              = {10.2139/ssrn.3664049},
  abstract         = {Mean-variance portfolio theory remains frequently used as investment rationale because of its simplicity, its closed-form solution, and the availability of many well-performing robust estimators. At the same time, it is also frequently rejected on the grounds that it ignores the higher moments of non-Gaussian returns. However, higher-moment portfolios are associated with many different objective functions, are numerically more complex, and exacerbate estimation risk. In this paper, we reconcile mean-variance portfolio theory with non-Gaussian returns by identifying, among all portfolios on the mean-variance efficient frontier, the one that optimizes a chosen higher-moment criterion. Via numerical simulations and an empirical analysis, we find that, for three higher-moment objective functions and adjusting for transaction costs, the resulting portfolios outperform the minimum-variance and fully optimized portfolios out of sample both in terms of Sharpe ratio and higher moments, thus striking a favorable tradeoff between specification and estimation error.},
  creationdate     = {2021-07-02T23:06:39},
  modificationdate = {2021-07-02T23:06:39},
  publisher        = {Elsevier {BV}},
  timestamp        = {2020-09-27 15:39},
}

@Article{Papathanakos-2020,
  author           = {Vassilios Papathanakos},
  date             = {2020},
  journaltitle     = {SSRN e-Print},
  title            = {Diversification Potential},
  url              = {https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3622775},
  abstract         = {In these working notes, we introduce the concept of the diversification potential, which expresses the maximum diversification premium available by rebalancing to fixed target weights for a collection of assets. We distinguish between two forms of diversification potential: the unconstrained (which allows portfolios with short positions) and the constrained (which only permits long-only portfolios).},
  creationdate     = {2021-07-02T23:07:19},
  modificationdate = {2021-07-02T23:07:19},
}

@Article{Oyenubi-2019,
  author               = {Oyenubi, Adeola},
  date                 = {2019},
  journaltitle         = {Computational Economics},
  title                = {Diversification Measures and the Optimal Number of Stocks in a Portfolio: An Information Theoretic Explanation},
  doi                  = {10.1007/s10614-016-9600-5},
  number               = {54},
  pages                = {11443-1471-29},
  abstract             = {This paper provides a plausible explanation for why the optimum number of stocks in a portfolio is elusive, and suggests a way to determine this optimal number. Diversification has a lot to do with the number of stocks in a portfolio. Adding stocks to a portfolio increases the level of diversification, and consequently leads to risk reduction up to a certain number of stocks, beyond which additional stocks are of no benefit, in terms of risk reduction. To explain this phenomenon, this paper investigates the relationship between portfolio diversification and concentration using a genetic algorithm. To quantify diversification, we use the portfolio Diversification Index (PDI). In the case of concentration, we introduce a new quantification method. Concentration is quantified as complexity of the correlation matrix. The proposed method quantifies the level of dependency (or redundancy) between stocks in a portfolio. By contrasting the two methods it is shown that the optimal number of stocks that optimizes diversification depends on both number of stocks and average correlation. Our result shows that, for a given universe, there is a set of Pareto optimal portfolios containing a different number of stocks that simultaneously maximizes diversification and minimizes concentration. The choice portfolio among the Pareto set will depend on the preference of the investor. Our result also suggests that an ideal condition for the optimal number of stocks is when variance reduction benefit of diversification is off-set by the variance contribution of complexity.},
  citeulike-article-id = {14398652},
  citeulike-linkout-0  = {http://dx.doi.org/10.1007/s10614-016-9600-5},
  citeulike-linkout-1  = {http://link.springer.com/article/10.1007/s10614-016-9600-5},
  creationdate         = {2021-07-02T23:07:57},
  groups               = {Diversification_Measure, Invest_Diversif},
  modificationdate     = {2021-07-02T23:07:57},
  posted-at            = {2017-07-23 16:05:16},
  publisher            = {Springer US},
  timestamp            = {2019-10-11 18:43},
}

@InCollection{Oyenubi-2020,
  author           = {Adeola Oyenubi},
  booktitle        = {Advanced Studies of Financial Technologies and Cryptocurrency Markets},
  date             = {2020},
  title            = {Optimal Portfolios on Mean-Diversification Efficient Frontiers},
  doi              = {10.1007/978-981-15-4498-9_3},
  pages            = {35--63},
  publisher        = {Springer Singapore},
  abstract         = {Recent research has seen increasing use of risk/diversification based approach to portfolio optimization. Under the risk-based approach, returns are ignored, and a diversification or risk measure is optimized in portfolio construction. This approach has been criticized for lacking a clearly defined objective like the trade-off between returns and risk in the Markowitz's Mean-variance set up. Optimizing risk/diversification alone is a single objective optimization approach to portfolio construction. This is in contrast to the usual bi-objective optimization that yields the portfolio that optimizes the trade-off between return and risk. In this paper, we note that portfolios that optimize the trade-off between returns and diversification measures exist (i.e. portfolios that replace variance with other risk measures). In theory, these mean-diversification optimal portfolios should dominate risk-based portfolio on a risk-adjusted basis. Using genetic algorithm, mean-diversification efficient frontiers are drawn for various diversification measures and portfolios that optimize the trade-off between returns and the diversification measures are identified. We argue that these portfolios are better candidates to be compared with the portfolio that is constructed to be mean-variance optimal since they sensitive to returns. Our results suggest that mean-diversification optimal portfolios are useful alternatives in terms of risk-reward trade-off based on their in-sample and out-of-sample performance.},
  creationdate     = {2021-07-02T23:07:57},
  modificationdate = {2021-07-02T23:07:57},
  timestamp        = {2020-09-27 15:15},
}

@Article{Baitinger-Flegel-2021,
  author           = {Eduard Baitinger and Samuel Flegel},
  date             = {2021-02},
  journaltitle     = {Financial Markets and Portfolio Management},
  title            = {The better turbulence index? Forecasting adverse financial markets regimes with persistent homology},
  doi              = {10.1007/s11408-020-00377-x},
  pages            = {Early View},
  abstract         = {Persistent homology is the workhorse of modern topological data analysis, which in recent years becomes increasingly powerful due to methodological and computing power advances. In this paper, after equipping the reader with the relevant background on persistent homology, we show how this tool can be harnessed for investment purposes. Specifically, we propose a persistent homology-based turbulence index for the detection of adverse market regimes. With the help of an out-of-sample study, we demonstrate that investment strategies relying on a persistent homology-based turbulence detection outperform investment strategies based on other popular turbulence indices. Additionally, we conduct a stability analysis of our findings. This analysis confirms the results from the previous out-of-sample study, as the outperformance prevails for most configurations of the respective investment strategy and thereby mitigating possible data mining concerns.},
  creationdate     = {2021-07-02T23:08:40},
  modificationdate = {2021-07-02T23:08:40},
  publisher        = {Springer Science and Business Media {LLC}},
  timestamp        = {2021-03-08 16:36},
}

@Article{Baitinger-Flegel-2019,
  author           = {Baitinger, Eduard and Flegel, Samuel},
  date             = {2019},
  journaltitle     = {SSRN e-Print},
  title            = {The Better Turbulence Index? Forecasting Adverse Financial Markets Regimes with Persistent Homology},
  doi              = {10.2139/ssrn.3490567},
  issn             = {1556-5068},
  url              = {https://www.ssrn.com/abstract=3490567},
  urldate          = {2019-12-26},
  abstract         = {Persistent homology is the workhorse of modern topological data analysis, which in recent years becomes increasingly powerful due to methodological and computing power advances. In this paper, after equipping the reader with the relevant background on persistent homology, we show how this tool can be harnessed for investment purposes. Specifically, we propose a persistent homology based turbulence index for the detection of adverse market regimes. With the help of an out-of-sample study, we demonstrate that investment strategies relying on a persistent homology based turbulence detection outperform investment strategies based on other popular turbulence indices. Additionally, we conduct a stability analysis of our findings. This analysis confirms the results from the previous out-of-sample study, as the outperformance prevails for most configurations of the respective investment strategy and hence mitigating possible data mining concerns.},
  creationdate     = {2021-07-02T23:08:40},
  f1000-projects   = {QuantInvest},
  modificationdate = {2021-07-02T23:08:40},
  timestamp        = {2020-07-23 13:43},
}

@Article{Baitinger-2019,
  author           = {Eduard Baitinger},
  date             = {2019},
  journaltitle     = {SSRN e-Print},
  title            = {Network-Based Financial Forecasting: A Statistical and Economic Analysis},
  doi              = {10.2139/ssrn.3370098},
  abstract         = {One of the main challenges facing researchers and industry professionals for decades is the successful prediction of asset returns. This paper enriches this endeavor by an in-depth analysis of topological metrics of correlation networks applied to financial forecasting. While academic research often focuses on statistical performance metrics, industry professionals are more interested in the economic value-added of competing forecasting approaches. Since statistical significance does not automatically imply economic significance, this article devotes attention to both types of performance metrics. We show that the benchmark mean model is indeed difficult to beat when it comes to statistical performance metrics. However, considering economic metrics, network-based predictors generate a clear value-added, which also applies to the multi risky asset allocation dimension.},
  creationdate     = {2021-07-02T23:08:40},
  modificationdate = {2021-07-02T23:08:40},
  publisher        = {Elsevier {BV}},
  timestamp        = {2020-09-23 12:37},
}

@Article{Cajas-2019,
  author           = {Dany Cajas},
  date             = {2019},
  journaltitle     = {SSRN e-Print},
  title            = {Robust Portfolio Selection with Near Optimal Centering},
  doi              = {10.2139/ssrn.3572435},
  abstract         = {Quantitative asset allocation models have not been widely adopted by practitioners because they suffer from two problems: the lack of robustness and diversification of portfolios obtained through these models. To solve these problems, I developed a new portfolio selection method that can be applied to any convex risk measure. The procedure begins selecting an optimal portfolio in the efficient frontier, then I define a near optimal region and finally I define the analytic center as the new optimal portfolio. I com- pare 30 portfolio optimization models for 4 asset samples, and the results suggest that the new method overcomes traditional methods in robustness and diversification.},
  creationdate     = {2021-07-02T23:08:59},
  modificationdate = {2021-07-02T23:08:59},
  publisher        = {Elsevier {BV}},
  timestamp        = {2020-10-29 07:11},
}

@Article{Javed-et-al-2020,
  author           = {Ali Javed and Byung Suk Lee and Dona M. Rizzo},
  date             = {2020-04-20},
  journaltitle     = {arXiv e-Print},
  title            = {A Benchmark Study on Time Series Clustering},
  eprinttype       = {arXiv},
  url              = {https://arxiv.org/abs/2004.09546},
  abstract         = {This paper presents the first time series clustering benchmark utilizing all time series datasets currently available in the University of California Riverside (UCR) archive -- the state of the art repository of time series data. Specifically, the benchmark examines eight popular clustering methods representing three categories of clustering algorithms (partitional, hierarchical and density-based) and three types of distance measures (Euclidean, dynamic time warping, and shape-based). We lay out six restrictions with special attention to making the benchmark as unbiased as possible. A phased evaluation approach was then designed for summarizing dataset-level assessment metrics and discussing the results. The benchmark study presented can be a useful reference for the research community on its own; and the dataset-level assessment metrics reported may be used for designing evaluation frameworks to answer different research questions.},
  creationdate     = {2021-07-02T23:09:47},
  file             = {http://arxiv.org/pdf/2004.09546v2:PDF},
  keywords         = {cs.LG, stat.ML},
  modificationdate = {2021-07-02T23:09:47},
  timestamp        = {2020-09-22 20:31},
}

@Article{Marti-et-al-2020,
  author           = {Marti, Gautier and Nielsen, Frank and Bihkowski, Mikolaj and Donnat, Philippe},
  date             = {2020},
  journaltitle     = {arXiv e-Print},
  title            = {A review of two decades of correlations, hierarchies, networks and clustering in financial markets},
  eprinttype       = {arXiv},
  url              = {https:://arxiv.org/abs/1703.00485},
  urldate          = {2020-10-07},
  abstract         = {This document is a preliminary version of an in-depth review on the state of the art of clustering financial time series and the study of correlation networks. This preliminary document is intended for researchers in this field so that they can feedback to allow amendments, corrections and addition of new material unknown to the authors of this review. The aim of the document is to gather in one place the relevant material that can help the researcher in the field to have a bigger picture, the quantitative researcher to play with this alternative modeling of the financial time series, and the decision maker to leverage the insights obtained from these methods. We hope that this document will form a basis for implementation of an open toolbox of standard tools to study correlations, hierarchies, networks and clustering in financial markets. We also plan to maintain pointers to online material and an updated version of this work at www.datagrapple.com/Tech.},
  creationdate     = {2021-07-02T23:10:57},
  day              = {1},
  groups           = {Networks and investment management, Clustering and network analysis, Invest_Network},
  modificationdate = {2021-07-02T23:10:57},
  timestamp        = {2020-10-23 13:33},
}

@InProceedings{Marti-2020,
  author           = {Marti, Gautier},
  booktitle        = {ICASSP - IEEE nternational Conference on Acoustics, Speech and Signal Processing},
  date             = {2020-05-04},
  title            = {{CORRGAN}: sampling realistic financial correlation matrices using generative adversarial networks},
  doi              = {10.1109/ICASSP40776.2020.9053276},
  isbn             = {978-1-5090-6631-5},
  pages            = {8459-8463},
  publisher        = {IEEE},
  url              = {https://ieeexplore.ieee.org/document/9053276/},
  urldate          = {2020-06-19},
  abstract         = {We propose a novel approach for sampling realistic financial correlation matrices. This approach is based on generative adversarial networks. Experiments demonstrate that generative adversarial networks are able to recover most of the known stylized facts about empirical correlation matrices estimated on asset returns. This is the first time such results are documented in the literature. Practical financial applications range from trading strategies enhancement to risk and portfolio stress testing. Such generative models can also help ground empirical finance deeper into science by allowing for falsifiability of statements and more objective comparison of empirical methods.},
  creationdate     = {2021-07-02T23:10:57},
  day              = {4},
  f1000-projects   = {QuantInvest},
  modificationdate = {2021-07-02T23:10:57},
  timestamp        = {2020-06-20 01:45},
}

@Online{Marti-2020a,
  author           = {Gautier Marti},
  date             = {2020},
  title            = {How to combine a handful of predictors using empirical copulas and maximum likelihood},
  url              = {https://marti.ai/qfin/2020/08/09/predictors-copula-combined.html},
  abstract         = {In this blog, I will present another empirical copula trick for combining a handful of forecasters. Each forecaster can have its own bias with different levels of noise. The copula is useful to learn and leverage the potentially non-linear correlations existing between the forecasters errors.},
  creationdate     = {2021-07-02T23:10:57},
  modificationdate = {2021-07-02T23:10:57},
  timestamp        = {2020-08-09 10:43},
}

@InProceedings{Marti-2020b,
  author           = {Gautier Marti},
  booktitle        = {IHS Markit Webinar},
  date             = {2020},
  title            = {Generating Realistic Synthetic Data in Finance: Applications of GANs},
  url              = {https://www.slideshare.net/GautierMarti/generating-realistic-synthetic-data-in-finance},
  abstract         = {Talk at IHS Markit Webinar (15 October 2020) on the potential Applications of GANs in Finance. These models could be useful for quants and their managers to avoid over-fitting, portfolio and risk managers for proper capital and risk allocation, cloud computing servicing willing to work with banks and other sensitive data rich organizations, auditors and regulators to detect frauds, and data vendors (such as IHS Markit) to bring new products to market and iterate quickly with clients.},
  creationdate     = {2021-07-02T23:10:57},
  modificationdate = {2021-07-02T23:10:57},
  timestamp        = {2020-10-15 13:26},
}

@Article{Yang-Kazemi-2020,
  author           = {Yang, Xiaohui and Kazemi, Hossein B.},
  date             = {2020-02-19},
  journaltitle     = {The Journal of Alternative Investments},
  title            = {Holdings concentration and hedge fund investment strategies},
  doi              = {10.3905/jai.2020.1.092},
  issn             = {1520-3255},
  number           = {4},
  pages            = {92-106},
  urldate          = {2020-03-07},
  volume           = {22},
  abstract         = {This article examines the risk-return performance of concentrated positions of hedge funds in large-cap and small-cap stocks. The research shows that small-cap stocks in which hedge funds have concentrated positions earn higher future returns than those that are not part of hedge funds  concentrated holdings. Also, stocks that are part of the concentrated positions of hedge funds display higher downside risks and relatively large downside returns during periods of market turmoil. The results presented indicate that hedge fund managers are skilled in making equity investments under different degrees of market efficiency. The authors  findings have two practical implications: (1) hedge funds that hold concentrated positions in small-cap stocks may outperform their peers and (2) investors may be able to improve the performance of their equity portfolios by monitoring hedge funds  positions in small-cap stocks.},
  creationdate     = {2021-07-02T23:12:56},
  day              = {19},
  f1000-projects   = {QuantInvest},
  modificationdate = {2021-07-02T23:12:56},
}

@Article{Jain-Jain-2019a,
  author           = {Jain, Prayut and Jain, Shashi},
  date             = {2019-05-26},
  journaltitle     = {SSRN e-Print},
  title            = {Can Machine Learning Based Portfolios Outperform Traditional Risk-Based Portfolios? The Need to Account for Covariance Misspecification},
  url              = {https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3394427},
  urldate          = {2019-09-06},
  abstract         = {The Hierarchical risk parity (HRP) approach of portfolio allocation, introduced by [Lopez de Prado, 2016], applies graph theory and machine learning to build a diversified portfolio. Like the traditional risk based allocation methods, HRP is also a function of the estimate of the covariance matrix, however, it doesn require its invertibility. In this paper we first study the impact of covariance misspecification on the performance of the different allocation methods. Next we study under appropriate covariance forecast model whether the machine learning based HRP out-performs the traditional risk based portfolios. For our analysis we use the test for superior predictive ability, on out-of-sample portfolio performance, to determine whether the observed excess performance is significant or occurred by chance. We find that when the covariance estimates are crude, inverse volatility weighted portfolios are more robust, followed by the machine learning based portfolios. Minimum variance and maximum diversification are most sensitive to covariance misspecification. HRP follows the middle ground, it is less sensitive to covariance misspecification when compared with minimum variance or maximum diversification portfolio, while it is not as robust as the inverse volatility weighed portfolio. We also study the impact of different rebalancing horizon and how the portfolios compare against a market-capitalization weighted portfolio.},
  creationdate     = {2021-07-02T23:13:20},
  day              = {26},
  f1000-projects   = {QuantInvest},
  groups           = {ML_InvestSelect, Invest_Risk},
  modificationdate = {2021-07-02T23:13:20},
  timestamp        = {2020-07-23 13:37},
}

@Article{Jain-Jain-2019,
  author           = {Prayut Jain and Shashi Jain},
  date             = {2019},
  journaltitle     = {Risks},
  title            = {Can Machine Learning-Based Portfolios Outperform Traditional Risk-Based Portfolios? The Need to Account for Covariance Misspecification},
  doi              = {10.3390/risks7030074},
  number           = {3},
  pages            = {74+},
  volume           = {7},
  abstract         = {The Hierarchical risk parity (HRP) approach of portfolio allocation, introduced by Lopez de Prado (2016), applies graph theory and machine learning to build a diversified portfolio. Like the traditional risk-based allocation methods, HRP is also a function of the estimate of the covariance matrix, however, it does not require its invertibility. In this paper, we first study the impact of covariance misspecification on the performance of the different allocation methods. Next, we study under an appropriate covariance forecast model whether the machine learning based HRP outperforms the traditional risk-based portfolios. For our analysis, we use the test for superior predictive ability on out-of-sample portfolio performance, to determine whether the observed excess performance is significant or if it occurred by chance. We find that when the covariance estimates are crude, inverse volatility weighted portfolios are more robust, followed by the machine learning-based portfolios. Minimum variance and maximum diversification are most sensitive to covariance misspecification. HRP follows the middle ground; it is less sensitive to covariance misspecification when compared with minimum variance or maximum diversification portfolio, while it is not as robust as the inverse volatility weighed portfolio. We also study the impact of the different rebalancing horizon and how the portfolios compare against a market-capitalization weighted portfolio.},
  creationdate     = {2021-07-02T23:13:20},
  journal          = {Risks},
  modificationdate = {2021-07-02T23:13:20},
  publisher        = {{MDPI} {AG}},
  timestamp        = {2020-09-07 22:31},
}

@Comment{jabref-meta: databaseType:biblatex;}
